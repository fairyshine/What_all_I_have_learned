# 1 torch å‘é‡(tensor)

Each torch.Tensor has a torch.dtype, torch.device, and torch.layout.

requires_grad:

- åœ¨ç”¨æˆ·æ‰‹åŠ¨å®šä¹‰tensoræ—¶ï¼Œå‚æ•°requires_gradé»˜è®¤å€¼æ˜¯Falseã€‚

- è€Œåœ¨Moduleä¸­çš„å±‚åœ¨å®šä¹‰æ—¶ï¼Œç›¸å…³tensorçš„requires_gradå‚æ•°é»˜è®¤æ˜¯Trueã€‚

- åœ¨è®¡ç®—å›¾ä¸­ï¼Œå¦‚æœæœ‰ä¸€ä¸ªè¾“å…¥çš„requires_gradæ˜¯Trueï¼Œé‚£ä¹ˆè¾“å‡ºçš„requires_gradä¹Ÿæ˜¯Trueã€‚åªæœ‰åœ¨æ‰€æœ‰è¾“å…¥çš„requires_gradéƒ½ä¸ºFalseæ—¶ï¼Œè¾“å‡ºçš„requires_gradæ‰ä¸ºFalseã€‚

backward():

- åªèƒ½å¯¹æ ‡é‡æ±‚å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰


detach():

- è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œå¹¶ä¸”è¿™ä¸ªtensoræ˜¯ä»å½“å‰çš„è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥çš„ï¼ˆæˆªæ–­è®¡ç®—å›¾ï¼‰ã€‚ä½†æ˜¯è¿”å›çš„tensorå’ŒåŸæ¥çš„tensoræ˜¯å…±äº«å†…å­˜ç©ºé—´çš„ã€‚

### å‘é‡æ“ä½œ

å˜å½¢ï¼š

âŒ  view() ï¼ˆå·²è¿‡æ—¶ï¼‰

âœ…  reshape()

- reshapeæ–¹æ³•æ›´å¼ºå¤§ï¼Œå¯ä»¥è®¤ä¸ºa.reshape = a.view() + a.contiguous().view()ã€‚

- https://blog.csdn.net/Flag_ing/article/details/109129752

# 2 åŸºç¡€ç½‘ç»œ torch.nn

## 2.1 çº¿æ€§å±‚

CLASS torch.nn.Linear(
in_features, 
out_features, 
bias=True, 
device=None, dtype=None
)



## 2.2 å…¶ä»–åŸºç¡€å±‚

### æ± åŒ–å±‚

CLASS torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)



(N,C,H,W) æ‰¹é‡å¤§å°ï¼Œé€šé“æ•°ï¼Œå›¾ç‰‡é«˜åº¦ï¼Œå›¾ç‰‡å®½åº¦

kernel_size ï¼šè¡¨ç¤ºåšæœ€å¤§æ± åŒ–çš„çª—å£å¤§å°ï¼Œå¯ä»¥æ˜¯å•ä¸ªå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯tupleå…ƒç»„

stride ï¼šæ­¥é•¿ï¼Œå¯ä»¥æ˜¯å•ä¸ªå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯tupleå…ƒç»„

padding ï¼šå¡«å……ï¼Œå¯ä»¥æ˜¯å•ä¸ªå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯tupleå…ƒç»„

dilation ï¼šæ§åˆ¶çª—å£ä¸­å…ƒç´ æ­¥å¹…

return_indices ï¼šå¸ƒå°”ç±»å‹ï¼Œè¿”å›æœ€å¤§å€¼ä½ç½®ç´¢å¼•

ceil_mode ï¼šå¸ƒå°”ç±»å‹ï¼Œä¸ºTrueï¼Œç”¨å‘ä¸Šå–æ•´çš„æ–¹æ³•ï¼Œè®¡ç®—è¾“å‡ºå½¢çŠ¶ï¼›é»˜è®¤æ˜¯å‘ä¸‹å–æ•´ã€‚



### å½’ä¸€åŒ–å±‚

CLASS torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)



num_featuresï¼šä¸€èˆ¬è¾“å…¥å‚æ•°ä¸ºbatch_sizeÃ—num_featuresÃ—heightÃ—widthï¼Œå³ä¸ºå…¶ä¸­ç‰¹å¾çš„æ•°é‡

epsï¼šåˆ†æ¯ä¸­æ·»åŠ çš„ä¸€ä¸ªå€¼ï¼Œç›®çš„æ˜¯ä¸ºäº†è®¡ç®—çš„ç¨³å®šæ€§ï¼Œé»˜è®¤ä¸ºï¼š1e-5

momentumï¼šä¸€ä¸ªç”¨äºè¿è¡Œè¿‡ç¨‹ä¸­å‡å€¼å’Œæ–¹å·®çš„ä¸€ä¸ªä¼°è®¡å‚æ•°ï¼ˆæˆ‘çš„ç†è§£æ˜¯ä¸€ä¸ªç¨³å®šç³»æ•°ï¼Œç±»ä¼¼äºSGDä¸­çš„momentumçš„ç³»æ•°ï¼‰

affineï¼šå½“è®¾ä¸ºtrueæ—¶ï¼Œä¼šç»™å®šå¯ä»¥å­¦ä¹ çš„ç³»æ•°çŸ©é˜µgammaå’Œbeta

## 2.3 å·ç§¯å±‚

CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)



groups:åˆ†ç»„ï¼Œè¾“å…¥ã€è¾“å‡ºé€šé“å‡åˆ†ç»„ï¼Œå‡éœ€è¢«å…¶æ•´é™¤



![å·ç§¯å±‚groups](./pic/å·ç§¯å±‚groups.jpg)

## 2.4 å¾ªç¯å±‚

nn.LSTM

åˆå§‹åŒ–ï¼š

``` python
CLASS torch.nn.LSTM(*args, **kwargs)
```
![LSTMå‚æ•°](pic/LSTMå‚æ•°.png)

- input_size â€“ The number of expected features in the input x
- hidden_size â€“ The number of features in the hidden state h
- num_layers â€“ Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and - computing the final results. Default: 1
- bias â€“ If False, then the layer does not use bias weights b_ih and b_hh. Default: True
- batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the - Inputs/Outputs sections below for details. Default: False
- dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0
- bidirectional â€“ If True, becomes a bidirectional LSTM. Default: False
- proj_size â€“ If > 0, will use LSTM with projections of corresponding size. Default: 0

è¾“å…¥ï¼šinput, (h_0, c_0)
- input: (L,H_in)  (L,N,H_in)or(N,L,H_in) depending on batch_first
- h_0:   (D*num_layers,H_out)   (D*num_layers,N,H_out)
- c_0:   (D*num_layers,H_cell)   (D*num_layers,N,H_cell)

N=batch size       L=sequence length         D=2 if åŒå‘ else 1

è¾“å‡ºï¼šoutput, (h_n, c_n)
- output: (L,D*H_out)         (L,N,D*H_out)or(N,L,D*H_out)
- h_n:   (D*num_layers,H_out)    (D*num_layers,N,H_out)
- c_n:   (D*num_layers,H_cell)   (D*num_layers,N,H_cell)

- H_in=input size
- H_cell=hidden size
- H_out=hidden size  OR  proj_size if it>0

## 2.5 Transformerå±‚

![Transformeræ•´ä½“ç»“æ„](pic/Transformeræ•´ä½“ç»“æ„.jpg)

æ•´ä½“ï¼š

ã€ğŸŒŸã€‘nn.Transformer

å¤šå±‚ï¼š

- nn.TransformerEncoder Nä¸ªç¼–ç å±‚
- nn.TransformerDecoder Nä¸ªè§£ç å±‚

å•å±‚ï¼š

- nn.TransformerEncoderLayer  ç¼–ç å±‚  å¤šå¤´æ³¨æ„åŠ›+å‰é¦ˆ
- nn.TransformerDecoderLayer è§£ç å±‚  å¤šå¤´/Maskedå¤šå¤´æ³¨æ„åŠ›+å‰é¦ˆ
