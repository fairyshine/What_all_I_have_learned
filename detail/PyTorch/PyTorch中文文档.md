# è¯´æ˜

## è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶

æ¦‚è¿°Autogradå¦‚ä½•å·¥ä½œå¹¶è®°å½•æ“ä½œã€‚

### ä»åå‘ä¸­æ’é™¤å­å›¾

æ¯ä¸ªå˜é‡éƒ½æœ‰ä¸¤ä¸ªæ ‡å¿—ï¼š`requires_grad`å’Œ`volatile`ã€‚

å®ƒä»¬éƒ½å…è®¸ä»æ¢¯åº¦è®¡ç®—ä¸­ç²¾ç»†åœ°æ’é™¤å­å›¾ï¼Œå¯ä»¥æé«˜æ•ˆç‡ã€‚

ï¼ˆ1ï¼‰`requires_grad`

**åªæœ‰æ‰€æœ‰è¾“å…¥éƒ½ä¸éœ€è¦æ¢¯åº¦ï¼ˆå‡è§†ä¸ºå¸¸é‡ï¼Ÿï¼‰ï¼Œè¾“å‡ºæ‰ä¸éœ€è¦æ¢¯åº¦ã€‚**

æƒ³è¦å†»ç»“éƒ¨åˆ†æ¨¡å‹æ—¶ï¼Œæˆ–æ˜ç¡®ä¸ä¼šä½¿ç”¨æŸäº›å‚æ•°çš„æ¢¯åº¦ï¼Œåˆ™å¯ä»¥åˆ‡æ¢å¯¹åº”æ¨¡å‹ä¸­çš„`requires_grad`æ ‡å¿—ã€‚

æ³¨ï¼šæ–°æ„é€ modulesçš„å‚æ•°é»˜è®¤`requires_gradj=True`

```python
>>> x = Variable(torch.randn(5, 5)) #Variableç”¨æ³•å·²ç»è¿‡æ—¶
>>> y = Variable(torch.randn(5, 5))
>>> z = Variable(torch.randn(5, 5), requires_grad=True)
>>> a = x + y
>>> a.requires_grad
False
>>> b = a + z
>>> b.requires_grad
True
```

ï¼ˆ2ï¼‰`volatile` (é‡Šä¹‰ï¼šæ˜“æŒ¥å‘çš„ï¼›å˜åŒ–æ— å¸¸çš„ï¼Œä¸ç¨³å®šçš„)

çº¯ç²¹çš„inferenceæ¨¡å¼ä¸‹æ¨èä½¿ç”¨`volatile`,æ˜ç¡®ä¸ä¼šè°ƒç”¨`.backward()`æ—¶ã€‚

å®ƒå°†ä½¿ç”¨ç»å¯¹æœ€å°çš„å†…å­˜æ¥è¯„ä¼°æ¨¡å‹ï¼Œä¸ä¼šä¿å­˜ä»»ä½•ä¸­é—´çŠ¶æ€ã€‚

`volatile=True` å†³å®šäº†`requires_grad=False`

ä¸€ä¸ªæ“ä½œåªè¦æœ‰ä¸€ä¸ª`volatile`çš„è¾“å…¥ï¼Œå®ƒçš„è¾“å‡ºä¹Ÿå¿…ç„¶æ˜¯`volatile`ã€‚

```python
>>> regular_input = Variable(torch.randn(5, 5))
>>> volatile_input = Variable(torch.randn(5, 5), volatile=True)
>>> model = torchvision.models.resnet18(pretrained=True)
>>> model(regular_input).requires_grad
True
>>> model(volatile_input).requires_grad
False
>>> model(volatile_input).volatile
True
>>> model(volatile_input).creator is None
True
```

### è‡ªåŠ¨æ±‚å¯¼å¦‚ä½•ç¼–ç å†å²ä¿¡æ¯

å˜é‡çš„`.creator`å±æ€§ï¼ŒæŒ‡å‘æŠŠå®ƒä½œä¸ºè¾“å‡ºçš„å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªç”±`Function`å¯¹è±¡ä½œä¸ºèŠ‚ç‚¹ç»„æˆçš„æœ‰å‘æ— ç¯å›¾(DAG)çš„å…¥å£ç‚¹ï¼Œå®ƒä»¬ä¹‹é—´çš„å¼•ç”¨å°±æ˜¯å›¾çš„è¾¹ã€‚

ï¼ˆå˜é‡æ˜¯å›¾çš„å…¥å£ï¼Œå‡½æ•°æ˜¯å›¾çš„å‡ºå£ã€‚ï¼‰

æ¯æ¬¡æ‰§è¡Œä¸€ä¸ªæ“ä½œï¼Œä¸€ä¸ªè¡¨ç¤ºå®ƒçš„æ–°çš„`Function`å¯¹è±¡å°±è¢«å®ä¾‹åŒ–ï¼Œå®ƒçš„`forward()`æ–¹æ³•è¢«è°ƒç”¨ï¼Œå¹¶ä¸”å®ƒè¾“å‡ºçš„`variable`çš„åˆ›å»ºè€…è¢«è®¾ç½®ä¸ºè¿™ä¸ª`Function`ã€‚

é€šè¿‡è·Ÿè¸ªå˜é‡åˆ°å¶èŠ‚ç‚¹çš„è·¯å¾„ï¼Œå¯é‡å»ºåˆ›å»ºæ•°æ®çš„æ“ä½œåºåˆ—ï¼Œå¹¶è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

> ç®—æ¢¯åº¦æ—¶ï¼Œæ–¹å‘ä¼ æ’­ï¼Œå¯èƒ½ç”¨åˆ°äº†é“¾å¼æ³•åˆ™ï¼Ÿ

è¡¥å……ï¼šéœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæ•´ä¸ªå›¾åœ¨æ¯æ¬¡è¿­ä»£æ—¶éƒ½æ˜¯ä»å¤´å¼€å§‹é‡æ–°åˆ›å»ºçš„ï¼Œè¿™å°±å…è®¸ä½¿ç”¨ä»»æ„çš„Pythonæ§åˆ¶æµè¯­å¥ï¼Œè¿™æ ·å¯ä»¥åœ¨æ¯æ¬¡è¿­ä»£æ—¶æ”¹å˜å›¾çš„æ•´ä½“å½¢çŠ¶å’Œå¤§å°ã€‚åœ¨å¯åŠ¨è®­ç»ƒä¹‹å‰ä¸å¿…å¯¹æ‰€æœ‰å¯èƒ½çš„è·¯å¾„è¿›è¡Œç¼–ç â€”â€” what you run is what you differentiate.

### Variableä¸Šçš„In-placeæ“ä½œ

åœ¨è‡ªåŠ¨æ±‚å¯¼ä¸­æ”¯æŒin-placeæ“ä½œæ˜¯ä¸€ä»¶å¾ˆå›°éš¾çš„äº‹æƒ…ï¼Œé™¤éæ‚¨çš„å†…å­˜å‹åŠ›å¾ˆå¤§ï¼Œå¦åˆ™æ‚¨å¯èƒ½æ°¸è¿œä¸éœ€è¦ä½¿ç”¨å®ƒä»¬ã€‚

é™åˆ¶in-placeæ“ä½œé€‚ç”¨æ€§ä¸»è¦æœ‰ä¸¤ä¸ªåŸå› ï¼š

ï¼‘ï¼è¦†ç›–æ¢¯åº¦è®¡ç®—æ‰€éœ€çš„å€¼ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå˜é‡ä¸æ”¯æŒ`log_`ã€‚

ï¼’ï¼æ¯ä¸ªin-placeæ“ä½œå®é™…ä¸Šéœ€è¦å®ç°é‡å†™è®¡ç®—å›¾ã€‚

### In-placeæ­£ç¡®æ€§æ£€æŸ¥

æ¯ä¸ªå˜é‡ä¿ç•™æœ‰version counterï¼Œå®ƒæ¯æ¬¡éƒ½ä¼šé€’å¢ï¼Œå½“åœ¨ä»»ä½•æ“ä½œä¸­è¢«ä½¿ç”¨æ—¶ã€‚å½“`Function`ä¿å­˜ä»»ä½•ç”¨äºåå‘çš„tensoræ—¶ï¼Œè¿˜ä¼šä¿å­˜å…¶åŒ…å«å˜é‡çš„version counterã€‚ä¸€æ—¦è®¿é—®`self.saved_tensors`ï¼Œå®ƒå°†è¢«æ£€æŸ¥ï¼Œå¦‚æœå®ƒå¤§äºä¿å­˜çš„å€¼ï¼Œåˆ™ä¼šå¼•èµ·é”™è¯¯ã€‚

## CUDAè¯­ä¹‰

`torch.cuda`ä¼šè®°å½•å½“å‰é€‰æ‹©çš„GPUï¼Œå¹¶ä¸”åˆ†é…çš„æ‰€æœ‰CUDAå¼ é‡å°†åœ¨ä¸Šé¢åˆ›å»ºã€‚å¯ä»¥ä½¿ç”¨`torch.cuda.device`ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ›´æ”¹æ‰€é€‰è®¾å¤‡ã€‚

ä¸€æ—¦å¼ é‡è¢«åˆ†é…ï¼Œå¯¹å…¶è¿›è¡Œæ“ä½œçš„ç»“æœå°†å§‹ç»ˆæ”¾åœ¨ä¸å¼ é‡ç›¸åŒçš„è®¾å¤‡ä¸Šã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸æ”¯æŒè·¨GPUæ“ä½œï¼Œå”¯ä¸€çš„ä¾‹å¤–æ˜¯`copy_()`ã€‚ é™¤éå¯ç”¨å¯¹ç­‰å­˜å‚¨å™¨è®¿é—®ï¼Œå¦åˆ™å¯¹åˆ†å¸ƒä¸åŒè®¾å¤‡ä¸Šçš„å¼ é‡ä»»ä½•å¯åŠ¨æ“ä½œçš„å°è¯•éƒ½å°†ä¼šå¼•å‘é”™è¯¯ã€‚

```python
x = torch.cuda.FloatTensor(1)
# x.get_device() == 0
y = torch.FloatTensor(1).cuda()
# y.get_device() == 0

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.cuda.FloatTensor(1)
    # transfers a tensor from CPU to GPU 1
    b = torch.FloatTensor(1).cuda()
    # a.get_device() == b.get_device() == 1

    c = a + b
    # c.get_device() == 1
    z = x + y
    # z.get_device() == 0

    # even within a context, you can give a GPU id to the .cuda call
    d = torch.randn(2).cuda(2)
    # d.get_device() == 2
```

### æœ€ä½³å®è·µ

#### ä½¿ç”¨å›ºå®šçš„å†…å­˜ç¼“å­˜åŒº

å½“å‰¯æœ¬æ¥è‡ªå›ºå®šï¼ˆé¡µé”ï¼‰å†…å­˜æ—¶ï¼Œä¸»æœºåˆ°GPUçš„å¤åˆ¶é€Ÿåº¦è¦å¿«å¾ˆå¤šã€‚CPUå¼ é‡å’Œå­˜å‚¨å¼€æ”¾äº†ä¸€ä¸ª`pin_memory()`æ–¹æ³•ï¼Œå®ƒè¿”å›è¯¥å¯¹è±¡çš„å‰¯æœ¬ï¼Œè€Œå®ƒçš„æ•°æ®æ”¾åœ¨å›ºå®šåŒºåŸŸä¸­ã€‚

å¦å¤–ï¼Œä¸€æ—¦å›ºå®šäº†å¼ é‡æˆ–å­˜å‚¨ï¼Œå°±å¯ä»¥ä½¿ç”¨å¼‚æ­¥çš„GPUå‰¯æœ¬ã€‚åªéœ€ä¼ é€’ä¸€ä¸ªé¢å¤–çš„`async=True`å‚æ•°åˆ°`cuda()`çš„è°ƒç”¨ã€‚è¿™å¯ä»¥ç”¨äºå°†æ•°æ®ä¼ è¾“ä¸è®¡ç®—é‡å ã€‚

é€šè¿‡å°†`pin_memory=True`ä¼ é€’ç»™å…¶æ„é€ å‡½æ•°ï¼Œå¯ä»¥ä½¿`DataLoader`å°†batchè¿”å›åˆ°å›ºå®šå†…å­˜ä¸­ã€‚

#### ä½¿ç”¨nn.DataParallel æ›¿ä»£ multiprocessing

å¤§å¤šæ•°æ¶‰åŠæ‰¹é‡è¾“å…¥å’Œå¤šä¸ªGPUçš„æƒ…å†µåº”é»˜è®¤ä½¿ç”¨`DataParallel`æ¥ä½¿ç”¨å¤šä¸ªGPUã€‚å°½ç®¡æœ‰GILçš„å­˜åœ¨ï¼Œå•ä¸ªpythonè¿›ç¨‹ä¹Ÿå¯èƒ½ä½¿å¤šä¸ªGPUé¥±å’Œã€‚

## æ‰©å±•PyTorch

æœ¬ç« åŒ…å«å¦‚ä½•æ‰©å±• `torch.nn`, `torch.autograd`å’Œ ä½¿ç”¨æˆ‘ä»¬çš„ `C åº“`ç¼–å†™è‡ªå®šä¹‰çš„`C`æ‰©å±•ã€‚

### æ‰©å±•torch.autograd

å¦‚æœä½ æƒ³è¦æ·»åŠ ä¸€ä¸ªæ–°çš„ `Operation` åˆ°`autograd`çš„è¯ï¼Œä½ çš„`Operation`éœ€è¦ç»§æ‰¿ `class Function`ã€‚`autograd`ä½¿ç”¨`Function`è®¡ç®—ç»“æœå’Œæ¢¯åº¦ï¼ŒåŒæ—¶ç¼–ç  `operation`çš„å†å²ã€‚æ¯ä¸ªæ–°çš„ `operation(function)` éƒ½éœ€è¦å®ç°ä¸‰ä¸ªæ–¹æ³•ï¼š

- `__init__ (optional)` - å¦‚æœä½ çš„`operation`åŒ…å«é`Variable`å‚æ•°ï¼Œé‚£ä¹ˆå°±å°†å…¶ä½œä¸º`__init__`çš„å‚æ•°ä¼ å…¥åˆ°`operation`ä¸­ã€‚ä¾‹å¦‚ï¼š`AddConstant Function`åŠ ä¸€ä¸ªå¸¸æ•°ï¼Œ`Transpose Function`éœ€è¦æŒ‡å®šå“ªä¸¤ä¸ªç»´åº¦éœ€è¦äº¤æ¢ã€‚å¦‚æœä½ çš„`operation`ä¸éœ€è¦é¢å¤–çš„å‚æ•°ï¼Œä½ å¯ä»¥å¿½ç•¥`__init__`ã€‚
- `forward()` - åœ¨é‡Œé¢å†™æ‰§è¡Œæ­¤`operation`çš„ä»£ç ã€‚å¯ä»¥æœ‰ä»»æ„æ•°é‡çš„å‚æ•°ã€‚å¦‚æœä½ å¯¹æŸäº›å‚æ•°æŒ‡å®šäº†é»˜è®¤å€¼ï¼Œåˆ™è¿™äº›å‚æ•°æ˜¯å¯ä¼ å¯ä¸ä¼ çš„ã€‚è®°ä½ï¼š`forward()`çš„å‚æ•°åªèƒ½æ˜¯`Variable`ã€‚å‡½æ•°çš„è¿”å›å€¼æ—¢å¯ä»¥æ˜¯ `Variable`ä¹Ÿå¯ä»¥æ˜¯`Variables`çš„`tuple`ã€‚åŒæ—¶ï¼Œè¯·å‚è€ƒ `Function`çš„ `doc`ï¼ŒæŸ¥é˜…æœ‰å“ªäº› æ–¹æ³•æ˜¯åªèƒ½åœ¨`forward`ä¸­è°ƒç”¨çš„ã€‚
- `backward()` - æ¢¯åº¦è®¡ç®—å…¬å¼ã€‚ å‚æ•°çš„ä¸ªæ•°å’Œ`forward`è¿”å›å€¼çš„ä¸ªæ•°ä¸€æ ·ï¼Œæ¯ä¸ªå‚æ•°ä»£è¡¨ä¼ å›åˆ°æ­¤`operation`çš„æ¢¯åº¦. `backward()`çš„è¿”å›å€¼çš„ä¸ªæ•°åº”è¯¥å’Œæ­¤`operation`è¾“å…¥çš„ä¸ªæ•°ä¸€æ ·ï¼Œæ¯ä¸ªè¿”å›å€¼å¯¹åº”äº†è¾“å…¥å€¼çš„æ¢¯åº¦ã€‚å¦‚æœ`operation`çš„è¾“å…¥ä¸éœ€è¦æ¢¯åº¦ï¼Œæˆ–è€…ä¸å¯å¯¼ï¼Œä½ å¯ä»¥è¿”å›`None`ã€‚ å¦‚æœ`forward()`å­˜åœ¨å¯é€‰å‚æ•°ï¼Œä½ å¯ä»¥è¿”å›æ¯”è¾“å…¥æ›´å¤šçš„æ¢¯åº¦ï¼Œåªæ˜¯è¿”å›çš„æ˜¯`None`ã€‚

```python
# Inherit from Function
class Linear(Function):

    # bias is an optional argument
    def forward(self, input, weight, bias=None):
        self.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    # This function has only a single output, so it gets only one gradient
    def backward(self, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = self.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if self.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if self.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and self.needs_input_grad[2]:
            grad_bias = grad_output.sum(0).squeeze(0)

        return grad_input, grad_weight, grad_bias
```

ç°åœ¨ï¼Œä¸ºäº†å¯ä»¥æ›´ç®€å•çš„ä½¿ç”¨è‡ªå®šä¹‰çš„`operation`ï¼Œæˆ‘ä»¬å»ºè®®å°†å…¶ç”¨ä¸€ä¸ªç®€å•çš„ `helper function`åŒ…è£…èµ·æ¥ã€‚ functions:

```python
def linear(input, weight, bias=None):
    # First braces create a Function object. Any arguments given here
    # will be passed to __init__. Second braces will invoke the __call__
    # operator, that will then use forward() to compute the result and
    # return it.
    return Linear()(input, weight, bias)
```

### æ‰©å±•torch.nn

`nn` åŒ…å«ä¸¤ç§æ¥å£ - `modules`å’Œä»–ä»¬çš„`functional`ç‰ˆæœ¬ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ¥å£ï¼Œä½ éƒ½å¯ä»¥æ‰©å±•`nn`ã€‚ä½†æ˜¯æˆ‘ä»¬å»ºè®®ï¼Œåœ¨æ‰©å±•`layer`çš„æ—¶å€™ï¼Œä½¿ç”¨`modules`ï¼Œ å› ä¸º`modules`ä¿å­˜ç€å‚æ•°å’Œ`buffer`ã€‚å¦‚æœä¸éœ€è¦å‚æ•°çš„è¯ï¼Œé‚£ä¹ˆå»ºè®®ä½¿ç”¨`functional`(æ¿€æ´»å‡½æ•°ï¼Œpoolingï¼Œè¿™äº›éƒ½ä¸éœ€è¦å‚æ•°)ã€‚

å¢åŠ ä¸€ä¸ª`operation`çš„ `functional`ç‰ˆæœ¬å·²ç»åœ¨ä¸Šé¢ä¸€èŠ‚ä»‹ç»å®Œæ¯•ã€‚

å¢åŠ ä¸€ä¸ªæ¨¡å—(`module`)ã€‚ ç”±äº`nn`é‡åº¦ä½¿ç”¨`autograd`ã€‚æ‰€ä»¥ï¼Œæ·»åŠ ä¸€ä¸ªæ–°`module`éœ€è¦å®ç°ä¸€ä¸ª ç”¨æ¥æ‰§è¡Œ è®¡ç®— å’Œ è®¡ç®—æ¢¯åº¦ çš„`Function`ã€‚ä»ç°åœ¨å¼€å§‹ï¼Œå‡å®šæˆ‘ä»¬æƒ³è¦å®ç°ä¸€ä¸ª`Linear module`ï¼Œè®°å¾—ä¹‹å‰æˆ‘ä»¬å·²ç»å®ç°äº†ä¸€ä¸ª`Linear Funciton`ã€‚ åªéœ€è¦å¾ˆå°‘çš„ä»£ç å°±å¯ä»¥å®Œæˆè¿™ä¸ªå·¥ä½œã€‚ ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸¤ä¸ªæ–¹æ³•ï¼š

- `__init__ (optional)` - è¾“å…¥å‚æ•°ï¼Œä¾‹å¦‚`kernel sizes`, `numbers of features`, ç­‰ç­‰ã€‚åŒæ—¶åˆå§‹åŒ– `parameters`å’Œ`buffers`ã€‚
- `forward()` - å®ä¾‹åŒ–ä¸€ä¸ªæ‰§è¡Œ`operation`çš„`Function`ï¼Œä½¿ç”¨å®ƒæ‰§è¡Œ`operation`ã€‚å’Œ`functional wrapper(ä¸Šé¢å®ç°çš„é‚£ä¸ªç®€å•çš„wrapper)`ååˆ†ç±»ä¼¼ã€‚

```python
class Linear(nn.Module):
    def __init__(self, input_features, output_features, bias=True):
        self.input_features = input_features
        self.output_features = output_features

        # nn.Parameter is a special kind of Variable, that will get
        # automatically registered as Module's parameter once it's assigned
        # as an attribute. Parameters and buffers need to be registered, or
        # they won't appear in .parameters() (doesn't apply to buffers), and
        # won't be converted when e.g. .cuda() is called. You can use
        # .register_buffer() to register buffers.
        # nn.Parameters can never be volatile and, different than Variables,
        # they require gradients by default.
        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(output_features))
        else:
            # You should always register all possible parameters, but the
            # optional ones can be None if you want.
            self.register_parameter('bias', None)

        # Not a very smart way to initialize weights
        self.weight.data.uniform_(-0.1, 0.1)
        if bias is not None:
            self.bias.data.uniform_(-0.1, 0.1)

    def forward(self, input):
        # See the autograd section for explanation of what happens here.
        return Linear()(input, self.weight, self.bias)
        #æ³¨æ„è¿™ä¸ªLinearæ˜¯ä¹‹å‰å®ç°è¿‡çš„Linear
```

### ç¼–å†™è‡ªå®šä¹‰Cæ‰©å±•

Coming soon. For now you can find an example at [GitHub](https://github.com/pytorch/extension-ffi).

## å¤šè¿›ç¨‹æœ€ä½³å®è·µ

`torch.multiprocessing`æ˜¯Python`multiprocessing`çš„æ›¿ä»£å“ã€‚å®ƒæ”¯æŒå®Œå…¨ç›¸åŒçš„æ“ä½œï¼Œä½†æ‰©å±•äº†å®ƒä»¥ä¾¿é€šè¿‡`multiprocessing.Queue`å‘é€çš„æ‰€æœ‰å¼ é‡å°†å…¶æ•°æ®ç§»åŠ¨åˆ°å…±äº«å†…å­˜ä¸­ï¼Œå¹¶ä¸”åªä¼šå‘å…¶ä»–è¿›ç¨‹å‘é€ä¸€ä¸ªå¥æŸ„ã€‚

> **Note**
>
> å½“`Variable`å‘é€åˆ°å¦ä¸€ä¸ªè¿›ç¨‹æ—¶ï¼Œ`Variable.data`å’Œ`Variable.grad.data`éƒ½å°†è¢«å…±äº«ã€‚

è¿™å…è®¸å®ç°å„ç§è®­ç»ƒæ–¹æ³•ï¼Œå¦‚Hogwildï¼ŒA3Cæˆ–éœ€è¦å¼‚æ­¥æ“ä½œçš„ä»»ä½•å…¶ä»–æ–¹æ³•ã€‚

### å…±äº«CUDAå¼ é‡

ä»…åœ¨Python 3ä¸­ä½¿ç”¨`spawn`æˆ–`forkserver`å¯åŠ¨æ–¹æ³•æ‰æ”¯æŒåœ¨è¿›ç¨‹ä¹‹é—´å…±äº«CUDAå¼ é‡ã€‚

> **Warning**
>
> CUDA APIè¦æ±‚å¯¼å‡ºåˆ°å…¶ä»–è¿›ç¨‹çš„åˆ†é…ï¼Œåªè¦å®ƒä»¬è¢«ä½¿ç”¨å°±è¦ä¸€ç›´ä¿æŒæœ‰æ•ˆã€‚æ‚¨åº”è¯¥å°å¿ƒï¼Œç¡®ä¿æ‚¨å…±äº«çš„CUDAå¼ é‡åªè¦æœ‰å¿…è¦å°±ä¸è¦è¶…å‡ºèŒƒå›´ã€‚è¿™ä¸æ˜¯å…±äº«æ¨¡å‹å‚æ•°çš„é—®é¢˜ï¼Œä½†ä¼ é€’å…¶ä»–ç±»å‹çš„æ•°æ®åº”è¯¥å°å¿ƒã€‚æ³¨æ„ï¼Œæ­¤é™åˆ¶ä¸é€‚ç”¨äºå…±äº«CPUå†…å­˜ã€‚

å‚è€ƒï¼š[ä½¿ç”¨ nn.DataParallel æ›¿ä»£ multiprocessing](https://pytorch-cn.readthedocs.io/zh/latest/notes/cuda/)

### æœ€ä½³å®è·µå’Œæç¤º

#### é¿å…å’ŒæŠµåˆ¶æ­»é”

#### é‡ç”¨ç»è¿‡é˜Ÿåˆ—çš„ç¼“å†²åŒº

#### å¼‚æ­¥å¤šè¿›ç¨‹è®­ç»ƒï¼ˆä¾‹å¦‚Hogwildï¼‰

## åºåˆ—åŒ–è¯­ä¹‰

### ä¿å­˜æ¨¡å‹çš„æ¨èæ–¹æ³•

è¿™ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•åºåˆ—åŒ–å’Œæ¢å¤æ¨¡å‹ã€‚

ç¬¬ä¸€ç§ï¼ˆæ¨èï¼‰ï¼šåªä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ğŸŒŸ

ä¿å­˜ï¼š

```python
torch.save(the_model.state_dict(), PATH)
```

åŠ è½½ï¼š

```python
the_model = TheModelClass(*args, **kwargs)
the_model.load_state_dict(torch.load(PATH))
```

---

ç¬¬äºŒç§ï¼šä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹

ä¿å­˜ï¼š

```python
torch.save(the_model, PATH)
```

åŠ è½½ï¼š

```python
the_model = torch.load(PATH)
```

ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåºåˆ—åŒ–çš„æ•°æ®è¢«ç»‘å®šåˆ°ç‰¹å®šçš„ç±»å’Œå›ºå®šçš„ç›®å½•ç»“æ„ï¼Œæ‰€ä»¥å½“åœ¨å…¶ä»–é¡¹ç›®ä¸­ä½¿ç”¨æ—¶ï¼Œæˆ–è€…åœ¨ä¸€äº›ä¸¥é‡çš„é‡æ„å™¨ä¹‹åå®ƒå¯èƒ½ä¼šä»¥å„ç§æ–¹å¼breakã€‚

# Packageå‚è€ƒ

## torch

### å¼ é‡ Tensors

#### torch.is_tensor

```python
torch.is_tensor(obj)
```

å¦‚æœ*obj* æ˜¯ä¸€ä¸ªpytorchå¼ é‡ï¼Œåˆ™è¿”å›True

- å‚æ•°ï¼š obj (Object) â€“ åˆ¤æ–­å¯¹è±¡

#### torch.is_storage

```python
torch.is_storage(obj)
```

å¦‚ä½•*obj* æ˜¯ä¸€ä¸ªpytorch storageå¯¹è±¡ï¼Œåˆ™è¿”å›True

- å‚æ•°ï¼š obj (Object) â€“ åˆ¤æ–­å¯¹è±¡

#### torch.set_default_tensot_type

#### torch.numel

```python
torch.numel(input)->int
```

è¿”å›`input` å¼ é‡ä¸­çš„å…ƒç´ ä¸ªæ•°

- å‚æ•°: input (*Tensor*) â€“ è¾“å…¥å¼ é‡

ä¾‹å­:

```python
>>> a = torch.randn(1,2,3,4,5)
>>> torch.numel(a)
120
>>> a = torch.zeros(4,4)
>>> torch.numel(a)
16
```

#### torch.set_printoptions

```python
torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)
```

è®¾ç½®æ‰“å°é€‰é¡¹ã€‚ å®Œå…¨å‚è€ƒè‡ª[ Numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html)ã€‚

å‚æ•°: 

- precision â€“ æµ®ç‚¹æ•°è¾“å‡ºçš„ç²¾åº¦ä½æ•° (é»˜è®¤ä¸º8 )
- threshold â€“ é˜ˆå€¼ï¼Œè§¦å‘æ±‡æ€»æ˜¾ç¤ºè€Œä¸æ˜¯å®Œå…¨æ˜¾ç¤º(repr)çš„æ•°ç»„å…ƒç´ çš„æ€»æ•° ï¼ˆé»˜è®¤ä¸º1000ï¼‰
- edgeitems â€“ æ±‡æ€»æ˜¾ç¤ºä¸­ï¼Œæ¯ç»´ï¼ˆè½´ï¼‰ä¸¤ç«¯æ˜¾ç¤ºçš„é¡¹æ•°ï¼ˆé»˜è®¤å€¼ä¸º3ï¼‰
- linewidth â€“ ç”¨äºæ’å…¥è¡Œé—´éš”çš„æ¯è¡Œå­—ç¬¦æ•°ï¼ˆé»˜è®¤ä¸º80ï¼‰ã€‚Thresholded matricies will ignore this parameter.
- profile â€“ prettyæ‰“å°çš„å®Œå…¨é»˜è®¤å€¼ã€‚ å¯ä»¥è¦†ç›–ä¸Šè¿°æ‰€æœ‰é€‰é¡¹ (é»˜è®¤ä¸ºshort, full)

### åˆ›å»ºæ“ä½œ Creation Ops

#### torch.eye

```
torch.eye(n, m=None, out=None)
```

è¿”å›ä¸€ä¸ª2ç»´å¼ é‡ï¼Œå¯¹è§’çº¿ä½ç½®å…¨1ï¼Œå…¶å®ƒä½ç½®å…¨0

å‚æ•°: 

- n (int) â€“ è¡Œæ•°
- m (int, *optional*) â€“ åˆ—æ•°.å¦‚æœä¸ºNone,åˆ™é»˜è®¤ä¸º*n*
- out (*Tensor*, *optinal*) - Output tensor

#### torch.from_numpy

```
torch.from_numpy(ndarray) â†’ Tensor
```

Numpyæ¡¥ï¼Œå°†`numpy.ndarray` è½¬æ¢ä¸ºpytorchçš„ `Tensor`ã€‚ è¿”å›çš„å¼ é‡tensorå’Œnumpyçš„ndarrayå…±äº«åŒä¸€å†…å­˜ç©ºé—´ã€‚ä¿®æ”¹ä¸€ä¸ªä¼šå¯¼è‡´å¦å¤–ä¸€ä¸ªä¹Ÿè¢«ä¿®æ”¹ã€‚è¿”å›çš„å¼ é‡ä¸èƒ½æ”¹å˜å¤§å°ã€‚

ä¾‹å­:

```
>>> a = numpy.array([1, 2, 3])
>>> t = torch.from_numpy(a)
>>> t
torch.LongTensor([1, 2, 3])
>>> t[0] = -1
>>> a
array([-1,  2,  3])
```

#### torch.linspace

```python
torch.linspace(start, end, steps=100, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ª1ç»´å¼ é‡ï¼ŒåŒ…å«åœ¨åŒºé—´`start` å’Œ `end` ä¸Šå‡åŒ€é—´éš”çš„`steps`ä¸ªç‚¹ã€‚ è¾“å‡º1ç»´å¼ é‡çš„é•¿åº¦ä¸º`steps`ã€‚

å‚æ•°:

- start (float) â€“ åºåˆ—çš„èµ·å§‹ç‚¹
- end (float) â€“ åºåˆ—çš„æœ€ç»ˆå€¼
- steps (int) â€“ åœ¨`start` å’Œ `end`é—´ç”Ÿæˆçš„æ ·æœ¬æ•°
- out (Tensor, optional) â€“ ç»“æœå¼ é‡

ä¾‹å­:

```
>>> torch.linspace(3, 10, steps=5)

  3.0000
  4.7500
  6.5000
  8.2500
 10.0000
[torch.FloatTensor of size 5]

>>> torch.linspace(-10, 10, steps=5)

-10
 -5
  0
  5
 10
[torch.FloatTensor of size 5]

>>> torch.linspace(start=-10, end=10, steps=5)

-10
 -5
  0
  5
 10
[torch.FloatTensor of size 5]
```

#### torch.logspace

```
torch.logspace(start, end, steps=100, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ª1ç»´å¼ é‡ï¼ŒåŒ…å«åœ¨åŒºé—´ 10start å’Œ 10endä¸Šä»¥å¯¹æ•°åˆ»åº¦å‡åŒ€é—´éš”çš„`steps`ä¸ªç‚¹ã€‚ è¾“å‡º1ç»´å¼ é‡çš„é•¿åº¦ä¸º`steps`ã€‚ 

å‚æ•°:

- start (float) â€“ åºåˆ—çš„èµ·å§‹ç‚¹
- end (float) â€“ åºåˆ—çš„æœ€ç»ˆå€¼
- steps (int) â€“ åœ¨`start` å’Œ `end`é—´ç”Ÿæˆçš„æ ·æœ¬æ•°
- out (Tensor, optional) â€“ ç»“æœå¼ é‡

ä¾‹å­:

```
>>> torch.logspace(start=-10, end=10, steps=5)

 1.0000e-10
 1.0000e-05
 1.0000e+00
 1.0000e+05
 1.0000e+10
[torch.FloatTensor of size 5]

>>> torch.logspace(start=0.1, end=1.0, steps=5)

  1.2589
  2.1135
  3.5481
  5.9566
 10.0000
[torch.FloatTensor of size 5]
```

#### torch.ones

```
torch.ones(*sizes, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ªå…¨ä¸º1 çš„å¼ é‡ï¼Œå½¢çŠ¶ç”±å¯å˜å‚æ•°`sizes`å®šä¹‰ã€‚

å‚æ•°:

- sizes (int...) â€“ æ•´æ•°åºåˆ—ï¼Œå®šä¹‰äº†è¾“å‡ºå½¢çŠ¶
- out (Tensor, optional) â€“ ç»“æœå¼ é‡ ä¾‹å­:

```
>>> torch.ones(2, 3)

 1  1  1
 1  1  1
[torch.FloatTensor of size 2x3]

>>> torch.ones(5)

 1
 1
 1
 1
 1
[torch.FloatTensor of size 5]
```

#### torch.rand

```
torch.rand(*sizes, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†ä»åŒºé—´[0,1)çš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„ä¸€ç»„éšæœºæ•°ï¼Œå½¢çŠ¶ç”±å¯å˜å‚æ•°`sizes` å®šä¹‰ã€‚

å‚æ•°:

- sizes (int...) â€“ æ•´æ•°åºåˆ—ï¼Œå®šä¹‰äº†è¾“å‡ºå½¢çŠ¶
- out ([*Tensor*](http://pytorch.org/docs/tensors.html#torch.Tensor), *optinal*) - ç»“æœå¼ é‡ ä¾‹å­ï¼š

```
>>> torch.rand(4)

 0.9193
 0.3347
 0.3232
 0.7715
[torch.FloatTensor of size 4]

>>> torch.rand(2, 3)

 0.5010  0.5140  0.0719
 0.1435  0.5636  0.0538
[torch.FloatTensor of size 2x3]
```

#### torch.randn

```
torch.randn(*sizes, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ(å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º 1ï¼Œå³é«˜æ–¯ç™½å™ªå£°)ä¸­æŠ½å–ä¸€ç»„éšæœºæ•°ï¼Œå½¢çŠ¶ç”±å¯å˜å‚æ•°`sizes`å®šä¹‰ã€‚ å‚æ•°:

- sizes (int...) â€“ æ•´æ•°åºåˆ—ï¼Œå®šä¹‰äº†è¾“å‡ºå½¢çŠ¶
- out ([*Tensor*](http://pytorch.org/docs/tensors.html#torch.Tensor), *optinal*) - ç»“æœå¼ é‡

ä¾‹å­ï¼š:

```
>>> torch.randn(4)

-0.1145
 0.0094
-1.1717
 0.9846
[torch.FloatTensor of size 4]

>>> torch.randn(2, 3)

 1.4339  0.3351 -1.0999
 1.5458 -0.9643 -0.3558
[torch.FloatTensor of size 2x3]
```

#### torch.randperm

```
torch.randperm(n, out=None) â†’ LongTensor
```

ç»™å®šå‚æ•°`n`ï¼Œè¿”å›ä¸€ä¸ªä»`0` åˆ°`n -1` çš„éšæœºæ•´æ•°æ’åˆ—ã€‚

å‚æ•°:

- n (int) â€“ ä¸Šè¾¹ç•Œ(ä¸åŒ…å«)

ä¾‹å­ï¼š

```
>>> torch.randperm(4)

 2
 1
 3
 0
[torch.LongTensor of size 4]
```

#### torch.arange

```
torch.arange(start, end, step=1, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ª1ç»´å¼ é‡ï¼Œé•¿åº¦ä¸º floor((endâˆ’start)/step)ã€‚åŒ…å«ä»`start`åˆ°`end`ï¼Œä»¥`step`ä¸ºæ­¥é•¿çš„ä¸€ç»„åºåˆ—å€¼(é»˜è®¤æ­¥é•¿ä¸º1)ã€‚

å‚æ•°:

- start (float) â€“ åºåˆ—çš„èµ·å§‹ç‚¹
- end (float) â€“ åºåˆ—çš„ç»ˆæ­¢ç‚¹
- step (float) â€“ ç›¸é‚»ç‚¹çš„é—´éš”å¤§å°
- out (Tensor, optional) â€“ ç»“æœå¼ é‡

ä¾‹å­ï¼š

```
>>> torch.arange(1, 4)

 1
 2
 3
[torch.FloatTensor of size 3]

>>> torch.arange(1, 2.5, 0.5)

 1.0000
 1.5000
 2.0000
[torch.FloatTensor of size 3]
```

#### torch.zeros

```
torch.zeros(*sizes, out=None) â†’ Tensor
```

è¿”å›ä¸€ä¸ªå…¨ä¸ºæ ‡é‡ 0 çš„å¼ é‡ï¼Œå½¢çŠ¶ç”±å¯å˜å‚æ•°`sizes` å®šä¹‰ã€‚

å‚æ•°:

- sizes (int...) â€“ æ•´æ•°åºåˆ—ï¼Œå®šä¹‰äº†è¾“å‡ºå½¢çŠ¶
- out ([Tensor](http://pytorch.org/docs/tensors.html#torch.Tensor), *optional*) â€“ ç»“æœå¼ é‡

ä¾‹å­ï¼š

```python
>>> torch.zeros(2, 3)

 0  0  0
 0  0  0
[torch.FloatTensor of size 2x3]

>>> torch.zeros(5)

 0
 0
 0
 0
 0
[torch.FloatTensor of size 5]
```

### ç´¢å¼•ï¼Œåˆ‡ç‰‡ï¼Œè¿æ¥ï¼Œæ¢ä½ Indexingï¼ŒSlicingï¼ŒJoiningï¼ŒOps



### éšæœºæŠ½æ · Random sampling

### åºåˆ—åŒ– Serialization

### æ•°å­¦æ“ä½œ Math operations

### Pointwise Ops

### Reduction Ops

### æ¯”è¾ƒæ“ä½œ Comparison Ops

### å…¶ä»–æ“ä½œ Other Operations

### BLAS and LAPACK Operations



## torch.Tensor



## torch.Storage



## torch.nn



## torch.nn.functional



## torch.nn.init



## torch.optim



## torch.autograd



## torch.multiprocessing



## torch.legacy



## torch.cuda



## torch.utils.ffi



## torch.utils.data



## torch.utils.model_zoo



# torchvisionå‚è€ƒ

## torchvision



## torchvision.datasets



## torchvision.models



## torchvision.transforms



## torchvision.utils

