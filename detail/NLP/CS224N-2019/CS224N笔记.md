# 1-Word Vector

NLP：nature language processing

## 如何表达一个单词的含义？

### 1.建立所有同义词synonym和下义词hypernym的词库

例：WordNet

### 2.独热编码（one-hot） 是一种spare vector

向量的维度就等于词库中的单词数目

### 3.Word2vec算法  是一种dense vector

direct prediction

#### (1)Skip-Gram model

Skip-Gram,求context word相对于center word的条件概率

![](pic/Word2vec.png)

每个单词w由两个向量表示:作为中心词center word $u_w$ ，作为上下文词context word $v_w$

#### (2)Continuous Bag of Words，简称CBOW

CBOW,求center相对于context word的条件概率

# 2-GloVe

## SVD（奇异值分解）模型

Count based模型的经典代表

通常我们先扫过一遍所有数据，然后得到单词同时出现的矩阵co-occurrence matrix，假设矩阵用X表示，然后我们对其进行SVD得到X的分解形式

### co-occurrence matrix

（1）word-document co-occurrence matrix

（2）window-based word-word co-occurrence matrix

## GloVe算法

结合以上两种

# 3、4-命名实体识别NER

**Named Entity Recognition**

# 5-Dependency Parsing

constituency parsing(成分句法分析)

dependency parsing(依存句法分析)

## Transition-based Dependency Parsing

以划分句子结构为中心

只有树的叶子节点是句子成分

## Neural Dependency Parsing

以动词为中心

# 6-语言模型与RNN

## 语言模型

研究的是根据已知序列推测下一个单词的问题

语言模型应用广泛，比如手机上打字可以智能预测下一个你要打的词是什么

语言模型是很多涉及到产生文字或预测文字概率的NLP问题的组成部分，如语音识别、手写文字识别、自动纠错、机器翻译等等

## 经典n-gram模型

n-gram 连续的n个单词

![](pic/n-gram例子.png)

问题：

1. 稀疏问题 Sparsity Problem

   在我们之前的大量文本中，可能分子或分母的组合没有出现过，则其计数为零。并且随着n的增大，稀疏性更严重。

2. 我们必须存储所有的n-gram对应的计数，随着n的增大，模型存储量也会增大。

## Window-based DNN

将定长窗口中的word embedding连在一起，将其经过神经网络做对下一个单词的分类预测，其类的个数为语料库中的词汇量

![](pic/Window-based_DNN.png)

## RNN

RNN(Recurrent Neural Network)结构通过不断的应用同一个矩阵W可实现参数的有效共享，并且没有固定窗口的限制（可处理任意长度）。

![](pic/RNN例子.png)

![](pic/RNNLM例子.png)

## 7-梯度消失、LSTM与GRU

### 梯度消失&梯度爆炸

vanishing gradient

exploding gradient

Clip your gradients！！！

### LSTM

Long Short Term Memory

![](pic/LSTM.png)

![](pic/LSTM2.png)

### GRU

GRU(gated recurrent unit)可以看做是将LSTM中的forget gate和input gate合并成了一个update gate

![](pic/GRU.png)

### Bidirectional RNNs

### Multi-layer RNNs

# 8-机器翻译、Seq2Seq与Attention

## 机器翻译

### SMT

Statistical Machine Translation

核心思想是从数据中学习出概率模型。

### NMT

Neural Machine Translation

NMT依赖于Sequence-to-Sequence的模型架构，即通过一个RNN作为encoder将输入的源语言转化为某表征空间中的向量，再通过另一个RNN作为decoder将其转化为目标语言中的句子。

## Seq2Seq

## BLEU

对于机器翻译模型，我们如何来衡量它的好坏呢？一个常用的指标是BLEU（全称是**B**i**l**ingual **E**valuation **U**nderstudy)。BLEU基本思想是看你machine translation中n-gram在reference translation（人工翻译作为reference）中相应出现的几率。

## Attention

# 9-项目实战经验

## 性能指标

### 准确率（Precision）

precision = TP/(TP + FP)

模型作出正例预测时正确的概率

### 召回率（Recall）

recall = TP/(TP + FN)

对于所有实际上正确的例子，模型检测到的比例。

# 10-问答系统

## SQuAD数据集

Stanford Question Answering Dataset

## Stanford Attentive Reader模型

Manning组的关于QA的模型

## BiDAF模型

其核心思想是Attention应该是双向的，既有从Context（即passage）到Query（即Question）的attention,又有从Query到Context的attention。

# 11-NLP中的CNN

RNN的问题：(不利用Attention的情况下），通常我们用最后的hidden vector来表示整个句子的所有信息，这就造成了信息的瓶颈。

例子：

![](pic/A_1D_convolutional_for_text.png)

![](pic/3_channel_1D_convolution.png)

## Single Layer CNN for Sentence Classification

![CS224N笔记(十一):NLP中的CNN](pic/CNN_for_sentence_classification.png)

# 12-Subword模型

## Character-Level Model

将字符作为基本单元

![](pic/Character_Level.jpg)

另：Revisiting Character-Based Neural Machine Translation with Capacity and Compression

## Byte Pair Encoding与SentencePiece

基本单元介于字符与单词之间的模型称作Subword Model

一种方法是Byte Pair Encoding,简称BPE。 BPE最早是一种压缩算法，基本思路是把经常出现的byte pair用一个新的byte来代替，例如假设('A', ’B‘）经常顺序出现，则用一个新的标志'AB'来代替它们。

谷歌的NMT模型用了BPE的变种，称作wordpiece model，BPE中利用了n-gram count来更新词汇库，而wordpiece model中则用了一种贪心算法来最大化语言模型概率，即选取新的n-gram时都是选择使得perplexity减少最多的ngram。进一步的，sentencepiece model将词间的空白也当成一种标记，可以直接处理sentence，而不需要将其pre-tokenize成单词。

## Hybrid Model

还有一种思路是在大多数情况下我们还是采用word level模型，而只在遇到OOV的情况才采用character level模型。

![](pic/Hyper_Model.jpg)

## FastText

# 13-ELMO,GPT与BERT

![CS224N笔记(十三):ELMO, GPT与BERT](pic/Contextual_Word_Embeddings.jpg)

## ELMO

ELMO的基本思想是利用双向的LSTM结构，对于某个语言模型的目标，在大量文本上进行预训练，从LSTM layer中得到contextual embedding，其中较低层的LSTM代表了比较简单的语法信息，而上层的LSTM捕捉的是依赖于上下文的语义信息。ELMO的全称就是Embeddings from Language Models。对于下游的任务，再将这些不同层的向量线性组合，再做监督学习。

## GPT

GPT全称是Generative Pre-Training, 和之后的BERT模型一样，它的基本结构也是Transformer



## BERT

BERT原理与GPT有相似之处，不过它利用了双向的信息，因而其全称是Bidirectional Encoder Representations from Transformers。

BERT做无监督的pre-training时有两个目标：

- 一个是将输入的文本中 k%的单词遮住，然后预测被遮住的是什么单词。
- 另一个是预测一个句子是否会紧挨着另一个句子出现。

预训练时在大量文本上对这两个目标进行优化，然后再对特定任务进行fine-tuning。

BERT由于采用了Transformer结构能够更好的捕捉全局信息，并且利用了上下文的双向信息，所以其效果要优于之前的方法，它大大提高了各项NLP任务的性能。

ELMO论文[https://arxiv.org/abs/1802.05365](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.05365)

GPT openAI blog [https://openai.com/blog/language-unsupervised/](https://link.zhihu.com/?target=https%3A//openai.com/blog/language-unsupervised/)

BERT论文[https://arxiv.org/abs/1810.04805](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805)

# 14-Transformers and Self- Attention

![](pic/Self_Attention.png)

# 15-自然语言生成

NLG Natural Language Generation

## 回顾语言模型

语言模型就是给定句子中的一部分词，要求预测下一个词是什么。形式化表述就是预测P(yt|y1,…,yt−1)，其中的y1,…,yt−1就是目前已知的词，yt就是要预测的下一个词。

条件语言模型是指除了已知y1,…,yt−1，还给定了x，这个x就是提供给语言模型的额外的信息。比如机器翻译的x就是源语言的句子信息；自动摘要的x就是输入的长文；对话系统的x就是历史对话内容等。

语言模型的Decoder：

1. Greedy search
2. Beam search
3. Pure sampling
4. Softmax temperature

## NLG子任务

自动摘要

问答系统

## NLG评价

目前来说，对于NLG任务，还没有一个能衡量生成句子总体好坏的Overall的指标，但是可以用一些细分指标，重点关注生成文本在某方面的表现，比如流畅度、多样性、相关性等。

## NLG趋势



# 16-指代消解
Coreference Resolution

指代（mention）是指句子中出现的名词、名词短语、代词等，指代消解就是把指向同一实体（entity）的指代聚类到一起的过程。



指代消解主要有两个步骤。

第一步是指代识别（mention detection），即找出句子中所有的指代，这一步相对简单。

第二步才是进行真正的指代消解（coreference resolution），这一步比较难，也是本节课的主要内容。

指代消解发展至今，经历了四种不同的方法：

1. Rule-based
2. Mention pair
3. Mention Ranking
4. Clustering

前面的内容都是假设我们计算好了任意两个指代是coreference的概率，那么，如何来计算这个概率呢？主要有三种方法

1. Non-neural statistical classifier
2. Simple neural network
3. More advanced model using LSTMs, attention

# 17-多任务学习

decaNLP

![](pic/MQAN.png)













