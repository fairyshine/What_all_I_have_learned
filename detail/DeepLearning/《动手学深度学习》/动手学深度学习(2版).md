# 01 è¯¾ç¨‹å®‰æ’

## å†…å®¹

â€¢æ·±åº¦å­¦ä¹ åŸºç¡€ä¸€çº¿æ€§ç¥ç»ç½‘ç»œï¼Œå¤šå±‚æ„ŸçŸ¥æœº

â€¢å·ç§¯ç¥ç»ç½‘ç»œâ€” LeNet, AlexNet, VGG, Inception, ResNet 

â€¢å¾ªç¯ç¥ç»ç½‘ç»œâ€”RNN, GRU, LSTM, seq2seq 

â€¢æ³¨æ„åŠ›æœºåˆ¶ï¼ Attention. Transformer 

â€¢ ä¼˜åŒ–ç®—æ³•ä¸€ SGD, Momentum, Adam 

â€¢é«˜æ€§èƒ½è®¡ç®—ä¸€å¹¶è¡Œï¼Œå¤šGPUï¼Œåˆ†å¸ƒå¼ 

â€¢ è®¡ç®—æœºè§†è§‰ ç›®æ ‡æ£€æµ‹ï¼Œè¯­ä¹‰åˆ†å‰² 

â€¢è‡ªç„¶è¯­è¨€å¤„ç†ä¸€è¯åµŒå…¥ï¼ŒBERT

## ä¼šå­¦åˆ°ä»€ä¹ˆ

æ·±åº¦å­¦ä¹ æœ‰å“ªäº›æŠ€æœ¯

å¦‚ä½•å®ç°å’Œè°ƒå‚

èƒŒåçš„åŸå› ï¼ˆç›´è§‰ã€æ•°å­¦ï¼‰

# 02 æ·±åº¦å­¦ä¹ ä»‹ç»

![AIåœ°å›¾](pic/AIåœ°å›¾.png)

# 03 å®‰è£…

conda ç¯å¢ƒ

pip install -y jupyter d2l torch torchvision ï¼ˆå®æµ‹d2låŒ…åªåœ¨pipä¸­ï¼Œä¸åœ¨condaä¸­ï¼‰

jupyter:

jupyterlab ä½œä¸ºmacçš„æ‰©å±•ï¼Œç”¨brewçš„åŒ…å®‰è£…

jupyter ä½œä¸ºpythonçš„æ‰©å±•ï¼Œç”¨pipã€condaå®‰è£…



Jupiter notebook å¯åŠ¨è®°äº‹æœ¬åº”ç”¨



# 04 æ•°æ®æ“ä½œ+æ•°æ®é¢„å¤„ç†

ç”¨åˆ°numpy, matplotlib, pandas, os

## Nç»´æ•°ç»„

### æ ·ä¾‹

Nç»´æ•°ç»„æ˜¯æœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„ä¸»è¦æ•°æ®ç»“æ„

â€‹	ä¸€ç»´ï¼š[]    äºŒç»´ï¼š[[ ] [ ]] â€¦â€¦

æ ·ä¾‹ï¼š0-d ä¸€ä¸ªç±»åˆ«  1-d ä¸€ä¸ªç‰¹å¾å‘é‡   2-d ä¸€ä¸ªæ ·æœ¬-ç‰¹å¾çŸ©é˜µ

3-d RGBå›¾ç‰‡  4-d ä¸€ä¸ªRGBå›¾ç‰‡æ‰¹é‡ 5-d ä¸€ä¸ªè§†é¢‘æ‰¹é‡

### åˆ›å»ºæ•°ç»„

1.å½¢çŠ¶ 2.æ¯ä¸ªå…ƒç´ çš„æ•°æ®ç±»å‹ 3.æ¯ä¸ªå…ƒç´ çš„å€¼

### è®¿é—®å…ƒç´ 

æŸå…ƒç´ ã€æŸè¡Œã€æŸåˆ—ã€æŸåŒºåŸŸ

## æ•°æ®æ“ä½œ

```python
import torch
x = torch.arange(12)
x.shape   #å½¢çŠ¶
x.numel   #æ€»æ•°
x.reshape(3,4) #æ”¹å˜å½¢çŠ¶è€Œä¸æ”¹å˜å…ƒç´ æ•°é‡å’Œå€¼
torch.zeros((x,y,z,â€¦â€¦))  #å…¨0æ•°ç»„
torch.tensor  #ç‰¹å®šå€¼æ•°ç»„
torch.cat((X,Y),dim=) #å¤šä¸ªå¼ é‡è¿ç»“åœ¨ä¸€èµ·
X==Y #æŒ‰é€»è¾‘è¿ç®—ç¬¦æ„å»ºäºŒå…ƒå¼ é‡

#è½¬æ¢NumPyå¼ é‡
A=X.numpy()
B=torch.tensor(A)
```

å¼ é‡è¡¨ç¤ºä¸€ä¸ªæ•°å€¼ç»„æˆçš„æ•°ç»„ï¼Œè¿™ä¸ªæ•°ç»„å¯èƒ½æœ‰å¤šä¸ªç»´åº¦

æ ‡å‡†è¿ç®—ç¬¦æŒ‰ç…§å…ƒç´ è¿ç®—ï¼ˆå³ä½¿å½¢çŠ¶ä¸åŒï¼Œé€šè¿‡è°ƒç”¨å¹¿æ’­æœºåˆ¶æ¥æ‰§è¡Œ



**æ³¨æ„**ï¼šè¿è¡Œä¸€äº›æ“ä½œå¯èƒ½ä¼šå¯¼è‡´ä¸ºæ–°ç»“æœåˆ†é…å†…å­˜

pythonçš„idï¼ˆç±»ä¼¼äºc++çš„æŒ‡é’ˆ

Y = Y + X åï¼ŒYçš„idå˜äº† => Y += X

## æ•°æ®é¢„å¤„ç†

```python
#åˆ›å»ºæ•°æ®é›†ï¼Œå­˜å‚¨åœ¨CSV
import os
os.makedirs(os.path.join('..','data'),exist_ok=True)
data_file=os.path.join('..','data','house_tiny.csv')
with open(data_file,'w')as f:
  f.write(',,\n')
  f.write(',,\n')
  
#ä»CSVä¸­åŠ è½½æ•°æ®é›†
import pandas as pd
data=pd.read_csv(data_file)

#å¤„ç†ç¼ºå¤±æ•°æ® æ’å€¼å’Œåˆ é™¤
inputs=inputs.fillna(inputs.mean()) #ç”¨å‡å€¼å¡«å……
inputs=pd.get_dummies(inputs,dummy_na=True) #å¯¹äºç±»åˆ«å€¼æˆ–ç¦»æ•£å€¼ï¼Œæ›¿æ¢ä¸º0ã€1 åˆ†ç±»å€¼
```

# 05 çº¿æ€§ä»£æ•°

## çº¿æ€§ä»£æ•°

å‘é‡

çŸ©é˜µ    èŒƒæ•°

## çº¿æ€§ä»£æ•°å®ç°

```python
A.T #çŸ©é˜µAçš„è½¬ç½®
A @ B #çŸ©é˜µä¹˜æ³•  torch.mm
A * B #çŸ©é˜µæŒ‰å…ƒç´ ä¹˜æ³•
sum  #æ±‚å’Œ  å‚æ•° keepdims=Trueä¿æŒè½´æ•°ä¸å˜
mean average
cumsum #ç´¯åŠ æ±‚å’Œ
torch.dot #ç‚¹ç§¯ï¼ŒæŒ‰å…ƒç´ ä¹˜ç§¯çš„å’Œ
torch.mv(A,x) #å‘é‡ç§¯
torch.norm()  #å‘é‡çš„L2èŒƒæ•°
torch.abs(u).sum() #å‘é‡çš„L1èŒƒæ•°
torch.norm() #çŸ©é˜µçš„è´¹ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°ï¼ŒçŸ©é˜µå…ƒç´ çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹
```

## æŒ‰ç‰¹å®šè½´æ±‚å’Œ

Shape[5,4]

axis=0ï¼Œ [5,  ]

axis=1,   [   ,4]

# 06 çŸ©é˜µè¿ç®—

çŸ©é˜µæ€ä¹ˆæ±‚å¯¼æ•°ï¼Ÿ

## æ ‡é‡å¯¼æ•°

## äºšå¯¼æ•°

![](pic/äºšå¯¼æ•°.png)

## çŸ©é˜µå¯¼æ•°

### æ¢¯åº¦

å°†å¯¼æ•°æ‰©å±•åˆ°å‘é‡

*æ ‡é‡/å‘é‡* å…³äº *æ ‡é‡/å‘é‡* çš„å¯¼æ•°

### æ ·ä¾‹

![](pic/å¯¼æ•°æ ·ä¾‹.png)

# 07 è‡ªåŠ¨æ±‚å¯¼

## å‘é‡é“¾å¼æ³•åˆ™

## è‡ªåŠ¨æ±‚å¯¼

è®¡ç®—ä¸€ä¸ªå‡½æ•°åœ¨æŒ‡å®šå€¼ä¸Šçš„å¯¼æ•°

å®ƒæœ‰åˆ«äº ç¬¦å·æ±‚å¯¼ã€æ•°å€¼æ±‚å¯¼

### è®¡ç®—å›¾

å°†ä»£ç åˆ†è§£æˆæ“ä½œå­

å°†è®¡ç®—è¡¨ç¤ºæˆä¸€ä¸ªæ— ç¯å›¾

æ˜¾å¼æ„é€  Tensorflow/Theano/MXNet

éšå¼æ„é€  PyTorch/MXNet

### è‡ªåŠ¨æ±‚å¯¼çš„ä¸¤ç§æ¨¡å¼

é“¾å¼æ³•åˆ™

â€‹	æ­£å‘ç´¯ç§¯

â€‹	åå‘ç´¯ç§¯ã€åˆç§°åå‘ä¼ é€’

## è‡ªåŠ¨æ±‚å¯¼å®ç°

```python
x=torch.arange(4.0)
x.requires_grad_(True) #å­˜å‚¨yå…³äºxçš„å¯¼æ•°
x.grad #é»˜è®¤å€¼æ˜¯None

y.backward() #åå‘ä¼ æ’­å‡½æ•°æ±‚å¯¼

#è®¡ç®—xçš„å¦ä¸€ä¸ªå¯¼æ•°
#é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼
x.grad.zero_()

#æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬çš„ç›®çš„ä¸æ˜¯è®¡ç®—å¾®åˆ†çŸ©é˜µï¼Œè€Œæ˜¯æ‰¹é‡ä¸­æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—çš„åå¯¼æ•°ä¹‹å’Œ
y=x*x #yæ˜¯ä¸€ç»´å¼ é‡
y.sum().backward()  #å°†ä¸€ç»´å¼ é‡è½¬åŒ–ä¸ºæ ‡é‡ï¼Œé¿å…æ±‚å¯¼å‡ºç°çŸ©é˜µ
#æ­¤æ­¥æ³¨æ„ï¼Œä¸è¦çº ç»“äºyåœ¨sumå¤„ç†åçš„æ•°å€¼ï¼Œy=x*xæ‰æ˜¯æ ¸å¿ƒ

u=y.detach() #uä¸æ˜¯å…³äºxçš„å‡½æ•°ï¼Œè€Œæ˜¯æ‹¥æœ‰yçš„å€¼çš„å¸¸æ•°

#æ„é€ å‡½æ•°çš„è®¡ç®—å›¾å¯é€šè¿‡æ§åˆ¶æµï¼ˆæ¡ä»¶åŠå¾ªç¯è¯­å¥ã€å‡½æ•°è°ƒç”¨ï¼‰
```

# 08 çº¿æ€§å›å½’+åŸºç¡€ä¼˜åŒ–ç®—æ³•

## çº¿æ€§å›å½’ğŸŒŸ

### çº¿æ€§æ¨¡å‹

$$
y=w_1x_1+w_2x_2+â€¦â€¦+w_nx_n+b
$$

çº¿æ€§æ¨¡å‹å¯çœ‹ä½œå•å±‚ç¥ç»ç½‘ç»œ

### è¡¡é‡é¢„ä¼°è´¨é‡

å¹³æ–¹æŸå¤±

### è®­ç»ƒæ•°æ®

è®­ç»ƒæ•°æ®ï¼šæ”¶é›†ä¸€äº›æ•°æ®ç‚¹æ¥å†³å®šå‚æ•°å€¼ï¼Œé€šå¸¸è¶Šå¤šè¶Šå¥½

### å‚æ•°å­¦ä¹ 

è®­ç»ƒæŸå¤± => æœ€å°åŒ–æŸå¤±æ¥å­¦ä¹ å‚æ•°

### æ€»ç»“

- çº¿æ€§å›å½’æ˜¯å¯¹nç»´è¾“å…¥çš„åŠ æƒï¼Œå¤–åŠ åå·®
- ä½¿ç”¨å¹³æ–¹æŸå¤±æ¥è¡¡é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å¼‚
- çº¿æ€§å›å½’æœ‰æ˜¾ç¤ºè§£
- çº¿æ€§å›å½’å¯ä»¥çœ‹åšæ˜¯å•å±‚ç¥ç»ç½‘ç»œ

## åŸºç¡€ä¼˜åŒ–ç®—æ³•

### æ¢¯åº¦ä¸‹é™

æŒ‘é€‰ä¸€ä¸ªåˆå§‹å€¼  w_0

é‡å¤è¿­ä»£å‚æ•°

æ²¿æ¢¯åº¦æ–¹å‘å°†å¢åŠ æŸå¤±å‡½æ•°å€¼

å­¦ä¹ ç‡ï¼š*æ­¥é•¿*çš„è¶…å‚æ•° ï¼ˆé€‰å–ï¼Œä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œè¦åˆé€‚ï¼‰

### å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šç®—æ¢¯åº¦å¤ªè´µ

æˆ‘ä»¬å¯ä»¥éšæœºé‡‡æ ·bä¸ªæ ·æœ¬æ¥è¿‘ä¼¼æŸå¤±ï¼ˆbæ˜¯æ‰¹é‡å¤§å°ï¼Œå¦ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°ï¼Œè¦åˆé€‚

### æ€»ç»“

æ¢¯åº¦ä¸‹é™é€šè¿‡ä¸æ–­æ²¿ç€**<u>å</u>**æ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°æ±‚è§£

å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ˜¯æ·±åº¦å­¦ä¹ é»˜è®¤çš„æ±‚è§£ç®—æ³•

ä¸¤ä¸ªé‡è¦çš„è¶…å‚æ•°æ˜¯**å­¦ä¹ ç‡**å’Œ**æ‰¹é‡å¤§å°**

## çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°ğŸŒŸ

æˆ‘ä»¬å°†ä»é›¶å¼€å§‹å®ç°æ•´ä¸ªæ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®æµæ°´çº¿ã€æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™å™¨

è¯­æ³•ç‚¹ï¼š

```python
torch.normal(A, B ,size(C, D), requires_grad=True)
#Aè¡¨ç¤ºå‡å€¼ï¼ŒBè¡¨ç¤ºæ ‡å‡†å·® ,Cä»£è¡¨ç”Ÿæˆçš„æ•°æ®è¡Œæ•°ï¼ŒDè¡¨ç¤ºåˆ—æ•°ï¼Œrequires_grad=Trueè¡¨ç¤ºå¯¹å¯¼æ•°å¼€å§‹è®°å½•ï¼Œå¯ä»¥å¿½ç•¥ã€‚

#1. æ‰€æœ‰çš„tensoréƒ½æœ‰.requires_gradå±æ€§,å¯ä»¥è®¾ç½®è¿™ä¸ªå±æ€§.
x = tensor.ones(2,4,requires_grad=True)
#2.å¦‚æœæƒ³æ”¹å˜è¿™ä¸ªå±æ€§ï¼Œå°±è°ƒç”¨tensor.requires_grad_()æ–¹æ³•ï¼š
x.requires_grad_(False)
```

torch.matmul çŸ©é˜µä¹˜æ³•ï¼Œé«˜ç»´å¹¿æ’­æœºåˆ¶

reshape((-1,1)) æ•°æ®é‡ç»„ï¼Œè¾“å…¥-1ä¼šè‡ªåŠ¨è°ƒæ•´

random.shuffle å°†åºåˆ—çš„æ‰€æœ‰å…ƒç´ éšæœºæ’åº

yield  å¸¦æœ‰ yield çš„å‡½æ•°åœ¨ Python ä¸­è¢«ç§°ä¹‹ä¸º generatorï¼ˆç”Ÿæˆå™¨ï¼‰

with è¯­å¥é€‚ç”¨äºå¯¹èµ„æºè¿›è¡Œè®¿é—®çš„åœºåˆï¼Œç¡®ä¿ä¸ç®¡ä½¿ç”¨è¿‡ç¨‹ä¸­æ˜¯å¦å‘ç”Ÿå¼‚å¸¸éƒ½ä¼šæ‰§è¡Œå¿…è¦çš„â€œæ¸…ç†â€æ“ä½œï¼Œé‡Šæ”¾èµ„æºï¼Œæ¯”å¦‚æ–‡ä»¶ä½¿ç”¨åè‡ªåŠ¨å…³é—­ï¼çº¿ç¨‹ä¸­é”çš„è‡ªåŠ¨è·å–å’Œé‡Šæ”¾ç­‰ã€‚

torch.no_grad ç”¨çœŸå®æ•°æ®åšéªŒè¯é›†è®¡ç®—lossçœ‹è®­ç»ƒæ•ˆæœï¼Œä¸æƒ³æ›´æ–°ç½‘ç»œï¼Œä¸è®¡å…¥æ¢¯åº¦ã€‚

```python
%matplotlib inline  #jupyteræ‰åŠ 
import random
import torch
from d2l import torch as d2l

#æ„é€ ä¸€ä¸ªäººé€ æ•°æ®é›†
def synthetic_data(w,b,num_examples):
  """ç”Ÿæˆy=Xw+b+å™ªå£°ã€‚"""
  X=torch.normal(0,1,(num_examples,len(w)))
  y=torch.matmul(X,w)+b
  y+=torch.normal(0,0.01,y.shape) #éšæœºå™ªéŸ³
  return X,y.reshape((-1,1))

true_w=torch.tensor([2,-3.4])
true_b=4.2
features,labels=synthetic_data(true_w,true_b,1000)

#featuresä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ä¸ªäºŒç»´æ•°æ®æ ·æœ¬ï¼Œlabelsä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ç»´æ ‡ç­¾å€¼
print('features:',features[0],'\nlabel:',labels[0])
d2l.set_figsize()  #ç”»å›¾
d2l.plt.scatter(features[:,(1)].detach().numpy(),
							labels.detach().numpy(),1);
d2l.plt.show() #æ˜¾ç¤ºå›¾ç‰‡ï¼ˆjupyterä¸­ä¸éœ€è¦

#å®šä¹‰ä¸€ä¸ªdata_iterå‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—æ‰¹é‡å¤§å°ã€ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå¤§å°ä¸ºbatch_sizeçš„å°æ‰¹é‡
def data_iter(batch_size,features,labels):
    num_examples=len(features)
    indices=list(range(num_examples))
    #è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices=torch.tensor(indices[i:min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]
  
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
    
#å®šä¹‰åˆå§‹åŒ–æ¨¡å‹å‚æ•°
w=torch.normal(0,0.01,size=(2,1),requires_grad=True)
b=torch.zeros(1,requires_grad=True)

#å®šä¹‰æ¨¡å‹
def linreg(X,w,b):
  """çº¿æ€§å›å½’æ¨¡å‹ã€‚"""
  return torch.matmul(X,w)+b

#å®šä¹‰æŸå¤±å‡½æ•°
def squared_loss(y_hat,y):
  """å‡æ–¹æŸå¤±ã€‚"""
  return (y_hat-y.reshape(y_hat.shape)) ** 2 / 2

#å®šä¹‰ä¼˜åŒ–ç®—æ³•
def sgd(params,lr,batch_size):
  """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚"""
  with torch.no_grad():
    for param in params:
      param-=lr*param.grad/batch_size
      param.grad.zero_()

#è®­ç»ƒè¿‡ç¨‹
lr = 0.03 #å­¦ä¹ ç‡
num_epochs = 3  #è¿­ä»£å‘¨æœŸä¸ªæ•°
net = linreg
loss = squared_loss
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # `X`å’Œ`y`çš„å°æ‰¹é‡æŸå¤±
        # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚`l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ
        # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[`w`, `b`]çš„æ¢¯åº¦
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
        
#æ¯”è¾ƒçœŸå®å‚æ•°å’Œé€šè¿‡è®­ç»ƒå­¦åˆ°çš„å‚æ•°æ¥è¯„ä¼°è®­ç»ƒçš„æˆåŠŸç¨‹åº¦        
print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')
```

## çº¿æ€§å›å½’çš„ç®€æ´å®ç°ğŸŒŸ

é€šè¿‡æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥ç®€æ´åœ°å®ç°çº¿æ€§å›å½’æ¨¡å‹

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

#ç”Ÿæˆæ•°æ®é›†
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

#è°ƒç”¨æ¡†æ¶ä¸­ç°æœ‰çš„apiæ¥è¯»å–æ•°æ®
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨ã€‚"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
next(iter(data_iter))  #iterç”Ÿæˆè¿­ä»£å™¨ï¼Œnextè¿”å›è¿­ä»£å™¨çš„ä¸‹ä¸€ä¸ªé¡¹ç›®

#ä½¿ç”¨æ¡†æ¶çš„é¢„å®šä¹‰å¥½çš„å±‚
# `nn` æ˜¯ç¥ç»ç½‘ç»œçš„ç¼©å†™
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))

#åˆå§‹åŒ–æ¨¡å‹å‚æ•°
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

#è®¡ç®—å‡æ–¹è¯¯å·®ä½¿ç”¨çš„æ˜¯MSELossç±»ï¼Œä¹Ÿç§°ä¸ºå¹³æ–¹èŒƒæ•°
loss = nn.MSELoss()
#å®ä¾‹åŒ–SGDå®ä¾‹
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

#è®­ç»ƒè¿‡ç¨‹
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
    
w = net[0].weight.data
print('wçš„ä¼°è®¡è¯¯å·®ï¼š', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('bçš„ä¼°è®¡è¯¯å·®ï¼š', true_b - b)
```

# 09 Softmaxå›å½’+æŸå¤±å‡½æ•°+å›¾ç‰‡åˆ†ç±»æ•°æ®é›†

## Softmaxå›å½’ ï¼ˆå®è´¨åˆ†ç±»ï¼‰

### å›å½’ VS åˆ†ç±»

å›å½’ä¼°è®¡ä¸€ä¸ªè¿ç»­å€¼

åˆ†ç±»é¢„æµ‹ä¸€ä¸ªç¦»æ•£ç±»åˆ«

### ä»å›å½’åˆ°å¤šç±»åˆ†ç±»â€”å‡æ–¹æŸå¤±

### Softmaxå’Œäº¤å‰ç†µæŸå¤±

### æ€»ç»“

- Softmaxå›å½’æ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»æ¨¡å‹
- ä½¿ç”¨Softmaxæ“ä½œå­å¾—åˆ°æ¯ä¸ªç±»çš„é¢„æµ‹ç½®ä¿¡åº¦
- ä½¿ç”¨äº¤å‰ç†µæ¥è¡¡é‡é¢„æµ‹å’Œæ ‡å·çš„åŒºåˆ«

## æŸå¤±å‡½æ•°

### L2 Loss

### L1 Loss

### Huber's Robust Loss

ç»“åˆäº†ä¸Šä¸¤è€…

## å›¾ç‰‡åˆ†ç±»æ•°æ®é›†

MNISTæ•°æ®é›†ï¼Œæ‰‹å†™æ•°å­—è¯†åˆ«

æˆ‘ä»¬å°†ä½¿ç”¨ç±»ä¼¼ä½†æ›´å¤æ‚çš„Fashion-MNISTæ•°æ®é›†

```python
#å¯¼å…¥æ•°æ®é›†
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

d2l.use_svg_display()

#è¯»å–æ•°æ®é›†
# é€šè¿‡ToTensorå®ä¾‹å°†å›¾åƒæ•°æ®ä»PILç±»å‹å˜æ¢æˆ32ä½æµ®ç‚¹æ•°æ ¼å¼
# å¹¶é™¤ä»¥255ä½¿å¾—æ‰€æœ‰åƒç´ çš„æ•°å€¼å‡åœ¨0åˆ°1ä¹‹é—´
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)

â€¦â€¦

#æ•´åˆå
def load_data_fashion_mnist(batch_size, resize=None):  #@save
    """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```

## Softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°

```python
import torch
from IPython import display
from d2l import torch as d2l

#å¯¼å…¥è®­ç»ƒé›†ã€æµ‹è¯•é›†
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#å±•å¹³å›¾åƒã€‚æ•°æ®é›†æœ‰10ä¸ªç±»åˆ«ï¼Œæ‰€ä»¥ç½‘ç»œè¾“å‡ºç»´åº¦ä¸º10
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)

#å®ç°softmax
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # è¿™é‡Œåº”ç”¨äº†å¹¿æ’­æœºåˆ¶

#å®ç°softmaxå›å½’æ¨¡å‹
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
  
#å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

#å°†é¢„æµ‹ç±»åˆ«ä¸çœŸå®yå…ƒç´ è¿›è¡Œæ¯”è¾ƒ
def accuracy(y_hat, y):  #@save
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡ã€‚"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
  
#è¯„ä¼°åœ¨ä»»æ„æ¨¡å‹netçš„å‡†ç¡®ç‡
def evaluate_accuracy(net, data_iter):  #@save
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦ã€‚"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

#å‚¨å­˜æ­£ç¡®é¢„æµ‹çš„æ•°é‡å’Œé¢„æµ‹çš„æ€»æ•°é‡
class Accumulator:  #@save
    """åœ¨`n`ä¸ªå˜é‡ä¸Šç´¯åŠ ã€‚"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
      
evaluate_accuracy(net, test_iter)

#softmaxå›å½’çš„è®­ç»ƒ
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰ã€‚"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.backward()
            updater.step()
            metric.add(float(l) * len(y), accuracy(y_hat, y),
                       y.size().numel())
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒå‡†ç¡®ç‡
    return metric[0] / metric[2], metric[1] / metric[2]
  
#å®šä¹‰ä¸€ä¸ªåœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®çš„å®ç”¨ç¨‹åºç±»
class Animator:  #@save
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®ã€‚"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # å¢é‡åœ°ç»˜åˆ¶å¤šæ¡çº¿
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # ä½¿ç”¨lambdaå‡½æ•°æ•è·å‚æ•°
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
    def add(self, x, y):
        # å‘å›¾è¡¨ä¸­æ·»åŠ å¤šä¸ªæ•°æ®ç‚¹
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
     
#è®­ç»ƒå‡½æ•°ğŸŒŸ
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰ã€‚"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    #æ ¸å¿ƒğŸŒŸ
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
    
#å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ¥ä¼˜åŒ–æ¨¡å‹çš„æŸå¤±å‡½æ•°
lr = 0.1
def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)

#è®­ç»ƒ10ä¸ªè¿­ä»£å‘¨æœŸ
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)

#å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»é¢„æµ‹
def predict_ch3(net, test_iter, n=6):  #@save
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰ã€‚"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```

## Softmaxå›å½’çš„ç®€æ´å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

#å¯¼å…¥è®­ç»ƒé›†ã€æµ‹è¯•é›†
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#Softmaxå›å½’çš„è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚
# PyTorchä¸ä¼šéšå¼åœ°è°ƒæ•´è¾“å…¥çš„å½¢çŠ¶ã€‚å› æ­¤ï¼Œ
# æˆ‘ä»¬åœ¨çº¿æ€§å±‚å‰å®šä¹‰äº†å±•å¹³å±‚ï¼ˆflattenï¼‰ï¼Œæ¥è°ƒæ•´ç½‘ç»œè¾“å…¥çš„å½¢çŠ¶
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);

#åœ¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­ä¼ é€’æœªå½’ä¸€åŒ–çš„é¢„æµ‹ï¼Œå¹¶åŒæ—¶è®¡ç®—softmaxåŠå…¶å¯¹æ•°
loss = nn.CrossEntropyLoss()

#ä½¿ç”¨å­¦ä¹ ç‡ä¸º0.1çš„å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä½œä¸ºä¼˜åŒ–ç®—æ³•
trainer = torch.optim.SGD(net.parameters(), lr=0.1)

#è°ƒç”¨ä¹‹å‰å®šä¹‰çš„è®­ç»ƒå‡½æ•°æ¥è®­ç»ƒæ¨¡å‹
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

```

# 10 å¤šå±‚æ„ŸçŸ¥æœº

## æ„ŸçŸ¥æœº perceptron

äºŒåˆ†ç±»ï¼šè¾“å‡º 1/-1

**æ–‡ç« ï¼šæœºå™¨å­¦ä¹ ä¹‹æ„ŸçŸ¥æœºä¸æ¢¯åº¦ä¸‹é™æ³•è®¤çŸ¥**[https://blog.csdn.net/zc20161202005/article/details/79983657]

### æ”¶æ•›å®šç†

### XORé—®é¢˜

æ„ŸçŸ¥æœºä¸èƒ½æ‹ŸåˆXORå‡½æ•°ï¼Œå®ƒåªèƒ½äº§ç”Ÿçº¿æ€§åˆ†å‰²é¢

### æ€»ç»“

æ„ŸçŸ¥æœºæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ï¼Œæ˜¯æœ€æ—©çš„AIæ¨¡å‹ä¹‹ä¸€

å®ƒçš„æ±‚è§£ç®—æ³•ç­‰ä»·äºä½¿ç”¨æ‰¹é‡å¤§å°ä¸º1çš„æ¢¯åº¦ä¸‹é™

å®ƒä¸èƒ½æ‹ŸåˆXORå‡½æ•°ï¼Œå¯¼è‡´çš„ç¬¬ä¸€æ¬¡AIå¯’å†¬

## å¤šå±‚æ„ŸçŸ¥æœº

### å­¦ä¹ XOR

è¾“å…¥å±‚ï¼Œå•éšè—å±‚ï¼ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Œéçº¿æ€§å‡½æ•°ï¼Œé¿å…æˆä¸ºçº¿æ€§æ¨¡å‹ï¼‰ï¼Œè¾“å‡ºå±‚

éšè—å±‚å¤§å°æ˜¯è¶…å‚æ•°

### Sigmoidæ¿€æ´»å‡½æ•°

### Tanhæ¿€æ´»å‡½æ•°

### ReLUæ¿€æ´»å‡½æ•°

ReLU(x)=max(x,0)

### å¤šç±»åˆ†ç±»

### å¤šéšè—å±‚

è¶…å‚æ•°ï¼š

- éšè—å±‚æ•°

- æ¯å±‚éšè—å±‚çš„å¤§å°

### æ€»ç»“

å¤šå±‚æ„ŸçŸ¥æœºä½¿ç”¨éšè—å±‚å’Œæ¿€æ´»å‡½æ•°æ¥å¾—åˆ°éçº¿æ€§æ¨¡å‹

ä½¿ç”¨æ¿€æ´»å‡½æ•°

ä½¿ç”¨Softmaxæ¥å¤„ç†å¤šç±»åˆ†ç±»

è¶…å‚æ•°ä¸ºéšè—å±‚æ•°ï¼Œå’Œå„ä¸ªéšè—å±‚å¤§å°  

## å¤šå±‚æ„ŸçŸ¥æœºçš„ä»é›¶å¼€å§‹å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

#å¯¼å…¥è®­ç»ƒé›†ã€æµ‹è¯•é›†
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#å®ç°ä¸€ä¸ªå…·æœ‰å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œå®ƒåŒ…å«256ä¸ªéšè—å•å…ƒ
#å„å±‚å•å…ƒæ•°
num_inputs, num_outputs, num_hiddens = 784, 10, 256
#å‚æ•°åˆå§‹åŒ–
W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]

#å®ç°ReLUæ¿€æ´»å‡½æ•°
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)

#å®ç°æˆ‘ä»¬çš„æ¨¡å‹
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # è¿™é‡Œâ€œ@â€ä»£è¡¨çŸ©é˜µä¹˜æ³•
    return (H@W2 + b2)
  
loss = nn.CrossEntropyLoss()

#è®­ç»ƒè¿‡ç¨‹å’Œsoftmaxå›å½’çš„è®­ç»ƒè¿‡ç¨‹ç›¸åŒ
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)

d2l.predict_ch3(net, test_iter)
```

## å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

# 11 æ¨¡å‹é€‰æ‹©+è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ

## æ¨¡å‹é€‰æ‹©

### è®­ç»ƒè¯¯å·®å’Œæ³›åŒ–è¯¯å·®

è®­ç»ƒè¯¯å·®ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®

æ³›åŒ–è¯¯å·®ï¼šæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¯¯å·®

### éªŒè¯æ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†

éªŒè¯æ•°æ®é›†ï¼šä¸€ä¸ªç”¨æ¥è¯„ä¼°æ¨¡å‹å¥½åçš„æ•°æ®é›†

æµ‹è¯•æ•°æ®é›†ï¼šåªç”¨ä¸€æ¬¡çš„æ•°æ®é›†

### K-åˆ™äº¤å‰éªŒè¯

å°†è®­ç»ƒæ•°æ®åˆ†å‰²æˆkå—ï¼Œæ¯æ¬¡ä½¿ç”¨ç¬¬iå—ä½œä¸ºéªŒè¯æ•°æ®é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒæ•°æ®é›†ï¼Œåškæ¬¡

æŠ¥å‘Škä¸ªéªŒè¯é›†è¯¯å·®çš„å¹³å‡

å¸¸ç”¨ï¼šK=5æˆ–10

#### æ€»ç»“

- è®­ç»ƒæ•°æ®é›†ï¼šè®­ç»ƒæ¨¡å‹å‚æ•°
- éªŒè¯æ•°æ®é›†ï¼šé€‰æ‹©æ¨¡å‹è¶…å‚æ•°
- éå¤§æ•°æ®é›†ä¸Šé€šå¸¸ä½¿ç”¨k-æŠ˜äº¤å‰éªŒè¯

## è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ

![](pic/è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ.png)

### æ¨¡å‹å®¹é‡

æ‹Ÿåˆå„ç§å‡½æ•°çš„èƒ½åŠ›

ä½å®¹é‡çš„æ¨¡å‹éš¾ä»¥æ‹Ÿåˆè®­ç»ƒæ•°æ®

é«˜å®¹é‡çš„æ¨¡å‹å¯ä»¥è®°ä½æ‰€æœ‰çš„è®­ç»ƒæ•°æ®

### VCç»´

ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„ä¸€ä¸ªæ ¸å¿ƒæ€æƒ³

å¯¹äºä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼ŒVCç­‰äºä¸€ä¸ªæœ€å¤§çš„æ•°æ®é›†çš„å¤§å°ï¼Œä¸ç®¡å¦‚ä½•ç»™ä¸æ ‡å·ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªæ¨¡å‹æ¥å¯¹å®ƒè¿›è¡Œå®Œç¾åˆ†ç±»

ä¾‹å­ï¼š2ç»´è¾“å…¥ä¸ªæ„ŸçŸ¥æœºï¼ŒVCç»´=3ï¼Œ!= 4(xor)

#### VCç»´çš„ç”¨å¤„

æä¾›ä¸ºä»€ä¹ˆä¸€ä¸ªæ¨¡å‹å¥½çš„ç†è®ºä¾æ®

ä½†æ·±åº¦å­¦ä¹ ä¸­å¾ˆå°‘ä½¿ç”¨

### æ•°æ®å¤æ‚åº¦

æ ·æœ¬ä¸ªæ•°ã€æ¯ä¸ªæ ·æœ¬çš„å…ƒç´ ä¸ªæ•°ã€æ—¶é—´ç©ºé—´ç»“æ„ã€å¤šæ ·æ€§

### æ€»ç»“

æ¨¡å‹å®¹é‡éœ€è¦åŒ¹é…æ•°æ®å¤æ‚åº¦ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ

ç»Ÿè®¡æœºå™¨å­¦ä¹ æä¾›æ•°å­¦å·¥å…·æ¥è¡¡é‡æ¨¡å‹å¤æ‚åº¦

å®é™…ä¸­ä¸€èˆ¬é è§‚å¯Ÿè®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®

# 12 æƒé‡è¡°é€€

weight decay

## æƒé‡è¡°é€€

### ä½¿ç”¨å‡æ–¹èŒƒæ•°ä½œä¸ºç¡¬æ€§é™åˆ¶

é€šè¿‡é™åˆ¶å‚æ•°å€¼çš„é€‰æ‹©èŒƒå›´æ¥æ§åˆ¶æ¨¡å‹å®¹é‡

### ä½¿ç”¨å‡æ–¹èŒƒæ•°ä½œä¸ºæŸ”æ€§é™åˆ¶

å¼•å…¥æ­£åˆ™é¡¹

**æ–‡ç« ï¼šæœºå™¨å­¦ä¹ ä¸­å¸¸å¸¸æåˆ°çš„æ­£åˆ™åŒ–åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ - åˆ˜é¥è¡Œçš„å›ç­” - çŸ¥ä¹** [https://www.zhihu.com/question/20924039/answer/240037674]

**æ–‡ç« ï¼šæ­£åˆ™é¡¹æµ…æ**[https://www.jianshu.com/p/70487abdf96b]

### å‚æ•°æ›´æ–°æ³•åˆ™

- è®¡ç®—æ¢¯åº¦
- æ—¶é—´tæ›´æ–°å‚æ•°

### æ€»ç»“

- æƒé‡è¡°é€€é€šè¿‡L2æ­£åˆ™é¡¹ä½¿å¾—æ¨¡å‹å‚æ•°ä¸ä¼šè¿‡å¤§ï¼Œä»è€Œæ§åˆ¶æ¨¡å‹å¤æ‚åº¦
- æ­£åˆ™é¡¹æƒé‡æ˜¯æ§åˆ¶æ¨¡å‹å¤æ‚åº¦çš„è¶…å‚æ•°

## ä»£ç å®ç°

è¯­æ³•ç‚¹ï¼š

lambda ç®€åŒ–å‡½æ•°å®šä¹‰çš„ä¹¦å†™å½¢å¼



### ä»é›¶å¼€å§‹å®ç°

### ç®€æ´å®ç°

# 13 ä¸¢å¼ƒæ³•

## ä¸¢å¼ƒæ³• dropout

### åŠ¨æœº

ä¸€ä¸ªå¥½çš„æ¨¡å‹éœ€è¦å¯¹è¾“å…¥æ•°æ®çš„æ‰°åŠ¨é²æ£’

ä½¿ç”¨æœ‰å™ªéŸ³çš„æ•°æ®ç­‰ä»·äºTikhonovæ­£åˆ™

ä¸¢å¼ƒæ³•ï¼šåœ¨å±‚ä¹‹é—´åŠ å…¥å™ªéŸ³

### æ— åå·®çš„åŠ å…¥å™ªéŸ³

æ¦‚ç‡på˜ä¸º0ï¼Œæ¦‚ç‡1-på˜å¤§

### ä½¿ç”¨ä¸¢å¼ƒæ³•

é€šå¸¸å°†ä¸¢å¼ƒæ³•ä½œç”¨åœ¨éšè—å…¨è¿æ¥å±‚çš„è¾“å‡ºä¸Š

### æ¨ç†ä¸­çš„ä¸¢å¼ƒæ³•

dropoutæ˜¯æ­£åˆ™é¡¹ï¼Œåªåœ¨è®­ç»ƒä¸­ä½¿ç”¨ï¼šä»–ä»¬å½±å“æ¨¡å‹å‚æ•°çš„æ›´æ–°

åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸¢å¼ƒæ³•ç›´æ¥è¿”å›è¾“å…¥ h=dropout(h),è¿™æ ·ä¹Ÿèƒ½ä¿è¯ç¡®å®šæ€§çš„è¾“å‡º

### æ€»ç»“

- ä¸¢å¼ƒæ³•å°†ä¸€äº›è¾“å‡ºé¡¹éšæœºç½®0æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
- å¸¸ä½œç”¨åœ¨å¤šå±‚æ„ŸçŸ¥æœºçš„éšè—å±‚è¾“å‡ºä¸Š
- ä¸¢å¼ƒæ¦‚ç‡æ˜¯æ§åˆ¶æ¨¡å‹å¤æ‚åº¦çš„è¶…å‚æ•°

## ä»£ç å®ç°

### ä»é›¶å¼€å§‹å®ç°

### ç®€æ´å®ç°

## 14 æ•°å€¼ç¨³å®šæ€§+æ¨¡å‹åˆå§‹åŒ–å’Œæ¿€æ´»å‡½æ•°

## æ•°å€¼ç¨³å®šæ€§

### ç¥ç»ç½‘ç»œçš„æ¢¯åº¦

æŸå¤±å‡½æ•°ç»è¿‡è®¸å¤šå±‚ç¥ç»ç½‘ç»œåï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¶æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±

æ¢¯åº¦çˆ†ç‚¸ï¼šå€¼è¶…å‡ºå€¼åŸŸï¼Œå¯¹å­¦ä¹ ç‡æ•æ„Ÿ

### æ€»ç»“

å½“æ•°å€¼è¿‡å¤§æˆ–è€…è¿‡å°æ—¶ä¼šå¯¼è‡´æ•°å€¼é—®é¢˜

å¸¸å‘ç”Ÿåœ¨æ·±åº¦æ¨¡å‹ä¸­ï¼Œå› ä¸ºå…¶ä¼šå¯¹nä¸ªæ•°ç´¯ä¹˜

## æ¨¡å‹åˆå§‹åŒ–å’Œæ¿€æ´»å‡½æ•°

### è®©è®­ç»ƒæ›´åŠ ç¨³å®š

ç›®æ ‡ï¼šè®©æ¢¯åº¦å€¼åœ¨åˆç†çš„èŒƒå›´å†…

å°†ä¹˜æ³•å˜åŠ æ³•ï¼šResNet,LSTM

å½’ä¸€åŒ–:æ¢¯åº¦å½’ä¸€åŒ–ï¼Œæ¢¯åº¦è£å‰ª

åˆç†çš„æƒé‡åˆå§‹å’Œæ¿€æ´»å‡½æ•°

### è®©æ¯å±‚çš„æ–¹å·®æ˜¯ä¸€ä¸ªå¸¸æ•°

å°†æ¯å±‚çš„è¾“å‡ºå’Œæ¢¯åº¦éƒ½çœ‹ä½œéšæœºå˜é‡

è®©ä»–ä»¬çš„å‡å€¼å’Œæ–¹å·®éƒ½ä¿æŒä¸€è‡´

### æƒé‡åˆå§‹åŒ–

åœ¨åˆç†å€¼åŒºé—´é‡Œéšæœºåˆå§‹å‚æ•°

è®­ç»ƒå¼€å§‹çš„æ—¶å€™æ›´å®¹æ˜“æœ‰æ•°å€¼ä¸ç¨³å®š

â€‹	è¿œç¦»æœ€ä¼˜è§£çš„åœ°æ–¹æŸå¤±å‡½æ•°è¡¨é¢å¯èƒ½å¾ˆå¤æ‚

â€‹	æœ€ä¼˜è§£é™„è¿‘è¡¨é¢ä¼šæ¯”è¾ƒå¹³

ä½¿ç”¨Nï¼ˆ0ï¼Œ0.01ï¼‰æ¥åˆå§‹å¯èƒ½å¯¹å°ç½‘ç»œæ²¡é—®é¢˜ï¼Œä½†ä¸èƒ½ä¿è¯æ·±åº¦ç¥ç»ç½‘ç»œ

### æ£€æŸ¥å¸¸ç”¨æ¿€æ´»å‡½æ•°

ä½¿ç”¨æ³°å‹’å±•å¼€

### æ€»ç»“ 

åˆç†çš„æƒé‡åˆå§‹å€¼å’Œæ¿€æ´»å‡½æ•°çš„é€‰å–å¯ä»¥æå‡æ•°å€¼ç¨³å®šæ€§

# 15 å®æˆ˜ï¼šKaggleæˆ¿ä»·é¢„æµ‹+è¯¾ç¨‹ç«èµ›ï¼šåŠ å·2020å¹´æˆ¿ä»·é¢„æµ‹



# 16 PyTorchç¥ç»ç½‘ç»œåŸºç¡€

## æ¨¡å‹æ„é€ 

### å±‚å’Œå—

```python
nn.Sequential

class MLP(nn.Module):
  def __init__(self):
    super().__init__()
    
   def forward(self,x)
```

å¯¹nn.Moduleç±»çš„ç»§æ‰¿ï¼Œå¯ä»¥ä½¿å—å†…éƒ¨æ›´åŠ çµæ´»

## å‚æ•°ç®¡ç†

### å‚æ•°è®¿é—®

```python
net=nm.Sequential(nn.linear,nn.ReLU,nn.Linear)
net(2).state.dict() #è®¿é—®ç¬¬3å±‚çš„ç›¸å…³å‚æ•° weight. bias

type(net[2].bias)
net[2].bias
net[2].bias.data
net[2].weight.grad

net[0].named_parameters() #ä¸€æ¬¡æ€§è®¿é—®æ‰€æœ‰å‚æ•°
net.named_parameters()

net.statue_dict()['2.bias'].data
```

### ä»åµŒå¥—å—æ”¶é›†å‚æ•°

sequential å¥—å¨ƒ

printç½‘ç»œï¼ŒæŸ¥çœ‹å®ƒæ˜¯å¦‚ä½•ç»„ç»‡çš„

### å†…ç½®åˆå§‹åŒ–

```python
def init_normal

net.apply(init_normal)
```

#### å¯¹æŸäº›å—åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•

#### è‡ªå®šä¹‰åˆå§‹åŒ–

### å‚æ•°ç»‘å®š

## è‡ªå®šä¹‰å±‚

## è¯»å†™æ–‡ä»¶

```python
#åŠ è½½å’Œä¿å­˜å‘é‡
x=torch.arange(4)
torch.save(x,'x-file')
x2=torch.load('x-file')

#å­˜å‚¨ä¸€ä¸ªå¼ é‡åˆ—è¡¨ï¼Œç„¶åæŠŠå®ƒä»¬è¯»å›å†…å­˜
y=torch.zeros(4)
torch.save([x,y],'x-file')
x2,y2=torch.load('x-file')

#å†™å…¥æˆ–è¯»å–ä»å­—ç¬¦ä¸²æ˜ å°„åˆ°å¼ é‡çš„å­—å…¸

#åŠ è½½å’Œä¿å­˜æ¨¡å‹å‚æ•°
torch.save(net.state_dict(),'mlp.params')

clone=MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()

```

# 17 ä½¿ç”¨å’Œè´­ä¹°GPU

## ä½¿ç”¨GPU

### å¼ é‡

```python
!nvidia-smi

torch.cuda.device

#æŸ¥è¯¢å¯ç”¨gpuçš„æ•°é‡
torch.cuda.device_count()

#æŸ¥è¯¢å¼ é‡æ‰€åœ¨çš„è®¾å¤‡
x = torch.tensor([1,2,3])
x.device

#å­˜å‚¨åœ¨gpuä¸Š
x=torch.ones(2,3,device= )

z=x.cuda(1) #å°†xçš„å€¼æ‹·è´åˆ°è®¾å¤‡cudaï¼š1ï¼Œè¿ç®—éœ€è¦åœ¨ç›¸åŒGPUä¸Šè¿›è¡Œ
```

### ç¥ç»ç½‘ç»œ

```python
net=nn.Sequential(nn.linear(3,1))
net=net.to(device=try_gpu())

#ç¡®å®šæ¨¡å‹å‚æ•°å­˜å‚¨åœ¨åŒä¸€ä¸ªGPUä¸Š
net[0].weight.weight.data.device
```

## è´­ä¹°GPU

# 18 é¢„æµ‹æˆ¿ä»·ç«èµ›æ€»ç»“

# 19 å·ç§¯å±‚

## ä»å…¨è¿æ¥åˆ°å·ç§¯

å›¾ç‰‡æŸ¥æ‰¾ï¼Œä¸¤ä¸ªåŸåˆ™ï¼šå¹³ç§»ä¸å˜æ€§ã€å±€éƒ¨æ€§

### é‡æ–°è€ƒå¯Ÿå…¨è¿æ¥å±‚

- å°†è¾“å…¥å’Œè¾“å‡ºå˜å½¢ä¸ºçŸ©é˜µ
- å°†æƒé‡å˜å½¢ä¸º4-Då¼ é‡
- væ˜¯wçš„é‡æ–°ç´¢å¼•

### åŸåˆ™#1-å¹³ç§»ä¸å˜æ€§

xçš„å¹³ç§»å¯¼è‡´hçš„å¹³ç§»

vä¸åº”è¯¥ä¾èµ–äºï¼ˆiï¼Œjï¼‰

### åŸåˆ™#2-å±€éƒ¨æ€§

è¯„ä¼°hijæ—¶ï¼Œæˆ‘ä»¬ä¸åº”è¯¥ç”¨è¿œç¦»xijçš„å‚æ•°

### æ€»ç»“

å¯¹å…¨è¿æ¥å±‚ä½¿ç”¨å¹³ç§»ä¸å˜æ€§å’Œå±€éƒ¨æ€§å¾—åˆ°å·ç§¯å±‚



å·ç§¯ï¼šæ•°å­—ä¿¡å·å¤„ç†

## å·ç§¯å±‚

### äºŒç»´äº¤å‰ç›¸å…³

![](pic/äºŒç»´äº¤å‰ç›¸å…³.png)

### äºŒç»´å·ç§¯å±‚

![](pic/äºŒç»´å·ç§¯å±‚.png)

### ä¾‹å­

![](pic/å·ç§¯ä¾‹å­.png)

### äº¤å‰ç›¸å…³ VS å·ç§¯

![](pic/äº¤å‰ç›¸å…³vså·ç§¯.png)

### ä¸€ç»´å’Œä¸‰ç»´äº¤å‰ç›¸å…³

ä¸€ç»´ï¼šæ–‡æœ¬ï¼Œè¯­è¨€ï¼Œæ—¶åºåºåˆ—

ä¸‰ç»´ï¼šè§†é¢‘ï¼ŒåŒ»å­¦å›¾åƒï¼Œæ°”è±¡åœ°å›¾

### æ€»ç»“

- å·ç§¯å±‚å°†è¾“å…¥å’Œæ ¸çŸ©é˜µè¿›è¡Œäº¤å‰ç›¸å…³ï¼ŒåŠ ä¸Šåç§»åå¾—åˆ°è¾“å‡º
- æ ¸çŸ©é˜µå’Œåç§»æ˜¯å¯å­¦ä¹ çš„å‚æ•°
- æ ¸çŸ©é˜µçš„å¤§å°æ˜¯è¶…å‚æ•°

## ä»£ç 

### å›¾åƒå·ç§¯

äº’ç›¸å…³è¿ç®—

```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """è®¡ç®—äºŒç»´äº’ç›¸å…³è¿ç®—ã€‚"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

å®ç°äºŒç»´å·ç§¯å±‚

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

å­¦ä¹ ç”±Xç”ŸæˆYçš„å·ç§¯æ ¸

```python
# æ„é€ ä¸€ä¸ªäºŒç»´å·ç§¯å±‚ï¼Œå®ƒå…·æœ‰1ä¸ªè¾“å‡ºé€šé“å’Œå½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ2ï¼‰çš„å·ç§¯æ ¸
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# è¿™ä¸ªäºŒç»´å·ç§¯å±‚ä½¿ç”¨å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰ï¼Œ
# å…¶ä¸­æ‰¹é‡å¤§å°å’Œé€šé“æ•°éƒ½ä¸º1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'batch {i+1}, loss {l.sum():.3f}')
```

# 20 å·ç§¯å±‚é‡Œçš„å¡«å……å’Œæ­¥å¹…

## å¡«å……

åœ¨è¾“å…¥å‘¨å›´æ·»åŠ é¢å¤–çš„è¡Œ/åˆ—

## æ­¥å¹…

å¡«å……å‡å°çš„è¾“å‡ºå¤§å°ä¸å±‚æ•°çº¿æ€§ç›¸å…³

æ­¥å¹…æ˜¯æŒ‡è¡Œ/åˆ—çš„æ»‘åŠ¨æ­¥é•¿ï¼ˆé»˜è®¤æ­¥å¹…ä¸º1ï¼‰

### æ€»ç»“

å¡«å……å’Œæ­¥å¹…æ˜¯å·ç§¯å±‚çš„è¶…å‚æ•°

å¡«å……åœ¨è¾“å…¥å‘¨å›´æ·»åŠ é¢å¤–çš„è¡Œ/åˆ—ï¼Œæ¥æ§åˆ¶è¾“å‡ºå½¢çŠ¶çš„å‡å°‘é‡

æ­¥å¹…æ˜¯æ¯æ¬¡æ»‘åŠ¨æ ¸çª—å£æ—¶çš„è¡Œ/åˆ—çš„æ­¥é•¿ï¼Œå¯ä»¥æˆå€çš„å‡å°‘è¾“å‡ºå½¢çŠ¶

## ä»£ç å®ç°

padding   stride

# 21 å·ç§¯å±‚é‡Œçš„å¤šè¾“å…¥å¤šè¾“å‡ºé€šé“

## å¤šè¾“å…¥å¤šè¾“å‡ºé€šé“ channel

### å¤šä¸ªè¾“å…¥é€šé“

- å½©è‰²å›¾åƒå¯èƒ½æœ‰RGBä¸‰ä¸ªé€šé“
- è½¬æ¢ä¸ºç°åº¦ä¼šä¸¢å¤±ä¿¡æ¯

æ¯ä¸ªé€šé“éƒ½æœ‰ä¸€ä¸ªå·ç§¯æ ¸ï¼Œç»“æœæ˜¯æ‰€æœ‰é€šé“å·ç§¯ç»“æœçš„æ ¸

![](pic/å¤šä¸ªè¾“å…¥é€šé“.png)

### å¤šä¸ªè¾“å‡ºé€šé“

æ— è®ºæœ‰å¤šå°‘è¾“å…¥é€šé“ï¼Œåˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬åªç”¨åˆ°å•è¾“å‡ºé€šé“

æˆ‘ä»¬å¯ä»¥æœ‰å¤šä¸ªä¸‰ç»´å·ç§¯æ ¸ï¼Œæ¯ä¸ªæ ¸ç”Ÿæˆä¸€ä¸ªè¾“å‡ºé€šé“

![](pic/å¤šä¸ªè¾“å‡ºé€šé“.png)

### ä½œç”¨

æ¯ä¸ªè¾“å‡ºé€šé“å¯ä»¥è¯†åˆ«ç‰¹å®šæ¨¡å¼

è¾“å…¥é€šé“æ ¸è¯†åˆ«å¹¶ç»„åˆè¾“å…¥ä¸­çš„æ¨¡å¼

### 1x1å·ç§¯å±‚

å®ƒä¸è¯†åˆ«ç©ºé—´æ¨¡å¼ï¼Œåªæ˜¯èåˆé€šé“ã€‚

### äºŒç»´å·ç§¯å±‚

![](pic/äºŒç»´å·ç§¯å±‚å¤šé€šé“.png)

### æ€»ç»“

è¾“å‡ºé€šé“æ•°æ˜¯å·ç§¯å±‚çš„è¶…å‚æ•°

æ¯ä¸ªè¾“å…¥é€šé“æœ‰ç‹¬ç«‹çš„äºŒç»´å·ç§¯æ ¸ï¼Œæ‰€æœ‰é€šé“ç»“æœç›¸åŠ å¾—åˆ°ä¸€ä¸ªè¾“å‡ºé€šé“ç»“æœ

æ¯ä¸ªè¾“å‡ºé€šé“æœ‰ç‹¬ç«‹çš„ä¸‰ç»´å·ç§¯æ ¸

## ä»£ç å®ç°

å®ç°ä¸€ä¸‹å¤šè¾“å…¥é€šé“äº’ç›¸å…³è®¡ç®—

```python
def corr2d_multi_in(X, K):
    # å…ˆéå† â€œXâ€ å’Œ â€œKâ€ çš„ç¬¬0ä¸ªç»´åº¦ï¼ˆé€šé“ç»´åº¦ï¼‰ï¼Œå†æŠŠå®ƒä»¬åŠ åœ¨ä¸€èµ·
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
```

è®¡ç®—å¤šä¸ªé€šé“çš„è¾“å‡ºçš„äº’ç›¸å…³å‡½æ•°

```python
def corr2d_multi_in_out(X, K):
    # è¿­ä»£â€œKâ€çš„ç¬¬0ä¸ªç»´åº¦ï¼Œæ¯æ¬¡éƒ½å¯¹è¾“å…¥â€œXâ€æ‰§è¡Œäº’ç›¸å…³è¿ç®—ã€‚
    # æœ€åå°†æ‰€æœ‰ç»“æœéƒ½å åŠ åœ¨ä¸€èµ·
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

1x1å·ç§¯

```python
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))
    K = K.reshape((c_o, c_i))
    # å…¨è¿æ¥å±‚ä¸­çš„çŸ©é˜µä¹˜æ³•
    Y = torch.matmul(K, X)
    return Y.reshape((c_o, h, w))
```

# 22 æ± åŒ–å±‚

## æ± åŒ–å±‚

ç§¯å¯¹ä½ç½®æ•æ„Ÿ                             æ£€æµ‹å‚ç›´è¾¹ç¼˜

éœ€è¦ä¸€å®šç¨‹åº¦çš„å¹³ç§»ä¸å˜æ€§      ç…§æ˜ï¼Œç‰©ä½“ä½ç½®ï¼Œæ¯”ä¾‹ï¼Œå¤–è§‚ç­‰ç­‰å› å›¾åƒè€Œå¼‚

### äºŒç»´æœ€å¤§æ± åŒ–

![](pic/äºŒç»´æœ€å¤§æ± åŒ–.png)

å¯å®¹1åƒç´ ç§»ä½

### å¡«å……ï¼Œæ­¥å¹…å’Œå¤šä¸ªé€šé“

æ± åŒ–å±‚ä¸å·ç§¯å±‚ç±»ä¼¼ï¼Œéƒ½å…·æœ‰å¡«å……å’Œæ­¥å¹…

æ²¡æœ‰å¯å­¦ä¹ çš„å‚æ•°

åœ¨æ¯ä¸ªè¾“å…¥é€šé“åº”ç”¨æ± åŒ–å±‚ä»¥è·å¾—ç›¸åº”çš„è¾“å‡ºé€šé“

è¾“å‡ºé€šé“æ•°=è¾“å…¥é€šé“æ•°

### å¹³å‡æ± åŒ–å±‚

æœ€å¤§æ± åŒ–å±‚ï¼šæ¯ä¸ªçª—å£ä¸­æœ€å¼ºçš„æ¨¡å¼ä¿¡å·

å¹³å‡æ± åŒ–å±‚ï¼šå°†æœ€å¤§æ± åŒ–å±‚ä¸­çš„â€œæœ€å¤§â€æ“ä½œæ›¿æ¢ä¸ºâ€œå¹³å‡â€

### æ€»ç»“

æ± åŒ–å±‚è¿”å›çª—å£ä¸­æœ€å¤§æˆ–å¹³å‡å€¼

ç¼“è§£å·ç§¯å±‚å¯¹ä½ç½®çš„æ•æ„Ÿæ€§

åŒæ ·æœ‰çª—å£å¤§å°ã€å¡«å……å’Œæ­¥å¹…ä½œä¸ºè¶…å‚æ•°

## ä»£ç å®ç°

å®ç°æ± åŒ–å±‚çš„æ­£å‘ä¼ æ’­

```python
import torch
from torch import nn
from d2l import torch as d2l

def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
```

å¡«å……å’Œæ­¥å¹…

```python
X = torch.arange(16, dtype=d2l.float32).reshape((1, 1, 4, 4))
X

pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)

pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))
pool2d(X)
```

å¤šä¸ªé€šé“

```python
X = torch.cat((X, X + 1), 1)
X

pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

# 23 ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œ LeNet

## LeNet

æ‰‹å†™çš„æ•°å­—è¯†åˆ«

MNIST

![](pic/LeNet.png)

### æ€»ç»“

LeNetæ˜¯æ—©æœŸæˆåŠŸçš„ç¥ç»ç½‘ç»œ

å…ˆä½¿ç”¨å·ç§¯å±‚æ¥å­¦ä¹ å›¾ç‰‡ç©ºé—´ä¿¡æ¯

ç„¶åä½¿ç”¨å…¨è¿æ¥å±‚æ¥è½¬æ¢åˆ°ç±»åˆ«ç©ºé—´

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l


class Reshape(torch.nn.Module):
    def forward(self, x):
        return x.view(-1, 1, 28, 28)

net = torch.nn.Sequential(
    Reshape(),
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))

X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
    
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """ä½¿ç”¨GPUè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„ç²¾åº¦ã€‚"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        if not device:
            device = next(iter(net.parameters())).device
    # æ­£ç¡®é¢„æµ‹çš„æ•°é‡ï¼Œæ€»é¢„æµ‹çš„æ•°é‡
    metric = d2l.Accumulator(2)
    for X, y in data_iter:
        if isinstance(X, list):
            # BERTå¾®è°ƒæ‰€éœ€çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰
            X = [x.to(device) for x in X]
        else:
            X = X.to(device)
        y = y.to(device)
        metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
  
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)ã€‚"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼ŒèŒƒä¾‹æ•°
        metric = d2l.Accumulator(3)  
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
    
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

cnn  explainer ç¥ç»ç½‘ç»œå¯è§†åŒ–ï¼Œæ¯å±‚å†…éƒ¨æƒ…å†µ

# 24 æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ AlexNet

## AlexNet

æ•°æ®é›† ImageNet(2010)

AlexNetèµ¢å¾—äº†2012å¹´ImageNetç«èµ›

æ›´æ·±æ›´å¤§çš„LeNet

ä¸»è¦æ”¹è¿›ï¼šä¸¢å¼ƒæ³• ReLU MaxPooling 

è®¡ç®—æœºè§†è§‰æ–¹æ³•è®ºçš„æ”¹å˜

### æ€»ç»“

AlexNetæ˜¯æ›´å¤§æ›´æ·±çš„LeNetï¼Œ10xå‚æ•°ä¸ªæ•°ï¼Œ260xè®¡ç®—å¤æ‚åº¦

æ–°è¿›å…¥äº†ä¸¢å¼ƒæ³•ï¼ŒReLUï¼Œæœ€å¤§æ± åŒ–å±‚ï¼Œå’Œæ•°æ®å¢å¼º

AlexNetèµ¢ä¸‹äº†2012ImageNetç«èµ›åï¼Œæ ‡å¿—ç€æ–°çš„ä¸€è½®ç¥ç»ç½‘ç»œçƒ­æ½®çš„å¼€å§‹

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª11*11çš„æ›´å¤§çª—å£æ¥æ•æ‰å¯¹è±¡ã€‚
    # åŒæ—¶ï¼Œæ­¥å¹…ä¸º4ï¼Œä»¥å‡å°‘è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ã€‚
    # å¦å¤–ï¼Œè¾“å‡ºé€šé“çš„æ•°ç›®è¿œå¤§äºLeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # å‡å°å·ç§¯çª—å£ï¼Œä½¿ç”¨å¡«å……ä¸º2æ¥ä½¿å¾—è¾“å…¥ä¸è¾“å‡ºçš„é«˜å’Œå®½ä¸€è‡´ï¼Œä¸”å¢å¤§è¾“å‡ºé€šé“æ•°
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # ä½¿ç”¨ä¸‰ä¸ªè¿ç»­çš„å·ç§¯å±‚å’Œè¾ƒå°çš„å·ç§¯çª—å£ã€‚
    # é™¤äº†æœ€åçš„å·ç§¯å±‚ï¼Œè¾“å‡ºé€šé“çš„æ•°é‡è¿›ä¸€æ­¥å¢åŠ ã€‚
    # åœ¨å‰ä¸¤ä¸ªå·ç§¯å±‚ä¹‹åï¼Œæ±‡èšå±‚ä¸ç”¨äºå‡å°‘è¾“å…¥çš„é«˜åº¦å’Œå®½åº¦
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # è¿™é‡Œï¼Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æ˜¯LeNetä¸­çš„å¥½å‡ å€ã€‚ä½¿ç”¨dropoutå±‚æ¥å‡è½»è¿‡åº¦æ‹Ÿåˆ
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # æœ€åæ˜¯è¾“å‡ºå±‚ã€‚ç”±äºè¿™é‡Œä½¿ç”¨Fashion-MNISTï¼Œæ‰€ä»¥ç”¨ç±»åˆ«æ•°ä¸º10ï¼Œè€Œéè®ºæ–‡ä¸­çš„1000
    nn.Linear(4096, 10))

batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)

lr, num_epochs = 0.01, 10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 25 ä½¿ç”¨å—çš„ç½‘ç»œ VGG

## VGG

AlexNetæ¯”LeNetæ›´æ·±æ›´å¤§æ¥å¾—åˆ°æ›´å¥½çš„ç²¾åº¦

èƒ½ä¸èƒ½æ›´æ·±å’Œæ›´å¤§ï¼Ÿ

é€‰é¡¹ï¼š

- â€‹	æ›´å¤šçš„å…¨è¿æ¥å±‚ï¼ˆå¤ªè´µï¼‰
- â€‹	æ›´å¤šçš„å·ç§¯å±‚
- â€‹	å°†å·ç§¯å±‚ç»„åˆæˆå—

### VGGå—

3x3å·ç§¯ï¼Œ2x2æœ€å¤§æ± åŒ–å±‚

æ·±ä½†çª„æ•ˆæœæ›´å¥½

### VGGæ¶æ„

å¤šä¸ªVGGå—åæ¥å…¨è¿æ¥å±‚

ä¸åŒæ¬¡æ•°çš„é‡å¤å—å¾—åˆ°ä¸åŒçš„æ¶æ„

### æ€»ç»“

VGGä½¿ç”¨å¯é‡å¤ä½¿ç”¨çš„å·ç§¯å—æ¥æ„å»ºæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ

ä¸åŒçš„å·ç§¯å—ä¸ªæ•°å’Œè¶…å‚æ•°å¯ä»¥å¾—åˆ°ä¸åŒå¤æ‚åº¦çš„å˜ç§

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

#VGGå—
def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
  
#VGGç½‘ç»œ  
 #å…ˆæŒ‡å®šæ¶æ„ å·ç§¯æ•°ï¼Œé€šé“æ•°
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    # å·ç§¯å±‚éƒ¨åˆ†
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # å…¨è¿æ¥å±‚éƒ¨åˆ†
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10))

net = vgg(conv_arch)

#è®¡ç®—é‡å¤§ï¼Œæ„é€ ä¸€ä¸ªé€šé“æ•°è¾ƒå°‘çš„ç½‘ç»œ
ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)

#æ¨¡å‹è®­ç»ƒ
lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 26 ç½‘ç»œä¸­çš„ç½‘ç»œ NiN

## NiN

### å…¨è¿æ¥å±‚çš„é—®é¢˜

å·ç§¯å±‚éœ€è¦è¾ƒå°‘çš„å‚æ•°ï¼Œä½†å·ç§¯å±‚åçš„ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„å‚æ•°å¾ˆå¤š

å ç”¨å†…å­˜ã€è®¡ç®—å¸¦å®½

### NiNå—

ä¸€ä¸ªå·ç§¯å±‚åè·Ÿä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆ1x1å·ç§¯å±‚ï¼‰

æ­¥å¹…1ï¼Œæ— å¡«å……ï¼Œè¾“å‡ºå½¢çŠ¶è·Ÿå·ç§¯å±‚è¾“å‡ºä¸€æ ·

èµ·åˆ°å…¨è¿æ¥å±‚çš„ä½œç”¨

### NiNæ¶æ„

æ— å…¨è¿æ¥å±‚

äº¤æ›¿ä½¿ç”¨NiNå—å’Œæ­¥å¹…ä¸º2çš„æœ€å¤§æ± åŒ–å±‚

â€‹		é€æ­¥å‡å°é«˜å®½å’Œå¢å¤§é€šé“æ•°

æœ€åä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–å±‚å¾—åˆ°è¾“å‡º

â€‹		å…¶è¾“å…¥é€šé“æ•°æ˜¯ç±»åˆ«æ•°

### æ€»ç»“

NiNå—ä½¿ç”¨å·ç§¯å±‚åŠ ä¸¤ä¸ª1x1å·ç§¯å±‚

â€‹		åè€…å¯¹æ¯ä¸ªåƒç´ å¢åŠ äº†éçº¿æ€§æ€§

NiNä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–å±‚æ¥æ›¿ä»£VGGå’ŒAlexNetä¸­çš„å…¨è¿æ¥å±‚

â€‹		ä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œæ›´å°‘çš„å‚æ•°ä¸ªæ•°

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

#NiNå—
def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())
  
#NiNæ¨¡å‹
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    # æ ‡ç­¾ç±»åˆ«æ•°æ˜¯10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),
    # å°†å››ç»´çš„è¾“å‡ºè½¬æˆäºŒç»´çš„è¾“å‡ºï¼Œå…¶å½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°, 10)
    nn.Flatten())

lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 27 å«å¹¶è¡Œè¿ç»“çš„ç½‘ç»œGoogLeNet/Inception V3

## GoogLeNet

### æœ€å¥½çš„å·ç§¯å±‚è¶…å‚æ•°ï¼Ÿ Inceptionå—ï¼šæˆ‘å…¨éƒ½è¦ï¼

4ä¸ªè·¯å¾„ä»ä¸åŒå±‚é¢æŠ½å–ä¿¡æ¯ï¼Œç„¶ååœ¨è¾“å‡ºé€šé“ç»´åˆå¹¶

è·Ÿå•3x3æˆ–5x5å·ç§¯å±‚æ¯”ï¼ŒInceptionå—æœ‰æ›´å°‘çš„å‚æ•°ä¸ªæ•°å’Œè®¡ç®—å¤æ‚åº¦

### GoogLeNet

5æ®µï¼Œ9ä¸ªInceptionå—

### Inception æœ‰å¤šç§åç»­å˜ç§

### æ€»ç»“

Inceptionå—ç”¨4æ¡æœ‰ä¸åŒè¶…å‚æ•°çš„å·ç§¯å±‚å’Œæ± åŒ–å±‚çš„è·¯æ¥æŠ½å–ä¸åŒçš„ä¿¡æ¯

â€‹		å®ƒçš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯æ¨¡å‹å‚æ•°å°ï¼Œè®¡ç®—å¤æ‚åº¦ä½

GoogLeNetä½¿ç”¨äº†9ä¸ªInceptionå—ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¾¾åˆ°ä¸Šç™¾å±‚çš„ç½‘ç»œ

â€‹		åç»­æœ‰ä¸€ç³»åˆ—æ”¹è¿›

## ä»£ç å®ç°

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

#Inceptionå—
class Inception(nn.Module):
    # `c1`--`c4` æ˜¯æ¯æ¡è·¯å¾„çš„è¾“å‡ºé€šé“æ•°
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # çº¿è·¯1ï¼Œå•1 x 1å·ç§¯å±‚
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # çº¿è·¯2ï¼Œ1 x 1å·ç§¯å±‚åæ¥3 x 3å·ç§¯å±‚
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # çº¿è·¯3ï¼Œ1 x 1å·ç§¯å±‚åæ¥5 x 5å·ç§¯å±‚
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # çº¿è·¯4ï¼Œ3 x 3æœ€å¤§æ±‡èšå±‚åæ¥1 x 1å·ç§¯å±‚
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # åœ¨é€šé“ç»´åº¦ä¸Šè¿ç»“è¾“å‡º
        return torch.cat((p1, p2, p3, p4), dim=1)

#å®ç°5ä¸ªstage
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))

#è®­ç»ƒæ¨¡å‹
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 28 æ‰¹é‡å½’ä¸€åŒ– batch normalization

## æ‰¹é‡å½’ä¸€åŒ–

é—®é¢˜ï¼š

æŸå¤±å‡ºç°åœ¨æœ€åï¼Œåé¢çš„å±‚è®­ç»ƒè¾ƒå¿«

æ•°æ®åœ¨æœ€åº•éƒ¨

â€‹	åº•éƒ¨çš„å±‚è®­ç»ƒè¾ƒæ…¢

â€‹	åº•éƒ¨å±‚ä¸€å˜åŒ–ï¼Œæ‰€æœ‰éƒ½å¾—è·Ÿç€å˜

â€‹	æœ€åçš„é‚£äº›å±‚éœ€è¦é‡æ–°å­¦ä¹ å¤šæ¬¡

â€‹	å¯¼è‡´æ”¶æ•›å˜æ…¢

æˆ‘ä»¬å¯ä»¥åœ¨å­¦ä¹ åº•éƒ¨å±‚çš„æ—¶å€™é¿å…å˜åŒ–é¡¶éƒ¨å±‚å—ï¼Ÿ

æ‰¹é‡å½’ä¸€åŒ–ï¼š

å›ºå®šå°æ‰¹é‡é‡Œé¢çš„å‡å€¼å’Œæ–¹å·®

ç„¶åå†åšé¢å¤–çš„è°ƒæ•´

### æ‰¹é‡å½’ä¸€åŒ–å±‚

å¯å­¦ä¹ çš„å‚æ•°ä¸ºä¼½é©¬å’Œè´å¡”

ä½œç”¨åœ¨

â€‹		å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å‡ºä¸Šï¼Œæ¿€æ´»å‡½æ•°å‰

â€‹		å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å…¥ä¸Š

å¯¹å…¨è¿æ¥å±‚ï¼Œä½œç”¨åœ¨ç‰¹å¾ç»´

å¯¹äºå·ç§¯å±‚ï¼Œä½œç”¨åœ¨é€šé“ç»´

### æ‰¹é‡å½’ä¸€åŒ–åœ¨åšä»€ä¹ˆï¼Ÿ

- æœ€åˆè®ºæ–‡æ˜¯æƒ³ç”¨å®ƒæ¥å‡å°‘å†…éƒ¨åå˜é‡è¿ç§»
- åç»­æœ‰è®ºæ–‡æŒ‡å‡ºå®ƒå¯èƒ½å°±æ˜¯é€šè¿‡åœ¨æ¯ä¸ªå°æ‰¹é‡é‡ŒåŠ å…¥å™ªéŸ³æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
- å› æ­¤æ²¡å¿…è¦è·Ÿä¸¢å¼ƒæ³•æ··åˆä½¿ç”¨

### æ€»ç»“

æ‰¹é‡å½’ä¸€åŒ–å›ºå®šå°æ‰¹é‡ä¸­çš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå­¦ä¹ å‡ºé€‚åˆçš„åç§»å’Œç¼©æ”¾

å¯ä»¥åŠ é€Ÿæ”¶æ•›é€Ÿåº¦ï¼Œä½†ä¸€èˆ¬ä¸æ”¹å˜æ¨¡å‹ç²¾åº¦

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l


def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # é€šè¿‡ `is_grad_enabled` æ¥åˆ¤æ–­å½“å‰æ¨¡å¼æ˜¯è®­ç»ƒæ¨¡å¼è¿˜æ˜¯é¢„æµ‹æ¨¡å¼
    if not torch.is_grad_enabled():
        # å¦‚æœæ˜¯åœ¨é¢„æµ‹æ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç§»åŠ¨å¹³å‡æ‰€å¾—çš„å‡å€¼å’Œæ–¹å·®
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # ä½¿ç”¨å…¨è¿æ¥å±‚çš„æƒ…å†µï¼Œè®¡ç®—ç‰¹å¾ç»´ä¸Šçš„å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # ä½¿ç”¨äºŒç»´å·ç§¯å±‚çš„æƒ…å†µï¼Œè®¡ç®—é€šé“ç»´ä¸Šï¼ˆaxis=1ï¼‰çš„å‡å€¼å’Œæ–¹å·®ã€‚
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒXçš„å½¢çŠ¶ä»¥ä¾¿åé¢å¯ä»¥åšå¹¿æ’­è¿ç®—
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œç”¨å½“å‰çš„å‡å€¼å’Œæ–¹å·®åšæ ‡å‡†åŒ–
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # æ›´æ–°ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # ç¼©æ”¾å’Œç§»ä½
    return Y, moving_mean.data, moving_var.data
  
class BatchNorm(nn.Module):
    # `num_features`ï¼šå®Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æˆ–å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚
    # `num_dims`ï¼š2è¡¨ç¤ºå®Œå…¨è¿æ¥å±‚ï¼Œ4è¡¨ç¤ºå·ç§¯å±‚
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # å‚ä¸æ±‚æ¢¯åº¦å’Œè¿­ä»£çš„æ‹‰ä¼¸å’Œåç§»å‚æ•°ï¼Œåˆ†åˆ«åˆå§‹åŒ–æˆ1å’Œ0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # éæ¨¡å‹å‚æ•°çš„å˜é‡åˆå§‹åŒ–ä¸º0å’Œ1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # å¦‚æœ `X` ä¸åœ¨å†…å­˜ä¸Šï¼Œå°† `moving_mean` å’Œ `moving_var`
        # å¤åˆ¶åˆ° `X` æ‰€åœ¨æ˜¾å­˜ä¸Š
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # ä¿å­˜æ›´æ–°è¿‡çš„ `moving_mean` å’Œ `moving_var`
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
      
      
#ç®€æ˜å®ç°
nn.BatchNorm1d()
nn.BatchNorm2d()
```

# 29 æ®‹å·®ç½‘ç»œ ResNet

## ResNet

åŠ æ›´å¤šçš„å±‚æ€»æ˜¯æ”¹è¿›ç²¾åº¦å—ï¼Ÿ

### æ®‹å·®å—

ä¸²è”ä¸€ä¸ªå±‚æ”¹å˜å‡½æ•°ç±»ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½æ‰©å¤§å‡½æ•°ç±»

æ®‹å·®å—åŠ å…¥å¿«é€Ÿé€šé“ï¼ˆå³è¾¹ï¼‰æ¥å¾—åˆ° f(x)=x+g(x)çš„ç»“æ„

### ResNetå—

é«˜å®½å‡åŠResNetå—ï¼ˆæ­¥å¹…2ï¼‰

åæ¥å¤šä¸ªé«˜å®½ä¸å˜ResNetå—

### ResNetæ¶æ„

ç±»ä¼¼VGGå’ŒGoogLenetçš„æ€»ä½“æ¶æ„

ä½†æ›¿æ¢æˆäº†ResNetå—

### æ€»ç»“

æ®‹å·®å—ä½¿å¾—å¾ˆæ·±çš„ç½‘ç»œæ›´åŠ å®¹æ˜“è®­ç»ƒ

â€‹	ç”šè‡³å¯ä»¥è®­ç»ƒä¸€åƒå±‚çš„ç½‘ç»œ

æ®‹å·®ç½‘ç»œå¯¹éšåçš„æ·±å±‚ç¥ç»ç½‘ç»œè®¾è®¡äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œæ— è®ºæ˜¯å·ç§¯ç±»ç½‘ç»œè¿˜æ˜¯å…¨è¿æ¥ç±»ç½‘ç»œã€‚

## ä»£ç å®ç°

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
      
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk
  
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))

net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))

X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
    
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 29.2 ResNetä¸ºä»€ä¹ˆèƒ½è®­ç»ƒå‡º1000å±‚çš„æ¨¡å‹

## ResNetçš„æ¢¯åº¦è®¡ç®—

é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼Œä¹˜æ³•å˜åŠ æ³•

![](pic/ResNetçš„æ¢¯åº¦è®¡ç®—.png)

# 30 ç¬¬äºŒéƒ¨åˆ†å®Œç»“ç«èµ›ï¼šå›¾ç‰‡åˆ†ç±»

# 31 æ·±åº¦å­¦ä¹ ç¡¬ä»¶ï¼šCPUå’ŒGPU

### æ€»ç»“

CPUï¼šå¯ä»¥å¤„ç†é€šç”¨è®¡ç®—ã€‚æ€§èƒ½ä¼˜åŒ–è€ƒè™‘æ•°æ®è¯»å†™æ•ˆç‡å’Œå¤šçº¿ç¨‹ã€‚

GPUï¼šä½¿ç”¨æ›´å¤šçš„å°æ ¸å’Œæ›´å¥½çš„å†…å­˜å¸¦å®½ï¼Œé€‚åˆèƒ½å¤§è§„æ¨¡å¹¶è¡Œçš„è®¡ç®—ä»»åŠ¡ã€‚

# 32 æ·±åº¦å­¦ä¹ ç¡¬ä»¶ï¼šTPUå’Œå…¶ä»–

# 33 å•æœºå¤šå¡å¹¶è¡Œ

## å¤šGPUå¹¶è¡Œ

### å•æœºå¤šå¡å¹¶è¡Œ

ä¸€å°æœºå™¨å¯ä»¥å®‰è£…å¤šä¸ªGPUï¼ˆ1-16ï¼‰

åœ¨è®­ç»ƒå’Œé¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå°æ‰¹é‡è®¡ç®—åˆ‡åˆ†åˆ°å¤šä¸ªGPUä¸Šæ¥è¾¾åˆ°åŠ é€Ÿç›®çš„

å¸¸è§åˆ‡åˆ†æ–¹æ¡ˆæœ‰ï¼š

æ•°æ®å¹¶è¡Œ

æ¨¡å‹å¹¶è¡Œ

é€šé“å¹¶è¡Œï¼ˆæ•°æ®+æ¨¡å‹å¹¶è¡Œï¼‰

### æ€»ç»“

å½“ä¸€ä¸ªæ¨¡å‹èƒ½ç”¨åˆ°å•å¡è®¡ç®—æ—¶ï¼Œé€šå¸¸ä½¿ç”¨æ•°æ®å¹¶è¡Œæ‰©å±•åˆ°å¤šå¡ä¸Š

æ¨¡å‹å¹¶è¡Œåˆ™ç”¨åœ¨è¶…å¤§æ¨¡å‹ä¸Š

# 34 å¤šGPUè®­ç»ƒå®ç°

## ä»é›¶å¼€å§‹

## ç®€æ´å®ç°

# 35 åˆ†å¸ƒå¼è®­ç»ƒ

### åˆ†å¸ƒå¼è®¡ç®—

### æ€»ç»“

åˆ†å¸ƒå¼åŒæ­¥æ•°æ®å¹¶è¡Œæ˜¯å¤šGPUæ•°æ®å¹¶è¡Œåœ¨å¤šæœºå™¨ä¸Šçš„æ‹“å±•

ç½‘ç»œé€šè®¯é€šå¸¸æ˜¯ç“¶é¢ˆ

éœ€è¦æ³¨æ„ä½¿ç”¨ç‰¹åˆ«å¤§çš„æ‰¹é‡å¤§å°æ—¶æ”¶æ•›æ•ˆç‡

æ›´å¤æ‚çš„åˆ†å¸ƒå¼æœ‰å¼‚æ­¥ã€æ¨¡å‹å¹¶è¡Œ

# 36 æ•°æ®å¢å¹¿

## æ•°æ®å¢å¹¿

### æ•°æ®å¢å¼º

å¢åŠ ä¸€ä¸ªå·²æœ‰æ•°æ®é›†ï¼Œä½¿å¾—æœ‰æ›´å¤šçš„å¤šæ ·æ€§

â€‹	åœ¨è¯­è¨€é‡Œé¢åŠ å…¥å„ç§ä¸åŒçš„èƒŒæ™¯å™ªéŸ³

â€‹	æ”¹å˜å›¾ç‰‡çš„é¢œè‰²å’Œå½¢çŠ¶

### ä½¿ç”¨å¢å¼ºæ•°æ®è®­ç»ƒ

### ä¾‹å­

ç¿»è½¬

â€‹		å·¦å³ç¿»è½¬

â€‹		ä¸Šä¸‹ç¿»è½¬ï¼ˆä¸æ€»æ˜¯å¯è¡Œï¼‰

åˆ‡å‰²

â€‹		ä»å›¾ç‰‡ä¸­åˆ‡å‰²ä¸€å—ï¼Œç„¶åå˜å½¢åˆ°å›ºå®šå½¢çŠ¶ï¼ˆéšæœºé«˜å®½æ¯”ã€å¤§å°ã€ä½ç½®ï¼‰

é¢œè‰²

â€‹		æ”¹å˜è‰²è°ƒï¼Œé¥±å’Œåº¦ï¼Œæ˜äº®åº¦

â€¦â€¦

## ä»£ç å®ç°

## å›¾åƒå¢å¹¿ image augmentation

```python
%matplotlib inline
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.Image.open('../img/cat1.jpg')
d2l.plt.imshow(img);

def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)

#æ°´å¹³ç¿»è½¬
apply(img, torchvision.transforms.RandomHorizontalFlip())
#ä¸Šä¸‹ç¿»è½¬
apply(img, torchvision.transforms.RandomVerticalFlip())
#éšæœºå‰ªè£
shape_aug = torchvision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
#éšæœºæ›´æ”¹å›¾åƒçš„äº®åº¦
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))

apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))
#éšæœºæ›´æ”¹äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦ã€è‰²è°ƒ
color_aug = torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
apply(img, color_aug)
#ç»“åˆå¤šç§å›¾åƒå¢å¹¿æ–¹æ³•
augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])
apply(img, augs)
#ä½¿ç”¨å›¾åƒå¢å¹¿è¿›è¡Œè®­ç»ƒ
all_images = torchvision.datasets.CIFAR10(train=True, root="../data",
                                          download=True)
d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);
	#åªä½¿ç”¨éšæœºå·¦å³ç¿»è½¬
train_augs = torchvision.transforms.Compose([
     torchvision.transforms.RandomHorizontalFlip(),
     torchvision.transforms.ToTensor()])

test_augs = torchvision.transforms.Compose([
     torchvision.transforms.ToTensor()])
#å®šä¹‰è¾…åŠ©å‡½æ•°ï¼Œä¾¿äºè¯»å–å›¾åƒå’Œåº”ç”¨å›¾åƒå¢å¹¿
def load_cifar10(is_train, augs, batch_size):
    dataset = torchvision.datasets.CIFAR10(root="../data", train=is_train,
                                           transform=augs, download=True)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())
    return dataloader
#å®šä¹‰å‡½æ•°ï¼Œä½¿ç”¨å¤šGPUå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„ä¼°  
#@save
def train_batch_ch13(net, X, y, loss, trainer, devices):
    if isinstance(X, list):
        # å¾®è°ƒBERTä¸­æ‰€éœ€ï¼ˆç¨åè®¨è®ºï¼‰
        X = [x.to(devices[0]) for x in X]
    else:
        X = X.to(devices[0])
    y = y.to(devices[0])
    net.train()
    trainer.zero_grad()
    pred = net(X)
    l = loss(pred, y)
    l.sum().backward()
    trainer.step()
    train_loss_sum = l.sum()
    train_acc_sum = d2l.accuracy(pred, y)
    return train_loss_sum, train_acc_sum
  
#@save
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus()):
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        # 4ä¸ªç»´åº¦ï¼šå‚¨å­˜è®­ç»ƒæŸå¤±ï¼Œè®­ç»ƒå‡†ç¡®åº¦ï¼Œå®ä¾‹æ•°ï¼Œç‰¹ç‚¹æ•°
        metric = d2l.Accumulator(4)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = train_batch_ch13(
                net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0], labels.numel())
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {metric[0] / metric[2]:.3f}, train acc '
          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
          f'{str(devices)}')
    
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)

def init_weights(m):
    if type(m) in [nn.Linear, nn.Conv2d]:
        nn.init.xavier_uniform_(m.weight)

net.apply(init_weights)

def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    loss = nn.CrossEntropyLoss(reduction="none")
    trainer = torch.optim.Adam(net.parameters(), lr=lr)
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
    
train_with_data_aug(train_augs, test_augs, net)
```

# 37 å¾®è°ƒ fine-tuning

è¿ç§»å­¦ä¹ 

## å¾®è°ƒ

### æ ‡æ³¨ä¸€ä¸ªæ•°æ®é›†å¾ˆè´µ

å¸Œæœ›å€ŸåŠ©å¤§æ•°æ®é›†çš„å¸®åŠ©ï¼Œå®Œæˆå°æ•°æ®é›†è®­ç»ƒçš„ä¼˜åŒ–

### ç½‘ç»œæ¶æ„

ä¸€ä¸ªç¥ç»ç½‘ç»œä¸€èˆ¬å¯ä»¥åˆ†æˆä¸¤å—

1.ç‰¹å¾æå–å°†åŸå§‹åƒç´ ç¼–ç¨‹å®¹æ˜“çº¿æ€§åˆ†å‰²çš„ç‰¹å¾ ï¼ˆç‰¹å¾æå–ï¼‰

2.çº¿æ€§åˆ†ç±»å™¨æ¥åšåˆ†ç±»

### å¾®è°ƒ

æºæ•°æ®é›†=>ç›®æ ‡æ•°æ®é›†

ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œå› ä¸ºæ ‡å·å¯èƒ½å˜äº†

å¯èƒ½ä»»ç„¶å¯ä»¥å¯¹æˆ‘æ•°æ®é›†åšç‰¹å¾æå–

### å¾®è°ƒä¸­çš„æƒé‡åˆå§‹åŒ–

![](pic/å¾®è°ƒä¸­çš„æƒé‡åˆå§‹åŒ–.png)

### è®­ç»ƒ

æ˜¯åœ¨ä¸€ä¸ªç›®æ ‡æ•°æ®é›†ä¸Šçš„æ­£å¸¸è®­ç»ƒä»»åŠ¡ï¼Œä½†ä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–

ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡

ä½¿ç”¨æ›´å°çš„æ•°æ®è¿­ä»£

æºæ•°æ®é›†è¿œå¤æ‚äºç›®æ ‡æ•°æ®ï¼Œé€šå¸¸å¾®è°ƒæ•ˆæœæ›´å¥½

### é‡ç”¨åˆ†ç±»å™¨æƒé‡

æºæ•°æ®é›†å¯èƒ½ä¹Ÿæœ‰ç›®æ ‡æ•°æ®ä¸­çš„æ ‡å·

å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒå¥½æ¨¡å‹åˆ†ç±»å™¨ä¸­å¯¹åº”æ ‡å·å¯¹åº”çš„å‘é‡æ¥åšåˆå§‹åŒ–

### å›ºå®šä¸€äº›å±‚

ç¥ç»ç½‘ç»œé€šå¸¸å­¦ä¹ æœ‰å±‚æ¬¡çš„ç‰¹å¾è¡¨ç¤º

â€‹		ä½å±‚æ¬¡çš„ç‰¹å¾æ›´åŠ é€šç”¨

â€‹		é«˜å±‚æ¬¡çš„ç‰¹å¾åˆ™æ›´è·Ÿæ•°æ®é›†ç›¸å…³

å¯ä»¥å›ºå®šåº•éƒ¨ä¸€äº›å±‚çš„å‚æ•°ï¼Œä¸å‚ä¸æ›´æ–°

â€‹		æ›´å¼ºçš„æ­£åˆ™

### æ€»ç»“

å¾®è°ƒé€šè¿‡ä½¿ç”¨åœ¨å¤§æ•°æ®ä¸Šå¾—åˆ°çš„é¢„è®­ç»ƒå¥½çš„æ¨¡å‹æ¥åˆå§‹åŒ–æ¨¡å‹æƒé‡æ¥å®Œæˆæå‡ç²¾åº¦

é¢„è®­ç»ƒæ¨¡å‹è´¨é‡å¾ˆé‡è¦

å¾®è°ƒé€šå¸¸é€Ÿåº¦æ›´å¿«ã€ç²¾åº¦æ›´é«˜

## ä»£ç å®ç°

```python
%matplotlib inline
import os
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

#çƒ­ç‹—æ•°æ®é›†æ¥æºäºç½‘ç»œ
#@save
d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip', 
                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')

data_dir = d2l.download_extract('hotdog')

train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))

hotdogs = [train_imgs[i][0] for i in range(8)]
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]
d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);

#æ•°æ®å¢å¹¿
# ä½¿ç”¨ä¸‰ä¸ªRGBé€šé“çš„å‡å€¼å’Œæ ‡å‡†åå·®ï¼Œä»¥æ ‡å‡†åŒ–æ¯ä¸ªé€šé“
normalize = torchvision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    normalize])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    normalize])

#å®šä¹‰å’Œåˆå§‹åŒ–æ¨¡å‹
pretrained_net = torchvision.models.resnet18(pretrained=True)

pretrained_net.fc

finetune_net = torchvision.models.resnet18(pretrained=True)
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
nn.init.xavier_uniform_(finetune_net.fc.weight);

# å¦‚æœ `param_group=True`ï¼Œè¾“å‡ºå±‚ä¸­çš„æ¨¡å‹å‚æ•°å°†ä½¿ç”¨åå€çš„å­¦ä¹ ç‡
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                      param_group=True):
    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train'), transform=train_augs),
        batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'test'), transform=test_augs),
        batch_size=batch_size)
    devices = d2l.try_all_gpus()
    loss = nn.CrossEntropyLoss(reduction="none")
    if param_group:
        params_1x = [param for name, param in net.named_parameters()
             if name not in ["fc.weight", "fc.bias"]]
        trainer = torch.optim.SGD([{'params': params_1x},
                                   {'params': net.fc.parameters(),
                                    'lr': learning_rate * 10}],
                                lr=learning_rate, weight_decay=0.001)
    else:
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)    
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)

#ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
train_fine_tuning(finetune_net, 5e-5)
#å¯¹æ¯”ï¼Œä¸ç”¨å¾®è°ƒ
scratch_net = torchvision.models.resnet18()
scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)
train_fine_tuning(scratch_net, 5e-4, param_group=False)
```



# 38 ç¬¬äºŒæ¬¡ç«èµ› æ ‘å¶åˆ†ç±» ç»“æœ

# 39 å®æˆ˜ Kaggleæ¯”èµ›ï¼šå›¾åƒåˆ†ç±»ï¼ˆCIFAR-10ï¼‰

# 40 å®æˆ˜ Kaggleæ¯”èµ›ï¼šç‹—çš„å“ç§è¯†åˆ«ï¼ˆImageNet Dogsï¼‰

# 41 ç‰©ä½“æ£€æµ‹å’Œæ•°æ®é›† object detection

## ç‰©ä½“æ£€æµ‹

### è¾¹ç¼˜æ¡†

é€šè¿‡å››ä¸ªæ•°å­—å®šä¹‰ å·¦ä¸Šx,å·¦ä¸Šy,å³ä¸‹x,å³ä¸‹y æˆ–å·¦ä¸Šx,å·¦ä¸Šy,å®½,é«˜

### ç›®æ ‡æ£€æµ‹æ•°æ®é›†

æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªç‰©ä½“    å›¾ç‰‡æ–‡ä»¶åï¼Œç‰©ä½“ç±»åˆ«ï¼Œè¾¹ç¼˜æ¡†

COCOï¼ˆcocodataset.orgï¼‰

### æ€»ç»“

ç‰©ä½“æ£€æµ‹è¯†åˆ«å›¾ç‰‡é‡Œçš„å¤šä¸ªç‰©ä½“çš„ç±»åˆ«å’Œä½ç½®

ä½ç½®é€šå¸¸ç”¨è¾¹ç¼˜æ¡†è¡¨ç¤º

## è¾¹ç¼˜æ¡†å®ç°

ç›®æ ‡æ£€æµ‹å’Œè¾¹ç•Œæ¡†

```python
%matplotlib inline
import torch
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.plt.imread('../img/catdog.jpg')
d2l.plt.imshow(img);

#@save å®šä¹‰åœ¨è¿™ä¸¤ç§è¡¨ç¤ºä¹‹é—´è¿›è¡Œè½¬æ¢çš„å‡½æ•°
def box_corner_to_center(boxes):
    """ä»ï¼ˆå·¦ä¸Šï¼Œå³ä¸‹ï¼‰è½¬æ¢åˆ°ï¼ˆä¸­é—´ï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes

#@save
def box_center_to_corner(boxes):
    """ä»ï¼ˆä¸­é—´ï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰è½¬æ¢åˆ°ï¼ˆå·¦ä¸Šï¼Œå³ä¸‹ï¼‰"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
  
# bboxæ˜¯è¾¹ç•Œæ¡†çš„è‹±æ–‡ç¼©å†™
#å®šä¹‰å›¾åƒä¸­ç‹—å’ŒçŒ«çš„è¾¹ç•Œæ¡†
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]

boxes = torch.tensor((dog_bbox, cat_bbox))
box_center_to_corner(box_corner_to_center(boxes)) == boxes

#@save å°†è¾¹ç•Œæ¡†åœ¨å›¾ä¸­ç”»å‡º
def bbox_to_rect(bbox, color):
    # å°†è¾¹ç•Œæ¡† (å·¦ä¸Šx, å·¦ä¸Šy, å³ä¸‹x, å³ä¸‹y) æ ¼å¼è½¬æ¢æˆ matplotlib æ ¼å¼ï¼š
    # ((å·¦ä¸Šx, å·¦ä¸Šy), å®½, é«˜)
    return d2l.plt.Rectangle(
        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
        fill=False, edgecolor=color, linewidth=2)
  
fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));
```

## æ•°æ®é›†

ç›®æ ‡æ£€æµ‹æ•°æ®é›†

```python
%matplotlib inline
import os
import pandas as pd
import torch
import torchvision
from d2l import torch as d2l

#@save
d2l.DATA_HUB['banana-detection'] = (
    d2l.DATA_URL + 'banana-detection.zip',
    '5de26c8fce5ccdea9f91267273464dc968d20d72')

#@save  è¯»å–é¦™è•‰æ£€æµ‹æ•°æ®é›†
def read_data_bananas(is_train=True):
    """è¯»å–é¦™è•‰æ£€æµ‹æ•°æ®é›†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ã€‚"""
    data_dir = d2l.download_extract('banana-detection')
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
                             else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else
                         'bananas_val', 'images', f'{img_name}')))
        # Here `target` contains (class, upper-left x, upper-left y,
        # lower-right x, lower-right y), where all the images have the same
        # banana class (index 0)
        targets.append(list(target))
    return images, torch.tensor(targets).unsqueeze(1) / 256
  
#@save  åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰Datasetå®ä¾‹
class BananasDataset(torch.utils.data.Dataset):
    """ä¸€ä¸ªç”¨äºåŠ è½½é¦™è•‰æ£€æµ‹æ•°æ®é›†çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚"""
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features)) + (f' training examples' if
              is_train else f' validation examples'))

    def __getitem__(self, idx):
        return (self.features[idx].float(), self.labels[idx])

    def __len__(self):
        return len(self.features)
      
#@save  ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿”å›ä¸¤ä¸ªæ•°æ®åŠ è½½å™¨å®ä¾‹
def load_data_bananas(batch_size):
    """åŠ è½½é¦™è•‰æ£€æµ‹æ•°æ®é›†ã€‚"""
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)
    return train_iter, val_iter
  
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
batch[0].shape, batch[1].shape

imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch[1][0:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
```

# 42 é”šæ¡† anchor

## é”šæ¡†

ä¸€ç±»ç›®æ ‡æ£€æµ‹ç®—æ³•æ˜¯åŸºäºé”šæ¡†çš„

- â€‹		æå‡ºå¤šä¸ªè¢«ç§°ä¸ºé”šæ¡†çš„åŒºåŸŸï¼ˆè¾¹ç¼˜æ¡†ï¼‰
- â€‹		é¢„æµ‹æ¯ä¸ªé”šæ¡†é‡Œæ˜¯å¦å«æœ‰å…³æ³¨çš„ç‰©ä½“
- â€‹		å¦‚æœæ˜¯ï¼Œé¢„æµ‹ä»è¿™ä¸ªé”šæ¡†åˆ°çœŸå®è¾¹ç¼˜æ¡†çš„åç§»

### IoU-äº¤å¹¶æ¯”

IoUç”¨æ¥è®¡ç®—ä¸¤ä¸ªæ¡†ä¹‹é—´çš„ç›¸ä¼¼åº¦

0è¡¨ç¤ºæ— é‡å ï¼Œ1è¡¨ç¤ºé‡åˆ

è¿™æ˜¯JacquardæŒ‡æ•°çš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µ

![](pic/IoU.png)

### èµ‹äºˆé”šæ¡†æ ‡å·

æ¯ä¸ªé”šæ¡†æ˜¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬

å°†æ¯ä¸ªé”šæ¡†ï¼Œè¦ä¹ˆæ ‡æ³¨æˆèƒŒæ™¯ï¼Œè¦ä¹ˆå…³è”ä¸Šä¸€ä¸ªçœŸå®è¾¹ç¼˜æ¡†

æˆ‘ä»¬å¯èƒ½ä¼šç”Ÿæˆå¤§é‡çš„é”šæ¡†

â€‹	è¿™ä¸ªå¯¼è‡´å¤§é‡çš„è´Ÿç±»æ ·æœ¬

### ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰è¾“å‡º

æ¯ä¸ªé”šæ¡†é¢„æµ‹ä¸€ä¸ªè¾¹ç¼˜æ¡†

NMSå¯ä»¥åˆå¹¶ç›¸ä¼¼çš„é¢„æµ‹

â€‹	é€‰ä¸­æ˜¯éèƒŒæ™¯ç±»çš„æœ€å¤§é¢„æµ‹å€¼

â€‹	å»æ‰æ‰€æœ‰å…¶ä»–å’Œå®ƒIoUå€¼å¤§äºcitaçš„é¢„æµ‹

â€‹	é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°æ‰€æœ‰é¢„æµ‹è¦ä¹ˆè¢«é€‰ä¸­ï¼Œè¦ä¹ˆè¢«å»æ‰

### æ€»ç»“

ä¸€ç±»ç›®æ ‡æ£€æµ‹ç®—æ³•åŸºäºé”šæ¡†æ¥é¢„æµ‹

é¦–å…ˆç”Ÿæˆå¤§é‡é”šæ¡†ï¼Œå¹¶èµ‹äºˆæ ‡å·ï¼Œæ¯ä¸ªé”šæ¡†ä½œä¸ºä¸€ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ

åœ¨é¢„æµ‹æ—¶ï¼Œä½¿ç”¨NMSæ¥å»æ‰å†—ä½™çš„é¢„æµ‹

## ä»£ç å®ç°ï¼ˆlaterï¼‰

```python
%matplotlib inline
import torch
from d2l import torch as d2l

torch.set_printoptions(2)  # ç²¾ç®€æ‰“å°ç²¾åº¦

#@save
def multibox_prior(data, sizes, ratios):
    """ç”Ÿæˆä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒå…·æœ‰ä¸åŒå½¢çŠ¶çš„é”šæ¡†ã€‚"""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = torch.tensor(sizes, device=device)
    ratio_tensor = torch.tensor(ratios, device=device)

    # ä¸ºäº†å°†é”šç‚¹ç§»åŠ¨åˆ°åƒç´ çš„ä¸­å¿ƒï¼Œéœ€è¦è®¾ç½®åç§»é‡ã€‚
    # å› ä¸ºä¸€ä¸ªåƒç´ çš„çš„é«˜ä¸º1ä¸”å®½ä¸º1ï¼Œæˆ‘ä»¬é€‰æ‹©åç§»æˆ‘ä»¬çš„ä¸­å¿ƒ0.5
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # Scaled steps in y axis
    steps_w = 1.0 / in_width  # Scaled steps in x axis

    # ç”Ÿæˆé”šæ¡†çš„æ‰€æœ‰ä¸­å¿ƒç‚¹
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    shift_y, shift_x = torch.meshgrid(center_h, center_w)
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    # ç”Ÿæˆâ€œboxes_per_pixelâ€ä¸ªé«˜å’Œå®½ï¼Œ
    # ä¹‹åç”¨äºåˆ›å»ºé”šæ¡†çš„å››è§’åæ ‡ (xmin, xmax, ymin, ymax)
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                   * in_height / in_width  # Handle rectangular inputs
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    # é™¤ä»¥2æ¥è·å¾—åŠé«˜å’ŒåŠå®½
    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(
                                        in_height * in_width, 1) / 2

    # æ¯ä¸ªä¸­å¿ƒç‚¹éƒ½å°†æœ‰â€œboxes_per_pixelâ€ä¸ªé”šæ¡†ï¼Œ
    # æ‰€ä»¥ç”Ÿæˆå«æ‰€æœ‰é”šæ¡†ä¸­å¿ƒçš„ç½‘æ ¼ï¼Œé‡å¤äº†â€œboxes_per_pixelâ€æ¬¡
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)
  
img = d2l.plt.imread('../img/catdog.jpg')
h, w = img.shape[:2]

print(h, w)
X = torch.rand(size=(1, 3, h, w))
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape

boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]

#@save
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """æ˜¾ç¤ºæ‰€æœ‰è¾¹ç•Œæ¡†ã€‚"""
    def _make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj
    labels = _make_list(labels)
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)
        axes.add_patch(rect)
        if labels and len(labels) > i:
            text_color = 'k' if color == 'w' else 'w'
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))
            
d2l.set_figsize()
bbox_scale = torch.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])

#@save
def box_iou(boxes1, boxes2):
    """è®¡ç®—ä¸¤ä¸ªé”šæ¡†æˆ–è¾¹ç•Œæ¡†åˆ—è¡¨ä¸­æˆå¯¹çš„äº¤å¹¶æ¯”ã€‚"""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # `boxes1`, `boxes2`, `areas1`, `areas2`çš„å½¢çŠ¶: 
    # `boxes1`ï¼š(boxes1çš„æ•°é‡, 4),
    # `boxes2`ï¼š(boxes2çš„æ•°é‡, 4), 
    # `areas1`ï¼š(boxes1çš„æ•°é‡,), 
    # `areas2`ï¼š(boxes2çš„æ•°é‡,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    #  `inter_upperlefts`, `inter_lowerrights`, `inters`çš„å½¢çŠ¶: 
    # (boxes1çš„æ•°é‡, boxes2çš„æ•°é‡, 2)
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # `inter_areas` and `union_areas`çš„å½¢çŠ¶: (boxes1çš„æ•°é‡, boxes2çš„æ•°é‡)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
  
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """å°†æœ€æ¥è¿‘çš„çœŸå®è¾¹ç•Œæ¡†åˆ†é…ç»™é”šæ¡†ã€‚"""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # ä½äºç¬¬iè¡Œå’Œç¬¬jåˆ—çš„å…ƒç´  x_ij æ˜¯é”šæ¡†iå’ŒçœŸå®è¾¹ç•Œæ¡†jçš„IoU
    jaccard = box_iou(anchors, ground_truth)
    # å¯¹äºæ¯ä¸ªé”šæ¡†ï¼Œåˆ†é…çš„çœŸå®è¾¹ç•Œæ¡†çš„å¼ é‡
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device)
    # æ ¹æ®é˜ˆå€¼ï¼Œå†³å®šæ˜¯å¦åˆ†é…çœŸå®è¾¹ç•Œæ¡†
    max_ious, indices = torch.max(jaccard, dim=1)
    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)
    box_j = indices[max_ious >= 0.5]
    anchors_bbox_map[anc_i] = box_j
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)
        box_idx = (max_idx % num_gt_boxes).long()
        anc_idx = (max_idx / num_gt_boxes).long()
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
  
#@save
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """å¯¹é”šæ¡†åç§»é‡çš„è½¬æ¢ã€‚"""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset

#@save
def multibox_target(anchors, labels):
    """ä½¿ç”¨çœŸå®è¾¹ç•Œæ¡†æ ‡è®°é”šæ¡†ã€‚"""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        # å°†ç±»æ ‡ç­¾å’Œåˆ†é…çš„è¾¹ç•Œæ¡†åæ ‡åˆå§‹åŒ–ä¸ºé›¶
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        # ä½¿ç”¨çœŸå®è¾¹ç•Œæ¡†æ¥æ ‡è®°é”šæ¡†çš„ç±»åˆ«ã€‚
        # å¦‚æœä¸€ä¸ªé”šæ¡†æ²¡æœ‰è¢«åˆ†é…ï¼Œæˆ‘ä»¬æ ‡è®°å…¶ä¸ºèƒŒæ™¯ï¼ˆå€¼ä¸ºé›¶ï¼‰
        indices_true = torch.nonzero(anchors_bbox_map >= 0)
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # åç§»é‡è½¬æ¢
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
  
#ä¸€ä¸ªä¾‹å­
#â€¦â€¦

#@save
def offset_inverse(anchors, offset_preds):
    """æ ¹æ®å¸¦æœ‰é¢„æµ‹åç§»é‡çš„é”šæ¡†æ¥é¢„æµ‹è¾¹ç•Œæ¡†ã€‚"""
    anc = d2l.box_corner_to_center(anchors)
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)
    return predicted_bbox

#@save
def nms(boxes, scores, iou_threshold):
    """å¯¹é¢„æµ‹è¾¹ç•Œæ¡†çš„ç½®ä¿¡åº¦è¿›è¡Œæ’åºã€‚"""
    B = torch.argsort(scores, dim=-1, descending=True)
    keep = []  # ä¿ç•™é¢„æµ‹è¾¹ç•Œæ¡†çš„æŒ‡æ ‡
    while B.numel() > 0:
        i = B[0]
        keep.append(i)
        if B.numel() == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
        B = B[inds + 1]
    return torch.tensor(keep, device=boxes.device)
  
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶æ¥é¢„æµ‹è¾¹ç•Œæ¡†ã€‚"""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = torch.max(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)

        # æ‰¾åˆ°æ‰€æœ‰çš„ non_keep ç´¢å¼•ï¼Œå¹¶å°†ç±»è®¾ç½®ä¸ºèƒŒæ™¯
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        combined = torch.cat((keep, all_idx))
        uniques, counts = combined.unique(return_counts=True)
        non_keep = uniques[counts == 1]
        all_id_sorted = torch.cat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # `pos_threshold` æ˜¯ä¸€ä¸ªç”¨äºéèƒŒæ™¯é¢„æµ‹çš„é˜ˆå€¼
        below_min_idx = (conf < pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = torch.cat((class_id.unsqueeze(1),
                               conf.unsqueeze(1),
                               predicted_bb), dim=1)
        out.append(pred_info)
    return torch.stack(out)
  

```

# 43 æ ‘å¶åˆ†ç±»ç«èµ›æŠ€æœ¯æ€»ç»“

# 44 ç‰©ä½“æ£€æµ‹ç®—æ³• R-CNNï¼ŒSSDï¼ŒYOLO

## ç›®æ ‡æ£€æµ‹

### åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œ

#### R-CNN

ä½¿ç”¨å¯å‘å¼æœç´¢ç®—æ³•æ¥é€‰æ‹©é”šæ¡†

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥å¯¹æ¯ä¸ªé”šæ¡†æŠ½å–ç‰¹å¾

è®­ç»ƒä¸€ä¸ªSVMæ¥å¯¹ç±»åˆ«åˆ†ç±»

è®­ç»ƒä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹æ¥é¢„æµ‹è¾¹ç¼˜æ¡†åç§»

![](pic/R-CNN.png)

#### å…´è¶£åŒºåŸŸï¼ˆRoIï¼‰æ± åŒ–å±‚

ç»™å®šä¸€ä¸ªé”šæ¡†ï¼Œå‡åŒ€åˆ†å‰²æˆnxmå—ï¼Œè¾“å‡ºæ¯å—é‡Œçš„æœ€å¤§å€¼

ä¸ç®¡é”šæ¡†å¤šå¤§ï¼Œæ€»æ˜¯è¾“å‡ºnmä¸ªå€¼

#### Fast RCNN

ä½¿ç”¨CNNå¯¹å›¾ç‰‡æŠ½å–ç‰¹å¾

ä½¿ç”¨ROIæ± åŒ–å±‚å¯¹æ¯ä¸ªé”šæ¡†ç”Ÿæˆå›ºå®šé•¿åº¦ç‰¹å¾

#### Faster R-CNN

ä½¿ç”¨ä¸€ä¸ªåŒºåŸŸæè®®ç½‘ç»œæ¥æ›¿ä»£å¯å‘å¼æœç´¢æ¥è·å¾—æ›´å¥½çš„é”šæ¡†

#### Mask R-CNN

å¦‚æœæœ‰åƒç´ çº§åˆ«çš„æ ‡å·ï¼Œåˆ™ä½¿ç”¨FCNæ¥åˆ©ç”¨è¿™äº›ä¿¡æ¯

### æ€»ç»“

R-CNNæ˜¯æœ€æ—©ã€ä¹Ÿæ˜¯æœ€æœ‰åçš„ä¸€ç±»åŸºäºé”šæ¡†å’ŒCNNçš„ç›®æ ‡æ£€æµ‹ç®—æ³•

Fast/Faster R-CNN æŒç»­æå‡æ€§èƒ½

Faster R-CNNå’ŒMask R-CNN æ˜¯åœ¨è¿½æ±‚é«˜ç²¾åº¦åœºæ™¯ä¸‹çš„å¸¸ç”¨ç®—æ³•

### å•å‘å¤šæ¡†æ£€æµ‹ï¼ˆSSDï¼‰

#### ç”Ÿæˆé”šæ¡†

å¯¹æ¯ä¸ªåƒç´ ï¼Œç”Ÿæˆå¤šä¸ªä»¥å®ƒä¸ºä¸­å¿ƒçš„é”šæ¡†

#### SSDæ¨¡å‹

ä¸€ä¸ªåŸºç¡€ç½‘ç»œæ¥æŠ½å–ç‰¹å¾ï¼Œç„¶åå¤šä¸ªå·ç§¯å±‚å—æ¥å‡åŠé«˜å®½

åœ¨æ¯æ®µéƒ½ç”Ÿæˆé”šæ¡†

â€‹	åº•éƒ¨æ®µæ¥æ‹Ÿåˆå°ç‰©ä½“ï¼Œé¡¶éƒ¨æ®µæ¥æ‹Ÿåˆå¤§ç‰©ä½“

å¯¹æ¯ä¸ªé”šæ¡†é¢„æµ‹ç±»åˆ«å’Œè¾¹ç¼˜æ¡†

### æ€»ç»“

SSDé€šè¿‡å•ç¥ç»ç½‘ç»œæ¥æ£€æµ‹æ¨¡å‹

ä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒçš„äº§ç”Ÿå¤šä¸ªé”šæ¡†

åœ¨å¤šä¸ªæ®µçš„è¾“å‡ºä¸Šè¿›è¡Œå¤šå°ºåº¦çš„æ£€æµ‹

### YOLO

you only look once

SSDä¸­é”šæ¡†å¤§é‡é‡å ï¼Œå› æ­¤æµªè´¹äº†å¾ˆå¤šè®¡ç®—

YOLOå°†å›¾ç‰‡å‡åŒ€åˆ†æˆSxSä¸ªé”šæ¡†

æ¯ä¸ªé”šæ¡†é¢„æµ‹Bä¸ªè¾¹ç¼˜æ¡†

åç»­ç‰ˆæœ¬æœ‰æŒç»­æ”¹è¿›

# 45 SSDå®ç°

## å¤šå°ºåº¦é”šæ¡†

## SSD

# 46 è¯­ä¹‰åˆ†å‰²å’Œæ•°æ®é›†

## è¯­ä¹‰åˆ†å‰²

è¯­ä¹‰åˆ†å‰²å°†å›¾ç‰‡ä¸­çš„æ¯ä¸ªåƒç´ åˆ†ç±»åˆ°å¯¹åº”çš„ç±»åˆ«

å›¾ç‰‡åˆ†ç±»=>ç›®æ ‡æ£€æµ‹=>è¯­ä¹‰åˆ†å‰²

![](pic/è¯­ä¹‰åˆ†å‰².png)

### åº”ç”¨ï¼šèƒŒæ™¯è™šåŒ–

### åº”ç”¨ï¼šè·¯é¢åˆ†å‰²

### VS å®ä¾‹åˆ†å‰²

![](pic/å®ä¾‹åˆ†å‰².png)

## è¯­ä¹‰åˆ†å‰²æ•°æ®é›†

æœ€é‡è¦çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¹‹ä¸€æ˜¯Pascal VOC2012

# 47 è½¬ç½®å·ç§¯

## è½¬ç½®å·ç§¯

å·ç§¯ä¸ä¼šå¢å¤§è¾“å…¥çš„é«˜å®½ï¼Œé€šå¸¸è¦ä¹ˆä¸å˜ã€è¦ä¹ˆå‡åŠ

è½¬ç½®å·ç§¯åˆ™å¯ä»¥ç”¨æ¥å¢å¤§è¾“å…¥é«˜å®½

### ä¸ºä»€ä¹ˆç§°ä¹‹ä¸ºâ€œè½¬ç½®â€

## ä»£ç å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

def trans_conv(X, K):
    h, w = K.shape
    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Y[i: i + h, j: j + w] += X[i, j] * K
    return Y
```

# 47.2 è½¬ç½®å·ç§¯æ˜¯ä¸€ç§å·ç§¯

## è½¬ç½®å·ç§¯

è½¬ç½®å·ç§¯æ˜¯ä¸€ç§å·ç§¯

å®ƒå°†è¾“å…¥å’Œæ ¸è¿›è¡Œäº†é‡æ–°æ’åˆ—

åŒå·ç§¯ä¸€èˆ¬æ˜¯åšä¸‹é‡‡æ ·ä¸åŒï¼Œå®ƒé€šå¸¸ç”¨ä½œä¸Šé‡‡æ ·

![](pic/é‡æ–°æ’åˆ—è¾“å…¥å’Œæ ¸.png)

### æ€»ç»“

è½¬ç½®å·ç§¯æ˜¯ä¸€ç§å˜åŒ–äº†è¾“å…¥å’Œæ ¸çš„å·ç§¯ï¼Œæ¥å¾—åˆ°ä¸Šé‡‡ç”¨çš„ç›®çš„

ä¸ç­‰åŒäºæ•°å­¦ä¸Šçš„åå·ç§¯æ“ä½œ

# 48 å…¨è¿æ¥å·ç§¯ç¥ç»ç½‘ç»œFCN

## FCN

## ä»£ç å®ç°

# 49 æ ·å¼è¿ç§»

## æ ·å¼è¿ç§»

## ä»£ç å®ç°

# 50 è¯¾ç¨‹ç«èµ›ï¼šç‰›ä»”è¡Œå¤´æ£€æµ‹

# 51 åºåˆ—æ¨¡å‹ sequence

## åºåˆ—æ¨¡å‹

### åºåˆ—æ•°æ®

### ç»Ÿè®¡å·¥å…·

åœ¨æ—¶é—´tè§‚å¯Ÿåˆ°xtï¼Œé‚£ä¹ˆå¾—åˆ°Tä¸ªä¸ç‹¬ç«‹çš„éšæœºå˜é‡

ä½¿ç”¨æ¡ä»¶æ¦‚ç‡å±•å¼€

å¯¹è§è¿‡çš„æ•°æ®å»ºæ¨¡ï¼Œä¹Ÿç§°è‡ªå›å½’æ¨¡å‹

### æ–¹æ¡ˆA-é©¬å°”å¯å¤«å‡è®¾

å‡è®¾å½“å‰æ•°æ®åªè·Ÿğœä¸ªè¿‡å»æ•°æ®ç‚¹ç›¸å…³

### æ–¹æ¡ˆB-æ½œå˜é‡æ¨¡å‹

å¼•å…¥æ½œå˜é‡htæ¥è¡¨ç¤ºè¿‡å»ä¿¡æ¯   æ‹†æˆ2ä¸ªæ¨¡å‹

### æ€»ç»“

æ—¶åºæ¨¡å‹ä¸­ï¼Œå½“å‰æ•°æ®è·Ÿä¹‹å‰è§‚æµ‹åˆ°çš„æ•°æ®ç›¸å…³

è‡ªå›å½’æ¨¡å‹ä½¿ç”¨è‡ªèº«è¿‡å»æ•°æ®æ¥é¢„æµ‹æœªæ¥

é©¬å°”å¯å¤«æ¨¡å‹å‡è®¾å½“å‰åªè·Ÿæœ€è¿‘å°‘æ•°æ•°æ®ç›¸å…³ï¼Œä»è€Œç®€åŒ–æ¨¡å‹

æ½œå˜é‡æ¨¡å‹ä½¿ç”¨æ½œå˜é‡æ¥æ¦‚æ‹¬å†å²ä¿¡æ¯

## ä»£ç å®ç°

é©¬å°”å¯å¤«æ–¹æ³•

```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

#ä½¿ç”¨æ­£å¼¦å‡½æ•°å’Œä¸€äº›å¯åŠ æ€§å™ªå£°æ¥ç”Ÿæˆåºåˆ—æ•°æ®ï¼Œæ—¶é—´æ­¥ä¸º1ï¼Œ2ï¼Œâ€¦â€¦ï¼Œ1000
T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹  
time = torch.arange(1, T + 1, dtype=d2l.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))

#å°†æ•°æ®æ˜ å°„ä¸ºæ•°æ®å¯¹
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))

batch_size, n_train = 16, 600
# åªæœ‰å‰`n_train`ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)

# åˆå§‹åŒ–ç½‘ç»œæƒé‡çš„å‡½æ•°
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

# ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº
def get_net():
    net = nn.Sequential(nn.Linear(4, 10),
                        nn.ReLU(),
                        nn.Linear(10, 1))
    net.apply(init_weights)
    return net

# å¹³æ–¹æŸå¤±
loss = nn.MSELoss()

def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)

onestep_preds = net(features)
d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))

#è¿›è¡Œå¤šæ­¥é¢„æµ‹
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(
        multistep_preds[i - tau:i].reshape((1, -1)))
    
d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))

max_steps = 64

features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))
# åˆ— `i` (`i` < `tau`) æ˜¯æ¥è‡ª `x` çš„è§‚æµ‹
# å…¶æ—¶é—´æ­¥ä» `i + 1` åˆ° `i + T - tau - max_steps + 1`
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# åˆ— `i` (`i` >= `tau`) æ˜¯ (`i - tau + 1`)æ­¥çš„é¢„æµ‹
# å…¶æ—¶é—´æ­¥ä» `i + 1` åˆ° `i + T - tau - max_steps + 1`
for i in range(tau, tau + max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)
    
steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
         [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',
         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
         figsize=(6, 3))
```

# 52 æ–‡æœ¬é¢„å¤„ç† text preprocessing

## ä»£ç å®ç°

```python
import collections
import re
from d2l import torch as d2l

#å°†æ•°æ®é›†è¯»å–åˆ°ç”±å¤šæ¡æ–‡æœ¬è¡Œç»„æˆçš„åˆ—è¡¨ä¸­
#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  #@save
    """Load the time machine dataset into a list of text lines."""
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

lines = read_time_machine()
print(f'# text lines: {len(lines)}')
print(lines[0])
print(lines[10])

#æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªæ ‡è®°åˆ—è¡¨
def tokenize(lines, token='word'):  #@save
    """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒã€‚"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])

#æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯æ±‡è¡¨ï¼Œç”¨æ¥å°†å­—ç¬¦ä¸²ç±»å‹çš„æ ‡è®°æ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­
class Vocab:  #@save
    """æ–‡æœ¬è¯æ±‡è¡¨"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = [] 
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                  reverse=True)
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
        uniq_tokens += [token for token, freq in self.token_freqs
                        if freq >= min_freq and token not in uniq_tokens]
        self.idx_to_token, self.token_to_idx = [], dict()
        for token in uniq_tokens:
            self.idx_to_token.append(token)
            self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

def count_corpus(tokens):  #@save
    """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡ã€‚"""
    # è¿™é‡Œçš„ `tokens` æ˜¯ 1D åˆ—è¡¨æˆ– 2D åˆ—è¡¨
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä½¿ç”¨è¯å…ƒå¡«å……çš„ä¸€ä¸ªåˆ—è¡¨
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

#æ„å»ºè¯æ±‡è¡¨
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])

#å°†æ¯ä¸ªæ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨
for i in [0, 10]:
    print('words:', tokens[i])
    print('indices:', vocab[tokens[i]])
    
def load_corpus_time_machine(max_tokens=-1):  #@save
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨ã€‚"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

# 53 è¯­è¨€æ¨¡å‹

## è¯­è¨€æ¨¡å‹

ç»™å®šæ–‡æœ¬åºåˆ—ï¼Œè¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯ä¼°è®¡è”åˆæ¦‚ç‡

å®ƒçš„åº”ç”¨åŒ…æ‹¬

- â€‹	åšé¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTï¼ŒGPT-3ï¼‰
- â€‹	ç”Ÿæˆæœ¬æ–‡ï¼Œç»™å®šå‰é¢å‡ ä¸ªè¯ï¼Œä¸æ–­åœ°ç”Ÿæˆåç»­æ–‡æœ¬
- â€‹	åˆ¤æ–­å¤šä¸ªåºåˆ—ä¸­å“ªä¸ªæ›´å¸¸è§

### ä½¿ç”¨è®¡æ•°æ¥å»ºæ¨¡

### Nå…ƒè¯­æ³•

å¥½å¤„ï¼šå¯ä»¥å¤„ç†æ¯”è¾ƒé•¿çš„åºåˆ—

### æ€»ç»“

è¯­è¨€æ¨¡å‹ä¼°è®¡æ–‡æœ¬åºåˆ—çš„è”åˆæ¦‚ç‡

ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ—¶å¸¸é‡‡ç”¨nå…ƒè¯­æ³•

## ä»£ç å®ç°

è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†

```python
import rando
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
# å› ä¸ºæ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œå› æ­¤æˆ‘ä»¬æŠŠæ‰€æœ‰æ–‡æœ¬è¡Œæ‹¼æ¥åˆ°ä¸€èµ·
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]

#æœ€æµè¡Œçš„è¯è¢«ç§°ä¸ºåœç”¨è¯ç”»å‡ºçš„è¯é¢‘å›¾
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')

bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]

trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]

bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])

#éšæœºåœ°ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ä»¥ä¾›è¯»å–ã€‚åœ¨éšæœºé‡‡æ ·ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åœ¨åŸå§‹çš„é•¿åºåˆ—ä¸Šä»»æ„æ•è·çš„å­åºåˆ—
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """ä½¿ç”¨éšæœºæŠ½æ ·ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—ã€‚"""
    # ä»éšæœºåç§»é‡å¼€å§‹å¯¹åºåˆ—è¿›è¡Œåˆ†åŒºï¼ŒéšæœºèŒƒå›´åŒ…æ‹¬`num_steps - 1`
    corpus = corpus[random.randint(0, num_steps - 1):]
    # å‡å»1ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦è€ƒè™‘æ ‡ç­¾
    num_subseqs = (len(corpus) - 1) // num_steps
    # é•¿åº¦ä¸º`num_steps`çš„å­åºåˆ—çš„èµ·å§‹ç´¢å¼•
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # åœ¨éšæœºæŠ½æ ·çš„è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œ
    # æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„ã€éšæœºçš„ã€å°æ‰¹é‡ä¸­çš„å­åºåˆ—ä¸ä¸€å®šåœ¨åŸå§‹åºåˆ—ä¸Šç›¸é‚»
    random.shuffle(initial_indices)

    def data(pos):
        # è¿”å›ä»`pos`ä½ç½®å¼€å§‹çš„é•¿åº¦ä¸º`num_steps`çš„åºåˆ—
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # åœ¨è¿™é‡Œï¼Œ`initial_indices`åŒ…å«å­åºåˆ—çš„éšæœºèµ·å§‹ç´¢å¼•
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
        
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
    
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """ä½¿ç”¨é¡ºåºåˆ†åŒºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—ã€‚"""
    # ä»éšæœºåç§»é‡å¼€å§‹åˆ’åˆ†åºåˆ—
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
        
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
   
class SeqDataLoader:  #@save
    """åŠ è½½åºåˆ—æ•°æ®çš„è¿­ä»£å™¨ã€‚"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
      
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¿­ä»£å™¨å’Œè¯æ±‡è¡¨ã€‚"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```

# 54 å¾ªç¯ç¥ç»ç½‘ç»œ RNN

## RNN

### æ½œå˜é‡è‡ªå›å½’æ¨¡å‹

ä½¿ç”¨æ½œå˜é‡htæ€»ç»“è¿‡å»ä¿¡æ¯

### å¾ªç¯ç¥ç»ç½‘ç»œ

æ›´æ–°éšè—çŠ¶æ€

è¾“å‡º

### ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹

### å›°æƒ‘åº¦ï¼ˆperplexityï¼‰

è¡¡é‡ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å¥½åå¯ä»¥ç”¨å¹³å‡äº¤å‰ç†µ

å†å²åŸå› NLPä½¿ç”¨å›°æƒ‘åº¦expï¼ˆpiï¼‰æ¥è¡¡é‡ï¼Œæ˜¯å¹³å‡æ¯æ¬¡å¯èƒ½é€‰é¡¹

â€‹	1è¡¨ç¤ºå®Œç¾ï¼Œæ— ç©·å¤§æ˜¯æœ€å·®æƒ…å†µ

### æ¢¯åº¦å‰ªè£

è¿­ä»£ä¸­è®¡ç®—è¿™Tä¸ªæ—¶é—´æ­¥ä¸Šçš„æ¢¯åº¦ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­äº§ç”Ÿé•¿åº¦ä¸ºOï¼ˆTï¼‰çš„çŸ©é˜µä¹˜æ³•é“¾ï¼Œå¯¼è‡´æ•°å€¼ä¸ç¨³å®š

æ¢¯åº¦å‰ªè£èƒ½æœ‰æ•ˆé¢„é˜²æ¢¯åº¦çˆ†ç‚¸

â€‹	å¦‚æœæ¢¯åº¦é•¿åº¦è¶…è¿‡citaï¼Œé‚£ä¹ˆæ‹–å½±å›é•¿åº¦cita

### æ›´å¤šçš„åº”ç”¨ RNNs

æ–‡æœ¬ç”Ÿæˆ  æ–‡æœ¬åˆ†ç±»  å›ç­”ã€æœºå™¨ç¿»è¯‘  Tagç”Ÿæˆ

### æ€»ç»“

å¾ªç¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºå–å†³äºå½“ä¸‹è¾“å…¥å’Œå‰ä¸€æ—¶é—´çš„éšå˜é‡

åº”ç”¨åˆ°è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œæ ¹æ®å½“å‰è¯é¢„æµ‹ä¸‹ä¸€æ¬¡æ—¶åˆ»è¯

é€šå¸¸ä½¿ç”¨å›°æƒ‘åº¦æ¥è¡¡é‡è¯­è¨€æ¨¡å‹çš„å¥½å

# 55 å¾ªç¯ç¥ç»ç½‘ç»œRNNçš„å®ç°

## ä»é›¶å¼€å§‹å®ç°

```python
%matplotlib inline
import math
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#ç‹¬çƒ­ç¼–ç 
F.one_hot(torch.tensor([0, 2]), len(vocab))

#å°æ‰¹é‡æ•°æ® å½¢çŠ¶ï¼ˆæ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°ï¼‰
X = torch.arange(10).reshape((2, 5))
F.one_hot(X.T, 28).shape

#åˆå§‹åŒ–å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¨¡å‹å‚æ•°
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # éšè—å±‚å‚æ•°
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # è¾“å‡ºå±‚å‚æ•°
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # é™„åŠ æ¢¯åº¦
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
  
def init_rnn_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )

#rnnå‡½æ•°å®šä¹‰äº†å¦‚ä½•åœ¨ä¸€ä¸ªæ—¶é—´æ­¥å†…è®¡ç®—éšè—çŠ¶æ€å’Œè¾“å‡º
def rnn(inputs, state, params):
    # `inputs`çš„å½¢çŠ¶ï¼š(`æ—¶é—´æ­¥æ•°é‡`ï¼Œ`æ‰¹é‡å¤§å°`ï¼Œ`è¯è¡¨å¤§å°`)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # `X`çš„å½¢çŠ¶ï¼š(`æ‰¹é‡å¤§å°`ï¼Œ`è¯è¡¨å¤§å°`)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
  
class RNNModelScratch: #@save
    """ä»é›¶å¼€å§‹å®ç°çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
      
num_hiddens = 512
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)
state = net.begin_state(X.shape[0], d2l.try_gpu())
Y, new_state = net(X.to(d2l.try_gpu()), state)
Y.shape, len(new_state), new_state[0].shape

#é¦–å…ˆå®šä¹‰é¢„æµ‹å‡½æ•°æ¥ç”Ÿæˆprefixä¹‹åçš„æ–°å­—ç¬¦
def predict_ch8(prefix, num_preds, net, vocab, device):  #@save
    """åœ¨`prefix`åé¢ç”Ÿæˆæ–°å­—ç¬¦ã€‚"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]:  # é¢„çƒ­æœŸ
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # é¢„æµ‹`num_preds`æ­¥
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
  
def grad_clipping(net, theta):  #@save
    """è£å‰ªæ¢¯åº¦ã€‚"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
            
#@save
def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬8ç« ï¼‰ã€‚"""
    state, timer = None, d2l.Timer()
    metric = d2l.Accumulator(2)  # è®­ç»ƒæŸå¤±ä¹‹å’Œ, è¯å…ƒæ•°é‡
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æˆ–ä½¿ç”¨éšæœºæŠ½æ ·æ—¶åˆå§‹åŒ–`state`
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # `state`å¯¹äº`nn.GRU`æ˜¯ä¸ªå¼ é‡
                state.detach_()
            else:
                # `state`å¯¹äº`nn.LSTM`æˆ–å¯¹äºæˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°çš„æ¨¡å‹æ˜¯ä¸ªå¼ é‡
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # å› ä¸ºå·²ç»è°ƒç”¨äº†`mean`å‡½æ•°
            updater(batch_size=1)
        metric.add(l * y.numel(), y.numel())
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
  
#@save
def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬8ç« ï¼‰ã€‚"""
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])
    # åˆå§‹åŒ–
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
    # è®­ç»ƒå’Œé¢„æµ‹
    for epoch in range(num_epochs):
        ppl, speed = train_epoch_ch8(
            net, train_iter, loss, updater, device, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    print(f'å›°æƒ‘åº¦ {ppl:.1f}, {speed:.1f} è¯å…ƒ/ç§’ {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
    
num_epochs, lr = 500, 1
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())

train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),
          use_random_iter=True)
```

## ç®€æ´å®ç°

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#å®šä¹‰æ¨¡å‹
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)

#ä½¿ç”¨å¼ é‡æ¥åˆå§‹åŒ–éšè—çŠ¶æ€
state = torch.zeros((1, batch_size, num_hiddens))
state.shape

#é€šè¿‡ä¸€ä¸ªéšè—çŠ¶æ€å’Œä¸€ä¸ªè¾“å…¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨æ›´æ–°åçš„éšè—çŠ¶æ€è®¡ç®—è¾“å‡º
X = torch.rand(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, state_new.shape

#@save
class RNNModel(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # å¦‚æœRNNæ˜¯åŒå‘çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰ï¼Œ`num_directions`åº”è¯¥æ˜¯2ï¼Œå¦åˆ™åº”è¯¥æ˜¯1ã€‚
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # å…¨è¿æ¥å±‚é¦–å…ˆå°†`Y`çš„å½¢çŠ¶æ”¹ä¸º(`æ—¶é—´æ­¥æ•°`*`æ‰¹é‡å¤§å°`, `éšè—å•å…ƒæ•°`)ã€‚
        # å®ƒçš„è¾“å‡ºå½¢çŠ¶æ˜¯ (`æ—¶é—´æ­¥æ•°`*`æ‰¹é‡å¤§å°`, `è¯è¡¨å¤§å°`)ã€‚
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # `nn.GRU` ä»¥å¼ é‡ä½œä¸ºéšè—çŠ¶æ€
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens), 
                                device=device)
        else:
            # `nn.LSTM` ä»¥å¼ é‡ä½œä¸ºéšè—çŠ¶æ€
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))

          #åŸºäºä¸€ä¸ªå…·æœ‰éšæœºæƒé‡çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)

num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)


```

# 56 é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰

## é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰

### å…³æ³¨ä¸€ä¸ªåºåˆ—

ä¸æ˜¯æ¯ä¸ªè§‚å¯Ÿå€¼éƒ½æ˜¯åŒç­‰é‡è¦

æƒ³åªè®°ä½ç›¸å…³çš„è§‚å¯Ÿéœ€è¦ï¼š

â€‹		èƒ½å…³æ³¨çš„æœºåˆ¶ï¼ˆæ›´æ–°é—¨ï¼‰

â€‹		èƒ½é—å¿˜çš„æœºåˆ¶ï¼ˆé‡ç½®é—¨ï¼‰

### é—¨

![](pic/é—¨.png)

### å€™é€‰éšçŠ¶æ€

![](pic/å€™é€‰éšçŠ¶æ€.png)

### éšçŠ¶æ€

![](pic/éšçŠ¶æ€.png)

### æ€»ç»“

![](pic/GRU-1.png)

![](pic/GRU-2.png)

## ä»£ç å®ç°

### ä»é›¶å¼€å§‹å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#åˆå§‹åŒ–æ¨¡å‹å‚æ•°
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xz, W_hz, b_z = three()  # æ›´æ–°é—¨å‚æ•°
    W_xr, W_hr, b_r = three()  # é‡ç½®é—¨å‚æ•°
    W_xh, W_hh, b_h = three()  # å€™é€‰éšè—çŠ¶æ€å‚æ•°
    # è¾“å‡ºå±‚å‚æ•°
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # é™„åŠ æ¢¯åº¦
    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
  
#å®šä¹‰éšè—çŠ¶æ€çš„åˆå§‹åŒ–å‡½æ•°
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
  
#å®šä¹‰é—¨æ§å¾ªç¯å•å…ƒæ¨¡å‹
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)

#è®­ç»ƒ
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                            init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

### ç®€æ´å®ç°

```python
num_inputs = vocab_size
gru_layer = nn.GRU(num_inputs, num_hiddens)
model = d2l.RNNModel(gru_layer, len(vocab))
model = model.to(device)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

# 57 é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)

## é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰

å¿˜è®°é—¨ï¼šå°†å€¼æœ0å‡å°‘

è¾“å…¥é—¨ï¼šå†³å®šæ˜¯ä¸æ˜¯å¿½ç•¥æ‰è¾“å…¥æ•°æ®

è¾“å‡ºé—¨ï¼šå†³å®šæ˜¯ä¸æ˜¯ä½¿ç”¨éšçŠ¶æ€

### å€™é€‰è®°å¿†å•å…ƒ

![](pic/å€™é€‰è®°å¿†å•å…ƒ.png)

### è®°å¿†å•å…ƒ

![](pic/è®°å¿†å•å…ƒ.png)

### éšçŠ¶æ€

![](pic/éšçŠ¶æ€- LSTM.png)

### æ€»ç»“

![](pic/LSTM.png)

![](pic/LSTM-2.png)

## ä»£ç å®ç°

### ä»é›¶å¼€å§‹å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#åˆå§‹åŒ–æ¨¡å‹å‚æ•°
def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = three()  # è¾“å…¥é—¨å‚æ•°
    W_xf, W_hf, b_f = three()  # é—å¿˜é—¨å‚æ•°
    W_xo, W_ho, b_o = three()  # è¾“å‡ºé—¨å‚æ•°
    W_xc, W_hc, b_c = three()  # å€™é€‰è®°å¿†å•å…ƒå‚æ•°
    # è¾“å‡ºå±‚å‚æ•°
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # é™„åŠ æ¢¯åº¦
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
  
#å®šä¹‰æ¨¡å‹
def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
 
#å®é™…æ¨¡å‹
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)

#è®­ç»ƒå’Œé¢„æµ‹
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
                            init_lstm_state, lstm)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)


```

### ç®€æ´å®ç°

```python
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

# 58 æ·±å±‚å¾ªç¯ç¥ç»ç½‘ç»œ

## æ·±å±‚å¾ªç¯ç¥ç»ç½‘ç»œ

![](pic/æ·±å±‚å¾ªç¯ç¥ç»ç½‘ç»œ.png)

![](pic/æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ2.png)

### æ€»ç»“

æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œä½¿ç”¨å¤šä¸ªéšè—å±‚æ¥è·å¾—æ›´å¤šçš„éçº¿æ€§æ€§

## ä»£ç å®ç°

ç®€æ´å®ç°

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#é€šè¿‡num_layersçš„å€¼æ¥è®¾å®šéšè—å±‚æ•° 
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)

num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

# 59 åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ bi-rnn

## åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ

### æœªæ¥å¾ˆé‡è¦

å–å†³äºè¿‡å»å’Œæœªæ¥çš„ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥å¡«å¾ˆä¸ä¸€æ ·çš„è¯

ç›®å‰ä¸ºæ­¢RNNåªçœ‹è¿‡å»

åœ¨å¡«ç©ºçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹æœªæ¥

### åŒå‘RNN

![](pic/åŒå‘RNN.png)

ä¸€ä¸ªå‰å‘RNNéšå±‚

ä¸€ä¸ªåå‘RNNéšå±‚

åˆå¹¶ä¸¤ä¸ªéšçŠ¶æ€å¾—åˆ°è¾“å‡º

### æ€»ç»“

åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œé€šè¿‡åå‘æ›´æ–°çš„éšè—å±‚æ¥åˆ©ç”¨æ–¹å‘æ—¶é—´ä¿¡æ¯

é€šå¸¸ç”¨æ¥å¯¹åºåˆ—æŠ½å–ç‰¹å¾ã€å¡«ç©ºï¼Œè€Œä¸æ˜¯é¢„æµ‹æœªæ¥

## ä»£ç å®ç°

nn.xx( , ,bidirectional=True)

### åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œçš„é”™è¯¯ä½¿ç”¨

```python
import torch
from torch import nn
from d2l import torch as d2l

# åŠ è½½æ•°æ®
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
# é€šè¿‡è®¾ç½®'bidirective=True'æ¥å®šä¹‰åŒå‘LSTMæ¨¡å‹
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
# è®­ç»ƒæ¨¡å‹
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

# 60 æœºå™¨ç¿»è¯‘ä¸æ•°æ®é›†

```python
import os
import torch
from d2l import torch as d2l

#ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†
#@save
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
                           '94646ad1522d915e7b0f9296181140edcf86a4f5')

#@save
def read_data_nmt():
    """è½½å…¥â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†ã€‚"""
    data_dir = d2l.download_extract('fra-eng')
    with open(os.path.join(data_dir, 'fra.txt'), 'r', 
             encoding='utf-8') as f:
        return f.read()

raw_text = read_data_nmt()
print(raw_text[:75])

#å‡ ä¸ªé¢„å¤„ç†æ­¥éª¤
#@save
def preprocess_nmt(text):
    """é¢„å¤„ç†â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®é›†ã€‚"""
    def no_space(char, prev_char):
        return char in set(',.!?') and prev_char != ' '

    # ä½¿ç”¨ç©ºæ ¼æ›¿æ¢ä¸é—´æ–­ç©ºæ ¼
    # ä½¿ç”¨å°å†™å­—æ¯æ›¿æ¢å¤§å†™å­—æ¯
    text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
    # åœ¨å•è¯å’Œæ ‡ç‚¹ç¬¦å·ä¹‹é—´æ’å…¥ç©ºæ ¼
    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char
           for i, char in enumerate(text)]
    return ''.join(out)

text = preprocess_nmt(raw_text)
print(text[:80])

#è¯å…ƒåŒ–
#@save
def tokenize_nmt(text, num_examples=None):
    """è¯å…ƒåŒ–â€œè‹±è¯­ï¼æ³•è¯­â€æ•°æ®æ•°æ®é›†ã€‚"""
    source, target = [], []
    for i, line in enumerate(text.split('\n')):
        if num_examples and i > num_examples:
            break
        parts = line.split('\t')
        if len(parts) == 2:
            source.append(parts[0].split(' '))
            target.append(parts[1].split(' '))
    return source, target

source, target = tokenize_nmt(text)
source[:6], target[:6]

#ç»˜åˆ¶æ¯ä¸ªæ–‡æœ¬åºåˆ—æ‰€åŒ…å«çš„æ ‡è®°æ•°é‡çš„ç›´æ–¹å›¾
d2l.set_figsize()
_, _, patches = d2l.plt.hist(
    [[len(l) for l in source], [len(l) for l in target]],
    label=['source', 'target'])
for patch in patches[1].patches:
    patch.set_hatch('/')
d2l.plt.legend(loc='upper right');

#è¯æ±‡è¡¨
src_vocab = d2l.Vocab(source, min_freq=2,
                      reserved_tokens=['<pad>', '<bos>', '<eos>'])
len(src_vocab)

#@save
def truncate_pad(line, num_steps, padding_token):
    """æˆªæ–­æˆ–å¡«å……æ–‡æœ¬åºåˆ—ã€‚"""
    if len(line) > num_steps:
        return line[:num_steps]  # æˆªæ–­
    return line + [padding_token] * (num_steps - len(line))  # å¡«å……

truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])

#@save
def build_array_nmt(lines, vocab, num_steps):
    """å°†æœºå™¨ç¿»è¯‘çš„æ–‡æœ¬åºåˆ—è½¬æ¢æˆå°æ‰¹é‡ã€‚"""
    lines = [vocab[l] for l in lines]
    lines = [l + [vocab['<eos>']] for l in lines]
    array = torch.tensor([truncate_pad(
        l, num_steps, vocab['<pad>']) for l in lines])
    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
    return array, valid_len
  
#@save
def load_data_nmt(batch_size, num_steps, num_examples=600):
    """è¿”å›ç¿»è¯‘æ•°æ®é›†çš„è¿­ä»£å™¨å’Œè¯æ±‡è¡¨ã€‚"""
    text = preprocess_nmt(read_data_nmt())
    source, target = tokenize_nmt(text, num_examples)
    src_vocab = d2l.Vocab(source, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    tgt_vocab = d2l.Vocab(target, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
    data_iter = d2l.load_array(data_arrays, batch_size)
    return data_iter, src_vocab, tgt_vocab
  
train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)
for X, X_valid_len, Y, Y_valid_len in train_iter:
    print('X:', X.type(torch.int32))
    print('valid lengths for X:', X_valid_len)
    print('Y:', Y.type(torch.int32))
    print('valid lengths for Y:', Y_valid_len)
    break
```

# 61 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ encoder decoder

## ç¼–ç å™¨-è§£ç å™¨æ¶æ„

### é‡æ–°è€ƒå¯ŸCNN

![](pic/é‡æ–°è€ƒå¯ŸCNN.png)

ç¼–ç å™¨ï¼šå°†è¾“å…¥ç¼–ç¨‹æˆä¸­é—´è¡¨è¾¾å½¢å¼ï¼ˆç‰¹å¾ï¼‰

è§£ç å™¨ï¼šå°†ä¸­é—´è¡¨ç¤ºè§£ç æˆè¾“å‡º

### é‡æ–°è€ƒå¯ŸRNN

![](pic/é‡æ–°è€ƒå¯ŸRNN.png)

ç¼–ç å™¨ï¼šå°†æ–‡æœ¬è¡¨ç¤ºæˆå‘é‡

è§£ç å™¨ï¼šå‘é‡è¡¨ç¤ºæˆè¾“å‡º

### ç¼–ç å™¨-è§£ç å™¨æ¶æ„

![](pic/ç¼–ç å™¨è§£ç å™¨æ¶æ„.png)

### æ€»ç»“

ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ¨¡å‹ï¼Œç¼–ç å™¨è´Ÿè´£è¡¨ç¤ºè¾“å‡ºï¼Œè§£ç å™¨è´Ÿè´£è¾“å‡º

## ä»£ç å®ç°

```python
from torch import nn

#ç¼–ç å™¨
#@save
class Encoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„åŸºæœ¬ç¼–ç å™¨æ¥å£ã€‚"""
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError

#è§£ç å™¨
#@save
class Decoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„åŸºæœ¬è§£ç å™¨æ¥å£ã€‚"""
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError

#åˆå¹¶ç¼–ç å™¨å’Œè§£ç å™¨
#@save
class EncoderDecoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„åŸºç±»ã€‚"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)
```

# 62 åºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼ˆSeq2Seqï¼‰

## Seq2Seq

### æœºå™¨ç¿»è¯‘

ç»™å®šä¸€ä¸ªæºè¯­è¨€çš„å¥å­ï¼Œè‡ªåŠ¨ç¿»è¯‘æˆç›®æ ‡è¯­è¨€

è¿™ä¸¤ä¸ªå¥å­å¯ä»¥æœ‰ä¸åŒçš„é•¿åº¦

### Seq2Seq

![](pic/Seq2Seq.png)

ç¼–ç å™¨æ˜¯ä¸€ä¸ªRNNï¼Œè¯»å–è¾“å…¥å¥å­

â€‹	å¯ä»¥æ˜¯åŒå‘

è§£ç å™¨ä½¿ç”¨å¦ä¸€ä¸ªRNNæ¥è¾“å‡º

### ç¼–ç å™¨-è§£ç å™¨ç»†èŠ‚

![](pic/ç¼–ç å™¨-è§£ç å™¨ç»†èŠ‚.png)

ç¼–ç å™¨æ˜¯æ²¡æœ‰è¾“å‡ºçš„RNN

è§£ç å™¨æœ€åæ—¶é—´æ­¥çš„éšçŠ¶æ€ç”¨ä½œè§£ç å™¨çš„åˆå§‹éšçŠ¶æ€

### è®­ç»ƒ

è®­ç»ƒæ—¶è§£ç å™¨ä½¿ç”¨ç›®æ ‡å¥å­ä½œä¸ºè¾“å…¥

#### è¡¡é‡ç”Ÿæˆåºåˆ—çš„å¥½åçš„BLEU

![](pic/BLEUå®šä¹‰.png)

### æ€»ç»“

Seq2Seqä»ä¸€ä¸ªå¥å­ç”Ÿæˆå¦ä¸€ä¸ªå¥å­

ç¼–ç å™¨å’Œè§£ç å™¨éƒ½æ˜¯RNN

å°†ç¼–ç å™¨æœ€åæ—¶é—´éšçŠ¶æ€æ¥åˆå§‹è§£ç å™¨éšçŠ¶æ€æ¥å®Œæˆä¿¡æ¯ä¼ é€’

å¸¸ç”¨BLEUæ¥è¡¡é‡ç”Ÿæˆåºåˆ—çš„å¥½å

## ä»£ç å®ç°

```python
import collections
import math
import torch
from torch import nn
from d2l import torch as d2l

#å®ç°å¾ªç¯ç¥ç»ç½‘ç»œç¼–ç å™¨
#@save
class Seq2SeqEncoder(d2l.Encoder):
    """ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„å¾ªç¯ç¥ç»ç½‘ç»œç¼–ç å™¨ã€‚"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
                          dropout=dropout)

    def forward(self, X, *args):
        # è¾“å‡º'X'çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`, `embed_size`)
        X = self.embedding(X)
        # åœ¨å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œç¬¬ä¸€ä¸ªè½´å¯¹åº”äºæ—¶é—´æ­¥
        X = X.permute(1, 0, 2)
        # å¦‚æœæœªæåŠçŠ¶æ€ï¼Œåˆ™é»˜è®¤ä¸º0
        output, state = self.rnn(X)
        # `output`çš„å½¢çŠ¶: (`num_steps`, `batch_size`, `num_hiddens`)
        # `state[0]`çš„å½¢çŠ¶: (`num_layers`, `batch_size`, `num_hiddens`)
        return output, state

#ä¸Šè¿°ç¼–ç å™¨çš„å®ç°
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
encoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X)

#å®ç°å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨
class Seq2SeqDecoder(d2l.Decoder):
    """ç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨ã€‚"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]

    def forward(self, X, state):
        # è¾“å‡º'X'çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`, `embed_size`)
        X = self.embedding(X).permute(1, 0, 2)
        # å¹¿æ’­`context`ï¼Œä½¿å…¶å…·æœ‰ä¸`X`ç›¸åŒçš„`num_steps`
        context = state[-1].repeat(X.shape[0], 1, 1)
        X_and_context = torch.cat((X, context), 2)
        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # `output`çš„å½¢çŠ¶: (`batch_size`, `num_steps`, `vocab_size`)
        # `state[0]`çš„å½¢çŠ¶: (`num_layers`, `batch_size`, `num_hiddens`)
        return output, state

#å®ä¾‹åŒ–è§£ç å™¨
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
decoder.eval()
state = decoder.init_state(encoder(X))
output, state = decoder(X, state)
output.shape, state.shape

#é€šè¿‡é›¶å€¼åŒ–å±è”½ä¸æƒ³å…³çš„é¡¹
#@save
def sequence_mask(X, valid_len, value=0):
    """åœ¨åºåˆ—ä¸­å±è”½ä¸ç›¸å…³çš„é¡¹ã€‚"""
    maxlen = X.size(1)
    mask = torch.arange((maxlen), dtype=torch.float32,
                        device=X.device)[None, :] < valid_len[:, None]
    X[~mask] = value
    return X

X = torch.tensor([[1, 2, 3], [4, 5, 6]])
sequence_mask(X, torch.tensor([1, 2]))

X = torch.ones(2, 3, 4)
sequence_mask(X, torch.tensor([1, 2]), value=-1)

#é€šè¿‡æ‰©å±•softmaxäº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å±è”½ä¸ç›¸å…³çš„é¢„æµ‹
#@save
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    """å¸¦é®è”½çš„softmaxäº¤å‰ç†µæŸå¤±å‡½æ•°"""
    # `pred` çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`, `vocab_size`)
    # `label` çš„å½¢çŠ¶ï¼š(`batch_size`, `num_steps`)
    # `valid_len` çš„å½¢çŠ¶ï¼š(`batch_size`,)
    def forward(self, pred, label, valid_len):
        weights = torch.ones_like(label)
        weights = sequence_mask(weights, valid_len)
        self.reduction='none'
        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
            pred.permute(0, 2, 1), label)
        weighted_loss = (unweighted_loss * weights).mean(dim=1)
        return weighted_loss

#ä»£ç å¥å…¨æ€§æ£€æŸ¥
loss = MaskedSoftmaxCELoss()
loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),
     torch.tensor([4, 2, 0]))

#è®­ç»ƒ
#@save
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """è®­ç»ƒåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskedSoftmaxCELoss()
    net.train()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])
    for epoch in range(num_epochs):
        timer = d2l.Timer()
        metric = d2l.Accumulator(2)  # è®­ç»ƒæŸå¤±æ€»å’Œï¼Œè¯å…ƒæ•°é‡
        for batch in data_iter:
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]
            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],
                          device=device).reshape(-1, 1)
            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # æ•™å¸ˆå¼ºåˆ¶
            Y_hat, _ = net(X, dec_input, X_valid_len)
            l = loss(Y_hat, Y, Y_valid_len)
            l.sum().backward()	# æŸå¤±å‡½æ•°çš„æ ‡é‡è¿›è¡Œâ€œåä¼ â€
            d2l.grad_clipping(net, 1)
            num_tokens = Y_valid_len.sum()
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)
        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')

#åˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œâ€œç¼–ç å™¨-è§£ç å™¨â€æ¨¡å‹
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)

#é¢„æµ‹
#@save
def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights=False):
    """åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„é¢„æµ‹"""
    # åœ¨é¢„æµ‹æ—¶å°†`net`è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    net.eval()
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']]
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
    # æ·»åŠ æ‰¹é‡è½´
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
    enc_outputs = net.encoder(enc_X, enc_valid_len)
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
    # æ·»åŠ æ‰¹é‡è½´
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
    output_seq, attention_weight_seq = [], []
    for _ in range(num_steps):
        Y, dec_state = net.decoder(dec_X, dec_state)
        # æˆ‘ä»¬ä½¿ç”¨å…·æœ‰é¢„æµ‹æœ€é«˜å¯èƒ½æ€§çš„è¯å…ƒï¼Œä½œä¸ºè§£ç å™¨åœ¨ä¸‹ä¸€æ—¶é—´æ­¥çš„è¾“å…¥
        dec_X = Y.argmax(dim=2)
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()
        # ä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼ˆç¨åè®¨è®ºï¼‰
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        # ä¸€æ—¦åºåˆ—ç»“æŸè¯å…ƒè¢«é¢„æµ‹ï¼Œè¾“å‡ºåºåˆ—çš„ç”Ÿæˆå°±å®Œæˆäº†
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq

#BLEUçš„ä»£ç å®ç°
def bleu(pred_seq, label_seq, k):  #@save
    """è®¡ç®— BLEU"""
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    score = math.exp(min(0, 1 - len_label / len_pred))
    for n in range(1, k + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        for i in range(len_label - n + 1):
            label_subs[''.join(label_tokens[i: i + n])] += 1
        for i in range(len_pred - n + 1):
            if label_subs[''.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[''.join(pred_tokens[i: i + n])] -= 1
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score

#å°†å‡ ä¸ªè‹±æ–‡å¥å­ç¿»è¯‘æˆæ³•è¯­
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')
```

# 63 æŸæœç´¢

## è´ªå¿ƒæœç´¢

åœ¨Seq2Seqä¸­æˆ‘ä»¬ä½¿ç”¨äº†è´ªå¿ƒæœç´¢æ¥é¢„æµ‹åºåˆ—

â€‹		å°†å½“å‰æ—¶åˆ»é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„è¯è¾“å‡º

ä½†è´ªå¿ƒå¾ˆå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„

## ç©·ä¸¾æœç´¢

æœ€ä¼˜ç®—æ³•ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„åºåˆ—ï¼Œè®¡ç®—å®ƒçš„æ¦‚ç‡ï¼Œç„¶åé€‰å–æœ€å¥½çš„é‚£ä¸ª

â€‹		è®¡ç®—ä¸Šä¸å¯è¡Œ

## æŸæœç´¢

ä¿å­˜æœ€å¥½çš„kä¸ªå€™é€‰

åœ¨æ¯ä¸ªæ—¶åˆ»ï¼Œå¯¹æ¯ä¸ªå€™é€‰æ–°åŠ ä¸€é¡¹ï¼ˆnç§å¯èƒ½ï¼‰ï¼Œåœ¨knä¸ªé€‰é¡¹ä¸­é€‰å‡ºæœ€å¥½çš„kä¸ª

![](pic/æŸæœç´¢.png)

## æ€»ç»“

æŸæœç´¢åœ¨æ¯æ¬¡æœç´¢æ—¶ä¿å­˜kä¸ªæœ€å¥½çš„å€™é€‰

k=1æ—¶æ˜¯è´ªå¿ƒæœç´¢

k=næ—¶æ˜¯ç©·ä¸¾æœç´¢

# 64 æ³¨æ„åŠ›æœºåˆ¶

## æ³¨æ„åŠ›æœºåˆ¶

### å¿ƒç†å­¦

åŠ¨ç‰©éœ€è¦åœ¨å¤æ‚ç¯å¢ƒä¸‹å–å¾—å€¼å¾—æ³¨æ„çš„ç‚¹

å¿ƒç†å­¦æ¡†æ¶ï¼šäººç±»æ ¹æ®éšæ„çº¿ç´¢å’Œä¸éšæ„çº¿ç´¢é€‰æ‹©æ³¨æ„ç‚¹

### æ³¨æ„åŠ›æœºåˆ¶

å·ç§¯ã€å…¨è¿æ¥ã€æ± åŒ–å±‚éƒ½åªè€ƒè™‘ä¸éšæ„çº¿ç´¢

æ³¨æ„åŠ›æœºåˆ¶åˆ™æ˜¾ç¤ºçš„è€ƒè™‘éšæ„çº¿ç´¢

â€‹	éšæ„çº¿ç´¢è¢«ç§°ä¹‹ä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰

â€‹	æ¯ä¸ªè¾“å…¥æ˜¯ä¸€ä¸ªå€¼ï¼ˆvalueï¼‰å’Œä¸éšæ„çº¿ç´¢ï¼ˆkeyï¼‰çš„å¯¹

â€‹	é€šè¿‡æ³¨æ„åŠ›æ± åŒ–å±‚æœ‰åå‘æ€§çš„é€‰æ‹©æŸäº›è¾“å…¥

### éå‚æ³¨æ„åŠ›æ± åŒ–å±‚

ç»™å®šæ•°æ®

æœ€ç®€å•æ–¹æ¡ˆï¼šå¹³å‡æ± åŒ–

æ›´å¥½çš„æ–¹æ¡ˆï¼šNadaraya-Watsonæ ¸å›å½’

![](pic/Nadaraya-Watsonæ ¸å›å½’.png)

![](pic/Nadaraya-Watsonæ ¸å›å½’2.png)

### å‚æ•°åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶

### æ€»ç»“

å¿ƒç†å­¦è®¤ä¸ºäººé€šè¿‡éšæ„çº¿ç´¢å’Œä¸éšæ„çº¿ç´¢é€‰æ‹©æ³¨æ„ç‚¹

æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé€šè¿‡queryï¼ˆéšæ„çº¿ç´¢ï¼‰å’Œkeyï¼ˆä¸éšæ„çº¿ç´¢ï¼‰æ¥æœ‰åå‘æ€§çš„é€‰æ‹©è¾“å…¥

â€‹	æ—©åœ¨60å¹´ä»£å°±æœ‰éå‚æ•°çš„æ³¨æ„åŠ›æœºåˆ¶

â€‹	æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šä»‹ç»å¤šä¸ªä¸åŒçš„æƒé‡è®¾è®¡

## ä»£ç å®ç°

æ³¨æ„åŠ›æ±‡èšï¼šNadaraya-Watsonæ ¸å›å½’

```python
import torch
from torch import nn
from d2l import torch as d2l

#ç”Ÿæˆæ•°æ®é›†
n_train = 50  # è®­ç»ƒæ ·æœ¬æ•°
x_train, _ = torch.sort(torch.rand(n_train) * 5)   # è®­ç»ƒæ ·æœ¬çš„è¾“å…¥

def f(x):
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # è®­ç»ƒæ ·æœ¬çš„è¾“å‡º
x_test = torch.arange(0, 5, 0.1)  # æµ‹è¯•æ ·æœ¬
y_truth = f(x_test)  # æµ‹è¯•æ ·æœ¬çš„çœŸå®è¾“å‡º
n_test = len(x_test)  # æµ‹è¯•æ ·æœ¬æ•°

def plot_kernel_reg(y_hat):
    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'],
             xlim=[0, 5], ylim=[-1, 5])
    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);
    
#éå‚æ•°æ³¨æ„åŠ›æ±‡èšï¼ˆæ± åŒ–ï¼‰    
# `X_repeat` çš„å½¢çŠ¶: (`n_test`, `n_train`), 
# æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„æµ‹è¯•è¾“å…¥ï¼ˆä¾‹å¦‚ï¼šåŒæ ·çš„æŸ¥è¯¢ï¼‰
X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
# `x_train` åŒ…å«ç€é”®ã€‚`attention_weights` çš„å½¢çŠ¶ï¼š(`n_test`, `n_train`), 
# æ¯ä¸€è¡Œéƒ½åŒ…å«ç€è¦åœ¨ç»™å®šçš„æ¯ä¸ªæŸ¥è¯¢çš„å€¼ï¼ˆ`y_train`ï¼‰ä¹‹é—´åˆ†é…çš„æ³¨æ„åŠ›æƒé‡
attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)
# `y_hat` çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å€¼çš„åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­çš„æƒé‡æ˜¯æ³¨æ„åŠ›æƒé‡
y_hat = torch.matmul(attention_weights, y_train)
plot_kernel_reg(y_hat)

#æ³¨æ„åŠ›æƒé‡
d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),
                  xlabel='Sorted training inputs',
                  ylabel='Sorted testing inputs')

X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape

weights = torch.ones((2, 10)) * 0.1
values = torch.arange(20.0).reshape((2, 10))
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))

class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))

    def forward(self, queries, keys, values):
        # `queries` å’Œ `attention_weights` çš„å½¢çŠ¶ä¸º (æŸ¥è¯¢ä¸ªæ•°, â€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w)**2 / 2, dim=1)
        # `values` çš„å½¢çŠ¶ä¸º (æŸ¥è¯¢ä¸ªæ•°, â€œé”®ï¼å€¼â€å¯¹ä¸ªæ•°)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)
      
# `X_tile` çš„å½¢çŠ¶: (`n_train`, `n_train`), æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥
X_tile = x_train.repeat((n_train, 1))
# `Y_tile` çš„å½¢çŠ¶: (`n_train`, `n_train`), æ¯ä¸€è¡Œéƒ½åŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å‡º
Y_tile = y_train.repeat((n_train, 1))
# `keys` çš„å½¢çŠ¶: ('n_train', 'n_train' - 1)
keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
# `values` çš„å½¢çŠ¶: ('n_train', 'n_train' - 1)
values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))

net = NWKernelRegression()
loss = nn.MSELoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])

for epoch in range(5):
    trainer.zero_grad()
    # æ³¨æ„ï¼šL2 Loss = 1/2 * MSE Lossã€‚
    # PyTorch çš„ MSE Loss ä¸ MXNet çš„ L2Loss å·®ä¸€ä¸ª 2 çš„å› å­ï¼Œå› æ­¤è¢«é™¤2ã€‚
    l = loss(net(x_train, keys, values), y_train) / 2
    l.sum().backward()
    trainer.step()
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')
    animator.add(epoch + 1, float(l.sum()))
    
# `keys` çš„å½¢çŠ¶: (`n_test`, `n_train`), æ¯ä¸€è¡ŒåŒ…å«ç€ç›¸åŒçš„è®­ç»ƒè¾“å…¥ï¼ˆä¾‹å¦‚ï¼šç›¸åŒçš„é”®ï¼‰
keys = x_train.repeat((n_test, 1))
# `value` çš„å½¢çŠ¶: (`n_test`, `n_train`)
values = y_train.repeat((n_test, 1))
y_hat = net(x_test, keys, values).unsqueeze(1).detach()
plot_kernel_reg(y_hat)

d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),
                  xlabel='Sorted training inputs',
                  ylabel='Sorted testing inputs')
```

# 65 æ³¨æ„åŠ›åˆ†æ•°

## æ³¨æ„åŠ›åˆ†æ•°

### æ‹“å±•åˆ°é«˜ç»´åº¦

### Additive Attention

![](pic/AdditiveAttention.png)

ç­‰ä»·äºå°†queryå’Œkeyåˆå¹¶èµ·æ¥åæ”¾å…¥åˆ°ä¸€ä¸ªéšè—å¤§å°ä¸ºhè¾“å‡ºå¤§å°ä¸º1çš„å•éšè—å±‚MLP

### Scaled Dot- Product Attention

![](pic/Scaled Dot- ProductAttention.png)

### æ€»ç»“

æ³¨æ„åŠ›åˆ†æ•°æ˜¯queryå’Œkeyçš„ç›¸ä¼¼åº¦ï¼Œæ³¨æ„åŠ›æƒé‡æ˜¯åˆ†æ•°çš„softmaxç»“æœ

ä¸¤ç§å¸¸è§çš„åˆ†æ•°è®¡ç®—ï¼š

â€‹	å°†queryå’Œkeyåˆå¹¶èµ·æ¥è¿›å…¥ä¸€ä¸ªå•è¾“å‡ºå•éšè—å±‚çš„MLP

â€‹	ç›´æ¥å°†queryå’Œkeyä½œå†…ç§¯

## ä»£ç å®ç°

```python
import math
import torch
from torch import nn
from d2l import torch as d2l

#é®è”½softmaxæ“ä½œ
#@save
def masked_softmax(X, valid_lens):
    """é€šè¿‡åœ¨æœ€åä¸€ä¸ªè½´ä¸Šé®è”½å…ƒç´ æ¥æ‰§è¡Œ softmax æ“ä½œ"""
    # `X`: 3Då¼ é‡, `valid_lens`: 1Dæˆ–2D å¼ é‡
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # åœ¨æœ€åçš„è½´ä¸Šï¼Œè¢«é®è”½çš„å…ƒç´ ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„è´Ÿå€¼æ›¿æ¢ï¼Œä»è€Œå…¶ softmax (æŒ‡æ•°)è¾“å‡ºä¸º 0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
      
#æ¼”ç¤ºæ­¤å‡½æ•°å¦‚ä½•å·¥ä½œ      
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))
masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))

#åŠ æ€§æ³¨æ„åŠ›
#@save
class AdditiveAttention(nn.Module):
    """åŠ æ€§æ³¨æ„åŠ›"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # åœ¨ç»´åº¦æ‰©å±•åï¼Œ
        # `queries` çš„å½¢çŠ¶ï¼š(`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°, 1, `num_hidden`)
        # `key` çš„å½¢çŠ¶ï¼š(`batch_size`, 1, â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, `num_hiddens`)
        # ä½¿ç”¨å¹¿æ’­æ–¹å¼è¿›è¡Œæ±‚å’Œ
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # `self.w_v` ä»…æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤ä»å½¢çŠ¶ä¸­ç§»é™¤æœ€åé‚£ä¸ªç»´åº¦ã€‚
        # `scores` çš„å½¢çŠ¶ï¼š(`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°, â€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # `values` çš„å½¢çŠ¶ï¼š(`batch_size`, â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, å€¼çš„ç»´åº¦)
        return torch.bmm(self.dropout(self.attention_weights), values)
      
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# `values` çš„å°æ‰¹é‡æ•°æ®é›†ä¸­ï¼Œä¸¤ä¸ªå€¼çŸ©é˜µæ˜¯ç›¸åŒçš„
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(
    2, 1, 1)
valid_lens = torch.tensor([2, 6])

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
attention.eval()
attention(queries, keys, values, valid_lens)

d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')

#@save
class DotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # `queries` çš„å½¢çŠ¶ï¼š(`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°, `d`)
    # `keys` çš„å½¢çŠ¶ï¼š(`batch_size`, â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, `d`)
    # `values` çš„å½¢çŠ¶ï¼š(`batch_size`, â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, å€¼çš„ç»´åº¦)
    # `valid_lens` çš„å½¢çŠ¶: (`batch_size`,) æˆ–è€… (`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # è®¾ç½® `transpose_b=True` ä¸ºäº†äº¤æ¢ `keys` çš„æœ€åä¸¤ä¸ªç»´åº¦
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
      
queries = torch.normal(0, 1, (2, 1, 2))
attention = DotProductAttention(dropout=0.5)
attention.eval()
attention(queries, keys, values, valid_lens)

d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
```

# 66 ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„seq2seq

## ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„seq2seq

### åŠ¨æœº

æœºå™¨ç¿»è¯‘ä¸­ï¼Œæ¯ä¸ªç”Ÿæˆçš„è¯å¯èƒ½ç›¸å…³äºæºå¥å­ä¸­ä¸åŒçš„è¯

seq2seqæ¨¡å‹ä¸­ä¸èƒ½å¯¹æ­¤ç›´æ¥å»ºæ¨¡

### åŠ å…¥æ³¨æ„åŠ›

ç¼–ç å™¨å¯¹æ¯æ¬¡è¯çš„è¾“å‡ºä½œä¸ºkeyå’Œvalueï¼ˆå®ƒä»¬æ˜¯ä¸€æ ·çš„ï¼‰

è§£ç å™¨RNNå¯¹ä¸Šä¸€ä¸ªè¯çš„è¾“å‡ºæ˜¯query

æ³¨æ„åŠ›çš„è¾“å‡ºå’Œä¸‹ä¸€ä¸ªè¯çš„è¯åµŒå…¥åˆå¹¶è¿›å…¥RNN

### æ€»ç»“

Seq2seqä¸­é€šè¿‡éšçŠ¶æ€åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­ä¼ é€’ä¿¡æ¯

æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ ¹æ®è§£ç å™¨RNNçš„è¾“å‡ºæ¥åŒ¹é…åˆ°åˆé€‚çš„ç¼–ç å™¨RNNçš„è¾“å‡ºæ¥æ›´æœ‰æ•ˆçš„ä¼ é€’ä¿¡æ¯

## ä»£ç å®ç°

Bahdanauæ³¨æ„åŠ›

```python
import torch
from torch import nn
from d2l import torch as d2l

#å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£
#@save
class AttentionDecoder(d2l.Decoder):
    """å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨åŸºæœ¬æ¥å£"""
    def __init__(self, **kwargs):
        super(AttentionDecoder, self).__init__(**kwargs)

    @property
    def attention_weights(self):
        raise NotImplementedError

#å®ç°å¸¦æœ‰Bahdanauæ³¨æ„åŠ›çš„å¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
        self.attention = d2l.AdditiveAttention(
            num_hiddens, num_hiddens, num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        # `enc_outputs`çš„å½¢çŠ¶ä¸º (`batch_size`, `num_steps`, `num_hiddens`).
        # `hidden_state`çš„å½¢çŠ¶ä¸º (`num_layers`, `batch_size`,
        # `num_hiddens`)
        outputs, hidden_state = enc_outputs
        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # `enc_outputs`çš„å½¢çŠ¶ä¸º (`batch_size`, `num_steps`, `num_hiddens`).
        # `hidden_state`çš„å½¢çŠ¶ä¸º (`num_layers`, `batch_size`,
        # `num_hiddens`)
        enc_outputs, hidden_state, enc_valid_lens = state
        # è¾“å‡º `X`çš„å½¢çŠ¶ä¸º (`num_steps`, `batch_size`, `embed_size`)
        X = self.embedding(X).permute(1, 0, 2)
        outputs, self._attention_weights = [], []
        for x in X:
            # `query`çš„å½¢çŠ¶ä¸º (`batch_size`, 1, `num_hiddens`)
            query = torch.unsqueeze(hidden_state[-1], dim=1)
            # `context`çš„å½¢çŠ¶ä¸º (`batch_size`, 1, `num_hiddens`)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿ç»“
            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
            # å°† `x` å˜å½¢ä¸º (1, `batch_size`, `embed_size` + `num_hiddens`)
            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # å…¨è¿æ¥å±‚å˜æ¢åï¼Œ `outputs`çš„å½¢çŠ¶ä¸º 
        # (`num_steps`, `batch_size`, `vocab_size`)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                          enc_valid_lens]
    
    @property
    def attention_weights(self):
        return self._attention_weights
      
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                             num_layers=2)
encoder.eval()
decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                                  num_layers=2)
decoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)  # (`batch_size`, `num_steps`)
state = decoder.init_state(encoder(X), None)
output, state = decoder(X, state)
output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape

embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 250, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = d2l.Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)

engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
    
attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape((
    1, 1, -1, num_steps))

# åŠ ä¸Šä¸€ä¸ªåŒ…å«åºåˆ—ç»“æŸè¯å…ƒ
d2l.show_heatmaps(
    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),
    xlabel='Key posistions', ylabel='Query posistions')
```

# 67 è‡ªæ³¨æ„åŠ›

## è‡ªæ³¨æ„åŠ›

### è·ŸCNNï¼ŒRNNå¯¹æ¯”

![](pic/è‡ªæ³¨æ„åŠ›å¯¹æ¯”.png)

![](pic/è‡ªæ³¨æ„åŠ›å¯¹æ¯”è¡¨æ ¼.png)

### ä½ç½®ç¼–ç 

è·ŸCNN/RNNä¸åŒï¼Œè‡ªæ³¨æ„åŠ›å¹¶æ²¡æœ‰è®°å½•ä½ç½®ä¿¡æ¯

ä½ç½®ç¼–ç å°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°è¾“å…¥é‡Œ

![](pic/ä½ç½®ç¼–ç .png)

### ä½ç½®ç¼–ç çŸ©é˜µ

![](pic/ä½ç½®ç¼–ç çŸ©é˜µ.png)

### ç»å¯¹ä½ç½®ä¿¡æ¯

è®¡ç®—æœºä½¿ç”¨çš„äºŒè¿›åˆ¶ç¼–ç 

ä½ç½®ç¼–ç çŸ©é˜µ

### ç›¸å¯¹ä½ç½®ä¿¡æ¯

ä½ç½®äºi+deltaå¤„çš„ä½ç½®ç¼–ç å¯ä»¥çº¿æ€§æŠ•å½±ä½ç½®iå¤„çš„ä½ç½®ç¼–ç æ¥è¡¨ç¤º

![](pic/ç›¸å¯¹ä½ç½®ä¿¡æ¯.png)

### æ€»ç»“

è‡ªæ³¨æ„åŠ›æ± åŒ–å±‚å°†xiå½“ä½œkeyï¼Œvalueï¼Œqueryæ¥å¯¹åºåˆ—æŠ½å–ç‰¹å¾

å®Œå…¨å¹¶è¡Œã€æœ€é•¿åºåˆ—ä¸º1ã€ä½†å¯¹é•¿åºåˆ—è®¡ç®—å¤æ‚åº¦é«˜

ä½ç½®ç¼–ç åœ¨è¾“å…¥ä¸­åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—è‡ªæ³¨æ„åŠ›èƒ½å¤Ÿè®°å¿†ä½ç½®ä¿¡æ¯

## ä»£ç å®ç°

```python
import math
import torch
from torch import nn
from d2l import torch as d2l

num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                                   num_hiddens, num_heads, 0.5)
attention.eval()

batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
attention(X, X, X, valid_lens).shape

#ä½ç½®ç¼–ç 
#@save
class PositionalEncoding(nn.Module):
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ `P`
        self.P = torch.zeros((1, max_len, num_hiddens))
        X = torch.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
      
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
pos_encoding.eval()
X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))
P = pos_encoding.P[:, :X.shape[1], :]
d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in torch.arange(6, 10)])

for i in range(8):
    print(f'{i} in binary is {i:>03b}')
    
P = P[0, :, :].unsqueeze(0).unsqueeze(0)
d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',
                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')


```

# 68 Transformer

## Transformer

### Transformeræ¶æ„

![](pic/Transformeræ¶æ„.png)

### å¤šå¤´æ³¨æ„åŠ›

å¯¹åŒä¸€keyï¼Œvalueï¼Œqueryï¼Œå¸Œæœ›æŠ½å–ä¸åŒçš„ä¿¡æ¯

â€‹		ä¾‹å¦‚çŸ­è·ç¦»å…³ç³»å’Œé•¿è·ç¦»å…³ç³»

å¤šå¤´æ³¨æ„åŠ›ä½¿ç”¨hä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›æ± åŒ–

â€‹		åˆå¹¶å„ä¸ªå¤´ï¼ˆheadï¼‰è¾“å‡ºå¾—åˆ°æœ€ç»ˆè¾“å‡º

![](pic/å¤šå¤´æ³¨æ„åŠ›.png)

### æœ‰æ©ç çš„å¤šå¤´æ³¨æ„åŠ›

è§£ç å™¨å¯¹åºåˆ—ä¸­ä¸€ä¸ªå…ƒç´ è¾“å‡ºæ—¶ï¼Œä¸åº”è¯¥è€ƒè™‘è¯¥å…ƒç´ ä¹‹åçš„å…ƒç´ 

å¯ä»¥é€šè¿‡æ©ç æ¥å®ç°

### åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ

å°†è¾“å‡ºå½¢çŠ¶ç”±ï¼ˆb,n,dï¼‰å˜æ¢ä¸ºï¼ˆbn,dï¼‰

ä½œç”¨ä¸¤ä¸ªå…¨è¿æ¥å±‚

è¾“å‡ºå½¢çŠ¶ç”±ï¼ˆbn,dï¼‰å˜åŒ–å›ï¼ˆb,n,dï¼‰

ç­‰ä»·äºä¸¤å±‚æ ¸çª—å£ä¸º1çš„ä¸€ç»´å·ç§¯å±‚

### å±‚å½’ä¸€åŒ–

æ‰¹é‡å½’ä¸€åŒ–å¯¹æ¯ä¸ªç‰¹å¾/é€šé“é‡Œå…ƒç´ è¿›è¡Œå½’ä¸€åŒ–

â€‹	ä¸é€‚åˆåºåˆ—é•¿åº¦ä¼šå˜çš„NLPåº”ç”¨

å±‚å½’ä¸€åŒ–å¯¹æ¯ä¸ªæ ·æœ¬é‡Œçš„å…ƒç´ è¿›è¡Œå½’ä¸€åŒ–

![](pic/å±‚å½’ä¸€åŒ–.png)

### ä¿¡æ¯ä¼ é€’

æ„å‘³ç€ç¼–ç å™¨å’Œè§£ç å™¨ä¸­å—çš„ä¸ªæ•°å’Œè¾“å‡ºç»´åº¦éƒ½æ˜¯ä¸€æ ·çš„

### é¢„æµ‹

é¢„æµ‹ç¬¬t+1ä¸ªè¾“å‡ºæ—¶ï¼Œè§£ç å™¨ä¸­è¾“å…¥å‰tä¸ªé¢„æµ‹å€¼

â€‹	åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼Œå‰tä¸ªé¢„æµ‹å€¼ä½œä¸ºkeyå’Œvalueï¼Œç¬¬tä¸ªé¢„æµ‹å€¼è¿˜ä½œä¸ºquery

### æ€»ç»“

Transformeræ˜¯ä¸€ä¸ªçº¯ä½¿ç”¨æ³¨æ„åŠ›çš„ç¼–ç -è§£ç å™¨

ç¼–ç å™¨å’Œè§£ç å™¨éƒ½æœ‰nä¸ªTransformerå—

æ¯ä¸ªå—é‡Œä½¿ç”¨å¤šå¤´ï¼ˆè‡ªï¼‰æ³¨æ„åŠ›ï¼ŒåŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œï¼Œå’Œå±‚å½’ä¸€åŒ–

## å¤šå¤´æ³¨æ„åŠ›ä»£ç 

```python
import math
import torch
from torch import nn
from d2l import torch as d2l

#é€‰æ‹©ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ä½œä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´
#@save
class MultiHeadAttention(nn.Module):
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        # `queries`, `keys`, or `values` çš„å½¢çŠ¶:
        # (`batch_size`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, `num_hiddens`)
        # `valid_lens`ã€€çš„å½¢çŠ¶:
        # (`batch_size`,) or (`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°)
        # ç»è¿‡å˜æ¢åï¼Œè¾“å‡ºçš„ `queries`, `keys`, or `values`ã€€çš„å½¢çŠ¶:
        # (`batch_size` * `num_heads`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°,
        # `num_hiddens` / `num_heads`)
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            # åœ¨è½´ 0ï¼Œå°†ç¬¬ä¸€é¡¹ï¼ˆæ ‡é‡æˆ–è€…çŸ¢é‡ï¼‰å¤åˆ¶ `num_heads` æ¬¡ï¼Œ
            # ç„¶åå¦‚æ­¤å¤åˆ¶ç¬¬äºŒé¡¹ï¼Œç„¶åè¯¸å¦‚æ­¤ç±»ã€‚
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # `output` çš„å½¢çŠ¶: (`batch_size` * `num_heads`, æŸ¥è¯¢çš„ä¸ªæ•°,
        # `num_hiddens` / `num_heads`)
        output = self.attention(queries, keys, values, valid_lens)

        # `output_concat` çš„å½¢çŠ¶: (`batch_size`, æŸ¥è¯¢çš„ä¸ªæ•°, `num_hiddens`)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
      
#ä½¿å¤šä¸ªå¤´å¹¶è¡Œè®¡ç®—      
#@save
def transpose_qkv(X, num_heads):
    # è¾“å…¥ `X` çš„å½¢çŠ¶: (`batch_size`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, `num_hiddens`).
    # è¾“å‡º `X` çš„å½¢çŠ¶: (`batch_size`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°, `num_heads`,
    # `num_hiddens` / `num_heads`)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # è¾“å‡º `X` çš„å½¢çŠ¶: (`batch_size`, `num_heads`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°,
    # `num_hiddens` / `num_heads`)
    X = X.permute(0, 2, 1, 3)

    # `output` çš„å½¢çŠ¶: (`batch_size` * `num_heads`, æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°,
    # `num_hiddens` / `num_heads`)
    return X.reshape(-1, X.shape[2], X.shape[3])


#@save
def transpose_output(X, num_heads):
    """é€†è½¬ `transpose_qkv` å‡½æ•°çš„æ“ä½œ"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)

#æµ‹è¯•
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                               num_hiddens, num_heads, 0.5)
attention.eval()

batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
Y = torch.ones((batch_size, num_kvpairs, num_hiddens))
attention(X, Y, Y, valid_lens).shape
```

## Transformerä»£ç 

```python
import math
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l

#åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ
#@save
class PositionWiseFFN(nn.Module):
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))

#æ”¹å˜å¼ é‡çš„æœ€é‡Œå±‚ç»´åº¦çš„å°ºå¯¸
ffn = PositionWiseFFN(4, 4, 8)
ffn.eval()
ffn(torch.ones((2, 3, 4)))[0]

ln = nn.LayerNorm(2)
bn = nn.BatchNorm1d(2)
X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)
# åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è®¡ç®— `X` çš„å‡å€¼å’Œæ–¹å·®
print('layer norm:', ln(X), '\nbatch norm:', bn(X))

#ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
#@save
class AddNorm(nn.Module):
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
      
add_norm = AddNorm([3, 4], 0.5) # Normalized_shape is input.size()[1:]
add_norm.eval()
add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape

#@save
class EncoderBlock(nn.Module):
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
      
X = torch.ones((2, 100, 24))
valid_lens = torch.tensor([3, 2])
encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)
encoder_blk.eval()
encoder_blk(X, valid_lens).shape

#@save
class TransformerEncoder(d2l.Encoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias=False, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                EncoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, use_bias))

    def forward(self, X, valid_lens, *args):
        # å› ä¸ºä½ç½®ç¼–ç å€¼åœ¨ -1 å’Œ 1 ä¹‹é—´ï¼Œ
        # å› æ­¤åµŒå…¥å€¼ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ï¼Œ
        # ç„¶åå†ä¸ä½ç½®ç¼–ç ç›¸åŠ ã€‚
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X

encoder = TransformerEncoder(
    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)
encoder.eval()
encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape

class DecoderBlock(nn.Module):
    """è§£ç å™¨ä¸­ç¬¬ i ä¸ªå—"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # è®­ç»ƒé˜¶æ®µï¼Œè¾“å‡ºåºåˆ—çš„æ‰€æœ‰è¯å…ƒéƒ½åœ¨åŒä¸€æ—¶é—´å¤„ç†ï¼Œ
        # å› æ­¤ `state[2][self.i]` åˆå§‹åŒ–ä¸º `None`ã€‚
        # é¢„æµ‹é˜¶æ®µï¼Œè¾“å‡ºåºåˆ—æ˜¯é€šè¿‡è¯å…ƒä¸€ä¸ªæ¥ç€ä¸€ä¸ªè§£ç çš„ï¼Œ
        # å› æ­¤ `state[2][self.i]` åŒ…å«ç€ç›´åˆ°å½“å‰æ—¶é—´æ­¥ç¬¬ `i` ä¸ªå—è§£ç çš„è¾“å‡ºè¡¨ç¤º
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # `dec_valid_lens` çš„å¼€å¤´: (`batch_size`, `num_steps`), 
            # å…¶ä¸­æ¯ä¸€è¡Œæ˜¯ [1, 2, ..., `num_steps`]
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
        else:
            dec_valid_lens = None

        # è‡ªæ³¨æ„åŠ›
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›ã€‚
        # `enc_outputs` çš„å¼€å¤´: (`batch_size`, `num_steps`, `num_hiddens`)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
      
decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)
decoder_blk.eval()
X = torch.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
decoder_blk(X, state)[0].shape

class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                DecoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, i))
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # è§£ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡
            self._attention_weights[0][
                i] = blk.attention1.attention.attention_weights
            # â€œç¼–ç å™¨ï¼è§£ç å™¨â€è‡ªæ³¨æ„åŠ›æƒé‡
            self._attention_weights[1][
                i] = blk.attention2.attention.attention_weights
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
      
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32, 32
norm_shape = [32]

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)

encoder = TransformerEncoder(
    len(src_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
decoder = TransformerDecoder(
    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)

engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
    
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
    
d2l.show_heatmaps(
    enc_attention_weights.cpu(), xlabel='Key positions',
    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))

dec_attention_weights_2d = [head[0].tolist()
                            for step in dec_attention_weight_seq
                            for attn in step for blk in attn for head in blk]
dec_attention_weights_filled = torch.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
dec_attention_weights = dec_attention_weights_filled.reshape((-1, 2, num_layers, num_heads, num_steps))
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.permute(1, 2, 3, 0, 4)
dec_self_attention_weights.shape, dec_inter_attention_weights.shape

# Plus one to include the beginning-of-sequence token
d2l.show_heatmaps(
    dec_self_attention_weights[:, :, :, :len(translation.split()) + 1],
    xlabel='Key positions', ylabel='Query positions',
    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))

d2l.show_heatmaps(
    dec_inter_attention_weights, xlabel='Key positions',
    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))
```

# 69 BERTé¢„è®­ç»ƒ

## BERT

### NLPé‡Œçš„è¿ç§»å­¦ä¹ 

ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹æ¥æŠ½å–è¯ã€å¥å­çš„ç‰¹å¾

â€‹	ä¾‹å¦‚word2vecæˆ–è¯­è¨€æ¨¡å‹

ä¸æ›´æ–°é¢„è®­ç»ƒå¥½çš„æ¨¡å‹

éœ€è¦æ„å»ºæ–°çš„ç½‘ç»œæ¥æŠ“å–æ–°ä»»åŠ¡éœ€è¦çš„ä¿¡æ¯

â€‹	word2vecå¿½ç•¥äº†æ—¶åºä¿¡æ¯ï¼Œè¯­è¨€æ¨¡å‹åªçœ‹äº†ä¸€ä¸ªæ–¹å‘

### BERTçš„åŠ¨æœº

åŸºäºå¾®è°ƒçš„NLPæ¨¡å‹

é¢„è®­ç»ƒçš„æ¨¡å‹æŠ½å–äº†è¶³å¤Ÿå¤šçš„ä¿¡æ¯

æ–°çš„ä»»åŠ¡åªéœ€è¦å¢åŠ ä¸€ä¸ªç®€å•çš„è¾“å‡ºå±‚

### BERTæ¶æ„

åªæœ‰ç¼–ç å™¨çš„Transformer

ä¸¤ä¸ªç‰ˆæœ¬

â€‹	Base

 	Large

åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒ>3Bè¯

### å¯¹è¾“å…¥çš„ä¿®æ”¹

æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå¥å­å¯¹

åŠ å…¥é¢å¤–çš„ç‰‡æ®µåµŒå…¥

ä½ç½®ç¼–ç å¯å­¦ä¹ 

### é¢„è®­ç»ƒä»»åŠ¡1:å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹

Transformerçš„ç¼–ç å™¨æ˜¯åŒå‘ï¼Œæ ‡å‡†è¯­è¨€æ¨¡å‹è¦æ±‚å•å‘

å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹æ¯æ¬¡éšæœºï¼ˆ15%æ¦‚ç‡ï¼‰å°†ä¸€äº›è¯å…ƒæ¢æˆ < mask >

å› ä¸ºå¾®è°ƒä»»åŠ¡ä¸­ä¸å‡ºç°< mask >

â€‹		80%æ¦‚ç‡ä¸‹ï¼Œå°†é€‰ä¸­çš„è¯å…ƒå˜æˆ< mask >

â€‹		10%æ¦‚ç‡ä¸‹æ¢æˆä¸€ä¸ªéšæœºè¯å…ƒ

â€‹		10%æ¦‚ç‡ä¸‹ä¿æŒåŸæœ‰çš„è¯å…ƒ

### é¢„è®­ç»ƒä»»åŠ¡2:ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹

é¢„æµ‹ä¸€ä¸ªå¥å­å¯¹ä¸­ä¸¤ä¸ªå¥å­æ˜¯ä¸æ˜¯ç›¸é‚»

è®­ç»ƒæ ·æœ¬ä¸­ï¼š
	50%æ¦‚ç‡é€‰æ‹©ç›¸é‚»å¥å­å¯¹

â€‹	50%æ¦‚ç‡é€‰æ‹©éšæœºå¥å­å¯¹

å°†< cls >å¯¹åº”çš„è¾“å‡ºæ”¾åˆ°ä¸€ä¸ªå…¨è¿æ¥å±‚æ¥é¢„æµ‹

### æ€»ç»“

BERTé’ˆå¯¹å¾®è°ƒè®¾è®¡

åŸºäºTransformerçš„ç¼–ç å™¨åšäº†å¦‚ä¸‹ä¿®æ”¹

â€‹		æ¨¡å‹æ›´å¤§ï¼Œè®­ç»ƒæ•°æ®æ›´å¤š

â€‹		è¾“å…¥å¥å­å¯¹ï¼Œç‰‡æ®µåµŒå…¥ï¼Œå¯å­¦ä¹ çš„ä½ç½®ç¼–ç 

â€‹		è®­ç»ƒæ—¶ä½¿ç”¨ä¸¤ä¸ªä»»åŠ¡ï¼š

â€‹				å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹

â€‹				ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹

## BERTä»£ç 

```python
import torch
from torch import nn
from d2l import torch as d2l

#@save
def get_tokens_and_segments(tokens_a, tokens_b=None):
    """è·å–è¾“å…¥åºåˆ—çš„è¯å…ƒåŠå…¶ç‰‡æ®µç´¢å¼•ã€‚"""
    tokens = ['<cls>'] + tokens_a + ['<sep>']
    # 0å’Œ1åˆ†åˆ«æ ‡è®°ç‰‡æ®µAå’ŒB
    segments = [0] * (len(tokens_a) + 2)
    if tokens_b is not None:
        tokens += tokens_b + ['<sep>']
        segments += [1] * (len(tokens_b) + 1)
    return tokens, segments
  
#@save
class BERTEncoder(nn.Module):
    """BERT encoder."""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(f"{i}", d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        # åœ¨BERTä¸­ï¼Œä½ç½®åµŒå…¥æ˜¯å¯å­¦ä¹ çš„ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ä½ç½®åµŒå…¥å‚æ•°
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
                                                      num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # åœ¨ä»¥ä¸‹ä»£ç æ®µä¸­ï¼Œ`X`çš„å½¢çŠ¶ä¿æŒä¸å˜ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼Œ`num_hiddens`ï¼‰
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
      
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
                      ffn_num_hiddens, num_heads, num_layers, dropout)

tokens = torch.randint(0, vocab_size, (2, 8))
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape

#@save
class MaskLM(nn.Module):
    """BERTçš„é®è”½è¯­è¨€æ¨¡å‹ä»»åŠ¡"""
    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
                                 nn.ReLU(),
                                 nn.LayerNorm(num_hiddens),
                                 nn.Linear(num_hiddens, vocab_size))

    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1]
        pred_positions = pred_positions.reshape(-1)
        batch_size = X.shape[0]
        batch_idx = torch.arange(0, batch_size)
        # å‡è®¾`batch_size=2ï¼Œ`num_pred_positions`=3
        # é‚£ä¹ˆ`batch_idx`æ˜¯`np.arrayï¼ˆ[0,0,0,1,1]ï¼‰`
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
        mlm_Y_hat = self.mlp(masked_X)
        return mlm_Y_hat
      
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape

mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])
loss = nn.CrossEntropyLoss(reduction='none')
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape

#@save
class NextSentencePred(nn.Module):
    """BERTçš„ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡"""
    def __init__(self, num_inputs, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, 2)

    def forward(self, X):
        # `X`çš„å½¢çŠ¶ï¼š (batch size, `num_hiddens`)
        return self.output(X)
      
# é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¸ä¼šåƒmxnetä¸­é‚£æ ·å±•å¹³å¼ é‡
# å¦‚æœflatten=Trueï¼Œåˆ™é™¤ç¬¬ä¸€ä¸ªè¾“å…¥æ•°æ®è½´å¤–ï¼Œæ‰€æœ‰è¾“å…¥æ•°æ®è½´éƒ½æŠ˜å åœ¨ä¸€èµ·
encoded_X = torch.flatten(encoded_X, start_dim=1)
# NSPçš„è¾“å…¥å½¢çŠ¶: (batch size, `num_hiddens`)
nsp = NextSentencePred(encoded_X.shape[-1])
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape

nsp_y = torch.tensor([0, 1])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape

#@save
class BERTModel(nn.Module):
    """BERTæ¨¡å‹"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 hid_in_features=768, mlm_in_features=768,
                 nsp_in_features=768):
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                    dropout, max_len=max_len, key_size=key_size,
                    query_size=query_size, value_size=value_size)
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
                                    nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self.nsp = NextSentencePred(nsp_in_features)

    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):
        encoded_X = self.encoder(tokens, segments, valid_lens)
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None
        # ç”¨äºä¸‹ä¸€å¥é¢„æµ‹çš„å¤šå±‚æ„ŸçŸ¥æœºåˆ†ç±»å™¨çš„éšè—å±‚ã€‚0æ˜¯â€œ<cls>â€æ ‡è®°çš„ç´¢å¼•ã€‚
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
        return encoded_X, mlm_Y_hat, nsp_Y_hat
```



## BERTé¢„è®­ç»ƒæ•°æ®ä»£ç 

```python
import os
import random
import torch
from d2l import torch as d2l

#@save
d2l.DATA_HUB['wikitext-2'] = (
    'https://s3.amazonaws.com/research.metamind.io/wikitext/'
    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')

#@save
def _read_wiki(data_dir):
    file_name = os.path.join(data_dir, 'wiki.train.tokens')
    with open(file_name, 'r') as f:
        lines = f.readlines()
    # å¤§å†™å­—æ¯è½¬æ¢ä¸ºå°å†™å­—æ¯
    paragraphs = [line.strip().lower().split(' . ')
                  for line in lines if len(line.split(' . ')) >= 2]
    random.shuffle(paragraphs)
    return paragraphs
  
#@save
def _get_next_sentence(sentence, next_sentence, paragraphs):
    if random.random() < 0.5:
        is_next = True
    else:
        # `paragraphs`æ˜¯ä¸‰é‡åˆ—è¡¨çš„åµŒå¥—
        next_sentence = random.choice(random.choice(paragraphs))
        is_next = False
    return sentence, next_sentence, is_next
  
#@save
def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
    nsp_data_from_paragraph = []
    for i in range(len(paragraph) - 1):
        tokens_a, tokens_b, is_next = _get_next_sentence(
            paragraph[i], paragraph[i + 1], paragraphs)
        # è€ƒè™‘1ä¸ª'<cls>'è¯å…ƒå’Œ2ä¸ª'<sep>'è¯å…ƒ
        if len(tokens_a) + len(tokens_b) + 3 > max_len:
            continue
        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
        nsp_data_from_paragraph.append((tokens, segments, is_next))
    return nsp_data_from_paragraph
  
#@save
def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
                        vocab):
    # ä¸ºé®è”½è¯­è¨€æ¨¡å‹çš„è¾“å…¥åˆ›å»ºæ–°çš„è¯å…ƒå‰¯æœ¬ï¼Œå…¶ä¸­è¾“å…¥å¯èƒ½åŒ…å«æ›¿æ¢çš„â€œ<mask>â€æˆ–éšæœºè¯å…ƒ
    mlm_input_tokens = [token for token in tokens]
    pred_positions_and_labels = []
    # æ‰“ä¹±åç”¨äºåœ¨é®è”½è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸­è·å–15%çš„éšæœºè¯å…ƒè¿›è¡Œé¢„æµ‹
    random.shuffle(candidate_pred_positions)
    for mlm_pred_position in candidate_pred_positions:
        if len(pred_positions_and_labels) >= num_mlm_preds:
            break
        masked_token = None
        # 80%çš„æ—¶é—´ï¼šå°†è¯æ›¿æ¢ä¸ºâ€œ<mask>â€è¯å…ƒ
        if random.random() < 0.8:
            masked_token = '<mask>'
        else:
            # 10%çš„æ—¶é—´ï¼šä¿æŒè¯ä¸å˜
            if random.random() < 0.5:
                masked_token = tokens[mlm_pred_position]
            # 10%çš„æ—¶é—´ï¼šç”¨éšæœºè¯æ›¿æ¢è¯¥è¯
            else:
                masked_token = random.randint(0, len(vocab) - 1)
        mlm_input_tokens[mlm_pred_position] = masked_token
        pred_positions_and_labels.append(
            (mlm_pred_position, tokens[mlm_pred_position]))
    return mlm_input_tokens, pred_positions_and_labels
  
#@save
def _get_mlm_data_from_tokens(tokens, vocab):
    candidate_pred_positions = []
    # `tokens`æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨
    for i, token in enumerate(tokens):
        # åœ¨é®è”½è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸­ä¸ä¼šé¢„æµ‹ç‰¹æ®Šè¯å…ƒ
        if token in ['<cls>', '<sep>']:
            continue
        candidate_pred_positions.append(i)
    # é®è”½è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸­é¢„æµ‹15%çš„éšæœºè¯å…ƒ
    num_mlm_preds = max(1, round(len(tokens) * 0.15))
    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
        tokens, candidate_pred_positions, num_mlm_preds, vocab)
    pred_positions_and_labels = sorted(pred_positions_and_labels,
                                       key=lambda x: x[0])
    pred_positions = [v[0] for v in pred_positions_and_labels]
    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
  
#@save
def _pad_bert_inputs(examples, max_len, vocab):
    max_num_mlm_preds = round(max_len * 0.15)
    all_token_ids, all_segments, valid_lens,  = [], [], []
    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
    nsp_labels = []
    for (token_ids, pred_positions, mlm_pred_label_ids, segments,
         is_next) in examples:
        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (
            max_len - len(token_ids)), dtype=torch.long))
        all_segments.append(torch.tensor(segments + [0] * (
            max_len - len(segments)), dtype=torch.long))
        # `valid_lens` ä¸åŒ…æ‹¬'<pad>'çš„è®¡æ•°
        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))
        all_pred_positions.append(torch.tensor(pred_positions + [0] * (
            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))
        # å¡«å……è¯å…ƒçš„é¢„æµ‹å°†é€šè¿‡ä¹˜ä»¥0æƒé‡åœ¨æŸå¤±ä¸­è¿‡æ»¤æ‰
        all_mlm_weights.append(
            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (
                max_num_mlm_preds - len(pred_positions)),
                dtype=torch.float32))
        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (
            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))
        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))
    return (all_token_ids, all_segments, valid_lens, all_pred_positions,
            all_mlm_weights, all_mlm_labels, nsp_labels)
  
#@save
class _WikiTextDataset(torch.utils.data.Dataset):
    def __init__(self, paragraphs, max_len):
        # è¾“å…¥`paragraphs[i]`æ˜¯ä»£è¡¨æ®µè½çš„å¥å­å­—ç¬¦ä¸²åˆ—è¡¨ï¼›è€Œè¾“å‡º`paragraphs[i]`æ˜¯ä»£è¡¨æ®µè½çš„å¥å­åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå¥å­éƒ½æ˜¯è¯å…ƒåˆ—è¡¨
        paragraphs = [d2l.tokenize(
            paragraph, token='word') for paragraph in paragraphs]
        sentences = [sentence for paragraph in paragraphs
                     for sentence in paragraph]
        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
            '<pad>', '<mask>', '<cls>', '<sep>'])
        # è·å–ä¸‹ä¸€å¥å­é¢„æµ‹ä»»åŠ¡çš„æ•°æ®
        examples = []
        for paragraph in paragraphs:
            examples.extend(_get_nsp_data_from_paragraph(
                paragraph, paragraphs, self.vocab, max_len))
        # è·å–é®è”½è¯­è¨€æ¨¡å‹ä»»åŠ¡çš„æ•°æ®
        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
                      + (segments, is_next))
                     for tokens, segments, is_next in examples]
        # å¡«å……è¾“å…¥
        (self.all_token_ids, self.all_segments, self.valid_lens,
         self.all_pred_positions, self.all_mlm_weights,
         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
            examples, max_len, self.vocab)

    def __getitem__(self, idx):
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx], self.all_pred_positions[idx],
                self.all_mlm_weights[idx], self.all_mlm_labels[idx],
                self.nsp_labels[idx])

    def __len__(self):
        return len(self.all_token_ids)
      
#@save
def load_data_wiki(batch_size, max_len):
    """åŠ è½½WikiText-2æ•°æ®é›†ã€‚"""
    num_workers = d2l.get_dataloader_workers()
    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
    paragraphs = _read_wiki(data_dir)
    train_set = _WikiTextDataset(paragraphs, max_len)
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                        shuffle=True, num_workers=num_workers)
    return train_iter, train_set.vocab
  
batch_size, max_len = 512, 64
train_iter, vocab = load_data_wiki(batch_size, max_len)

for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
     mlm_Y, nsp_y) in train_iter:
    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
          nsp_y.shape)
    break
    
len(vocab)
```

## BERTé¢„è®­ç»ƒä»£ç 

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, max_len = 512, 64
train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)

net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],
                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,
                    num_layers=2, dropout=0.2, key_size=128, query_size=128,
                    value_size=128, hid_in_features=128, mlm_in_features=128,
                    nsp_in_features=128)
devices = d2l.try_all_gpus()
loss = nn.CrossEntropyLoss()

#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
                         segments_X, valid_lens_x,
                         pred_positions_X, mlm_weights_X,
                         mlm_Y, nsp_y):
    # å‰å‘ä¼ æ’­
    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
                                  valid_lens_x.reshape(-1),
                                  pred_positions_X)
    # è®¡ç®—é®è”½è¯­è¨€æ¨¡å‹æŸå¤±
    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
    mlm_weights_X.reshape(-1, 1)
    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
    # è®¡ç®—ä¸‹ä¸€å¥å­é¢„æµ‹ä»»åŠ¡çš„æŸå¤±
    nsp_l = loss(nsp_Y_hat, nsp_y)
    l = mlm_l + nsp_l
    return mlm_l, nsp_l, l
  
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    trainer = torch.optim.Adam(net.parameters(), lr=1e-3)
    step, timer = 0, d2l.Timer()
    animator = d2l.Animator(xlabel='step', ylabel='loss',
                            xlim=[1, num_steps], legend=['mlm', 'nsp'])
    # é®è”½è¯­è¨€æ¨¡å‹æŸå¤±çš„å’Œï¼Œä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡æŸå¤±çš„å’Œï¼Œå¥å­å¯¹çš„æ•°é‡ï¼Œè®¡æ•°
    metric = d2l.Accumulator(4)
    num_steps_reached = False
    while step < num_steps and not num_steps_reached:
        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\
            mlm_weights_X, mlm_Y, nsp_y in train_iter:
            tokens_X = tokens_X.to(devices[0])
            segments_X = segments_X.to(devices[0])
            valid_lens_x = valid_lens_x.to(devices[0])
            pred_positions_X = pred_positions_X.to(devices[0])
            mlm_weights_X = mlm_weights_X.to(devices[0])
            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])
            trainer.zero_grad()
            timer.start()
            mlm_l, nsp_l, l = _get_batch_loss_bert(
                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
            l.backward()
            trainer.step()
            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)
            timer.stop()
            animator.add(step + 1,
                         (metric[0] / metric[3], metric[1] / metric[3]))
            step += 1
            if step == num_steps:
                num_steps_reached = True
                break

    print(f'MLM loss {metric[0] / metric[3]:.3f}, '
          f'NSP loss {metric[1] / metric[3]:.3f}')
    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
          f'{str(devices)}')
    
train_bert(train_iter, net, loss, len(vocab), devices, 50)

def get_bert_encoding(net, tokens_a, tokens_b=None):
    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)
    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)
    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)
    encoded_X, _, _ = net(token_ids, segments, valid_len)
    return encoded_X
  
tokens_a = ['a', 'crane', 'is', 'flying']
encoded_text = get_bert_encoding(net, tokens_a)
# è¯å…ƒï¼š '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'
encoded_text_cls = encoded_text[:, 0, :]
encoded_text_crane = encoded_text[:, 2, :]
encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]

tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
# è¯å…ƒï¼š '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',
# 'left', '<sep>'
encoded_pair_cls = encoded_pair[:, 0, :]
encoded_pair_crane = encoded_pair[:, 2, :]
encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]
```

