# 认知神经学中的注意力

聚焦式注意力/选择性注意力（Focus Attention/Selective Attention）：自上而下的有意识的注意力

基于显著性的注意力（Saliency-Based Attention）：自下而上的无意识的注意力

# 注意力机制/模型

计算分为两步：
	一、在所有输入信息上计算**注意力分布**；（score函数-==计算相似性==  alignment函数-==归一化==）
	二、根据注意力分布来计算输入信息的**加权平均**。 （context函数-==Attention计算==）

## 注意力分布

为了从输入向量$X=[x_1,\ldots ,x_N]$中选择出和需求的特定任务相关的信息，我们引入和任务相关的**查询向量**(Query Vector)
通过==注意力打分函数== $s(x,q)$ 计算每个==输入向量== $x$ 和==查询向量== $q$ 的相关性。
==注意力分布== $\alpha _n$ 为根据注意力打分函数计算的输入向量各个位置的注意力，总和为1。

### 注意力打分函数 score function

| 类型             | 公式                           |
| :--------------- | :----------------------------- |
| 加性模型         | $s(x,q)=v^T\tanh (Wx+Uq)$      |
| 双线性(乘法)模型 | $s(x,q)=x^TWq$                 |
| 点积模型         | $s(x,q)=x^Tq$                  |
| 缩放点积模型     | $s(x,q)=\frac{x^Tq}{\sqrt{D}}$ |

### 注意力分布 alignment function

<img src="pic/计算注意力分布.png" alt="计算注意力分布" style="zoom:50%;" />

## 加权平均 context function

注意力分布 $\alpha _n$ 可以解释为在给定任务相关的查询 $q$ 时，第 $n$ 个输入向量受关注的程度。
根据该分布，对输入信息进行汇总。

### 软性(Soft)注意力

<img src="pic/软性注意力.png" alt="软性注意力" style="zoom:50%;" />

### 硬性(Hard)注意力

实现方式（1）

​			选取最高概率的一个输入向量

<img src="pic/硬性注意力.png" alt="硬性注意力" style="zoom: 45%;" />

实现方式（2）
			通过在注意力分布式上随机采样的方式实现。

缺点：基于**最大采样**或**随机采样**的方式选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法用反向传播算法进行训练。

### Local注意力

这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。

## 注意力机制的变体

### 健值对注意力

用健值对(key-value pair)格式来表示输入信息 
$(K,V)=[(k_1,v_1),\ldots ,(k_N,v_N)]$ 表示 $N$ 组输入信息。
其中key用来计算注意力分布 $\alpha _n$ , value用来计算聚合信息。
给定任务相关的查询向量q时，注意力函数为

<img src="pic/健值对注意力.png" alt="健值对注意力" style="zoom:50%;" />

其中 $s(k_n,q)$ 为打分函数。

### 多头注意力

利用多个查询 $Q=[q_1,\ldots , q_M]$ 来并行地从输入信息中获取多组信息。

### 结构化注意力

输入信息具有层次结构(比如可分为句子、段落、篇章等不同粒度)，可以使用层次化的注意力。
	ß多层Attention，一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。

### 指针网络

只利用注意力机制的第一步，将注意力分布作为一个软性的指针来指出相关信息的位置。
seq2seq ，输出序列是输入序列的下标（索引）

# 自注意力模型

健值对 + 缩放点积的注意力打分函数 
$q$ $k$ $v$ 全部来自输入序列$X=[x_1,\ldots x_N]$:
	将输入$x_i$线性映射到三个不同的空间，得到查询-健-值向量 ($q_i$ $k_i$ $v_i$) ，
	$W_q$ $W_k$ $W_v$ 分别为线性映射的参数矩阵。





# 参考

邱锡鹏 nndl-book 第二部分 第8章 注意力机制与外部记忆

一文看懂 Attention（本质原理+3大优点+5大类型） - 赵强的文章 - 知乎 https://zhuanlan.zhihu.com/p/91839581

超详细图解Self-Attention - ConquerJ的文章 - 知乎 https://zhuanlan.zhihu.com/p/410776234
