# 预备知识

先验概率：根据以往经验和分析得到的概率。

后验概率：在得到“结果”的信息后重新修正的概率。后验概率的计算要以先验概率为基础。

贝叶斯公式：P(A|B)=P(B|A)*P(A)/P(B)



条件独立性：如果P(X,Y|Z)=P(X|Z)P(Y|Z)，或等价地P(X|Y,Z）=P(X|Z），则称事件X,Y对于给定事件Z是条件独立的

# 简介

==概率模型==(Probabilistic Model):
将学习任务归结于计算变量的概率分布。
**推断**(Inference)：利用已知变量推测未知变量的分布。

所关心的变量集合Y，可观测变量集合O，其他变量集合R
“生成式”(generative)模型：考虑联合分布P(Y,R,O),然后通过贝叶斯公式求P(Y|O)
	常见模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。
“判别式”(discriminative)模型：考虑条件分布P(Y,R|O)
	常见模型有 线性/Logistic回归模型、线性判别分析、支持向量机SVM、神经网络、CRF等。
推断：给定一组观测变量值O，由P(Y,R,O)或P(Y,R|O)得到条件概率分布P(Y|O)

<img src="pic/生成式与判别式.png" alt="生成式与判别式" style="zoom:50%;" />

> 在机器学习中任务是从属性X预测标记Y，判别模型求的是P(Y|X)，即[后验概率](https://www.zhihu.com/search?q=后验概率&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"256466823"})；而生成模型最后求的是P(X,Y)，即联合概率。从本质上来说：
>
> **[判别模型](https://www.zhihu.com/search?q=判别模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"256466823"})之所以称为“判别”模型**，是因为其根据X“判别”Y；
>
> 而**生成模型之所以称为“生成”模型**，是因为其预测的根据是联合概率P(X,Y)，而联合概率可以理解为“生成”(X,Y)样本的概率分布（或称为 依据）；具体来说，机器学习已知X，从Y的候选集合中选出一个来，可能的样本有(X,Y_1), (X,Y_2), (X,Y_3),……，(X,Y_n),实际数据是如何“生成”的依赖于P(X,Y)，那么最后的预测结果选哪一个Y呢？那就选“生成”概率最大的那个吧~
>
> 作者：politer
> 链接：https://www.zhihu.com/question/20446337/answer/256466823
>
> ---
>
> 二者目的都是在使后验概率最大化，判别式是直接对后验概率建模，但是生成模型通过贝叶斯定理这一“桥梁”使问题转化为求联合概率.

==概率图模型==（Probabilistic Graphical Model，PGM），
简称图模型（Graphical Model，GM），
是指一种用图结构来描述多元随机变量之间条件独立关系的**概率模型**.

图模型的基本问题：
  (1)表示问题
  (2)推断问题（应用） 
  (3)学习问题（训练）

### 有向图模型 概览

​	也叫**贝叶斯网**，贝叶斯网络/信念网络
​	有向无环图/有向非循环图（Directed Acyclic Graph，DAG)

**节点间关系**：==条件概率分布== <=> 联合概率分布==
	(CPD/conditional probability distribution)

### 无向图模型 概览

​	也叫**马尔可夫网**，马尔可夫随机场/马尔可夫网络
​	无向图（Undirected Graph）

**节点间关系**：==势函数 + 图结构上的团 -> 概率分布==
==势函数==(potential function)/因子(factor)：定义在变量子集上的非负实函数。
	正如贝叶斯网络有 CPD 一样，马尔可夫网络也有用来整合节点之间的关系的表格。但是，这些表格和 CPD 之间有两个关键差异。
	首先，这些值不需要总和为 1，也就是说这个表格并没有定义一个概率分布。它只是告诉我们值更高的配置有更高的可能性。
	其次，其中没有条件关系。它与所涉及到的所有变量的联合分布成正比，这与 CPD 中的条件分布不同。
	**一般而言，你要为图中的每个极大团（maximal clique）定义一个势函数**。无向图中的一个全连通子图，称为==团（Clique）==。在所有团中，如果一个团不能被其他的团包含，这个团就是一个==极大团==。

### 为什么我们需要有向图，也需要无向图？

​	变量间的==依赖关系==：有些问题使用有向图表示会更加自然，比如上面提到的学生网络，有向图可以轻松描述变量之间的**因果关系**——学生的智力水平会影响 SAT 分数，但 SAT 分数不会影响智力水平（尽管它也许能反映学生的智力水平）。

​	变量间的==相互关系==：而对于其它一些问题，比如图像，你可能需要将每个像素都表示成一个节点。我们知道相邻的像素互有影响，但像素之间并不存在因果关系；它们之间的**相互作用是对称的**。所以我们在这样的案例中使用无向图模型。

# 有向图模型

![TheStudentNetwork](pic/TheStudentNetwork.jpg)

**条件独立性**：在贝叶斯网络中，如果两个节点是直接连接的，它们肯定是非条件独立的，是直接因果关系．父节点是“因”，子节点是“果”。
如果两个节点不是直接连接的，但可以由一条经过其他节点的路径来连接，那么这两个节点之间的条件独立性就比较复杂．以三个节点的贝叶斯网络为例，

![贝叶斯网络条件独立性](pic/贝叶斯网络条件独立性.png)

**局部马尔可夫性质**：对一个更一般的贝叶斯网络，其局部马尔可夫性质为：每个随机变量在给定父节点的情况下，条件独立于它的非后代节点．

## Sigmoid信念网络

Sigmoid 信念网络（Sigmoid Belief Network，SBN）中的变量取值为{0, 1}．
生成式模型

<img src="pic/Sigmoid信念网络.png" alt="Sigmoid信念网络" style="zoom: 50%;" />

> ​	值得一提的是，Sigmoid 信念网络与Logistic 回归模型都采用Logistic 函数来计算条件概率．如果假设Sigmoid 信念网络中只有一个叶子节点，其所有的父节点之间没有连接，且取值为实数，那么Sigmoid 信念网络的网络结构和Logistic回归模型类似，如图所示．
> ​	但是，这两个模型的区别在于
> Logistic 回归模型只建模条件概率𝑝(𝑦|𝒙)，是一种判别模型；
> 而Sigmoid 信念网络建模联合概率𝑝(𝒙,𝑦)，是一种生成模型．

## 朴素贝叶斯分类器

朴素贝叶斯（Naive Bayes，NB）分类器是一类简单的概率分类器，在强（朴素）独立性假设的条件下运用贝叶斯公式来计算每个类别的条件概率．
生成式模型

<img src="pic/朴素贝叶斯分类器.png" alt="朴素贝叶斯分类器" style="zoom:50%;" />

学习过程：
	估计类先验概率$P(y)$
	为每个属性估计条件概率$P(x_i|y)$

推理过程：
	$P(x_1,\ldots,x_n,y)=P(y)\cdot P(x_1,\ldots,x_n|y)=P(y)\cdot P(x_1|y)\cdots P(x_n|y)$


## 隐马尔可夫模型

隐马尔可夫模型（Hidden Markov Model，HMM）是用来表示一种含有隐变量的马尔可夫过程．
生成式模型

<img src="pic/隐马尔可夫模型.png" alt="隐马尔可夫模型" style="zoom:50%;" />

状态变量$y_1,\ldots,y_n \in \{s_1,\ldots , s_N\}$   观测值$x_1,\ldots ,x_n\in \{o_1,\ldots , o_M\}$
状态转移概率：$a_{ij}=P(y_{t+1}=s_j|y_t=s_i)\in A$ ,      $1\leq i,j \leq N$
输出观测概率：$b_{ij}=P(x_t=o_j|y_t=s_i)\in B $,     $1\leq i\leq N,1\leq j\leq M$
初始状态概率：$\pi=(\pi_1,\ldots,\pi_N)$,其中$\pi_i=P(y_1=s_i)$,   $1\leq i\leq N$	

# 无向图模型

全局马尔可夫性：给定两个变量子集的分离集，则这两个变量子集条件独立。
局部马尔可夫性：给定某变量的邻接变量，则该变量条件独立于其他变量。
成对马尔可夫性：给定所有邻接变量，两个非邻接变量条件独立。

## 马尔可夫随机场

生成式模型

## 对数线性模型

对数线性模型（Log-Linear Model）或称最大熵模型（Maximum Entropy Model）



<img src="pic/最大熵模型.png" alt="最大熵模型" style="zoom:50%;" />

## 条件随机场

条件随机场（Conditional Random Field，CRF）是一种直接建模条件概率的无向图模型．
判别式模型
一个最常用的条件随机场为图中所示的链式结构，称为线性链条件随机场（Linear-Chain CRF）。



<img src="pic/线性链的条件随机场.png" alt="线性链的条件随机场" style="zoom:50%;" />

# 推断

## 精确推断

### 精确：变量消去（消除法）

一种动态规划算法，利用图模型所描述的条件独立性来消减计算目标概率所需计算量
是最直观的精确推断方法，也是构建其他精确推断算法的基础

### 信息传递

#### - 信念传播

#### - 和-积算法

### = = = = = =

### 针对HMM

#### - 直接计算法

列举所有可能的状态序列，分别计算观测概率进行求和

#### - 前向算法

#### - 后向算法

#### - 前-后向混合算法

#### - 维特比(Viterbi)算法

### 针对CRF

#### - 前向算法

#### - 后向算法

#### - 维特比算法

## 近似推断

### 近似：变分方法

### 近似：采样法

#### - 原始随机采样方法

#### - MCMC方法

#### - Metropolis-Hastings(MH)算法

#### - Metropolis算法

#### - 吉布斯采样

### 信念传播（循环）

### = = = = = =

# 学习

图模型的学习可以分为两部分：
	一是==网络结构学习==，即寻找最优的网络结构；
	二是==网络参数估计==，即已知网络结构，估计每个条件概率分布的参数．

​	网络结构学习比较困难，一般是由领域专家来构建．本节只讨论在给定网络结构条件下的**参数估计问题**．图模型的参数估计问题又分为**不包含隐变量时的参数估计问题**和**包含隐变量时的参数估计问题**．

## 极大似然估计

>注意：极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的



## EM算法

EM(Expectation Maximization)算法

> 注意：EM算法和极大似然估计的前提是一样的，都要假设数据总体的分布，如果不知道数据分布，是无法使用EM算法的

EM算法相当于极大似然估计的扩展。极大似然算法确实可以很方便的根据样本估算模型的参数，如果样本来自一个以上的模型，我们又不知道某个样本点到底是来自某个模型的，那么此时极大似然算法就无能为力了。

## 贝叶斯网络

​	估计贝叶斯网络的CPD表格中的数值很简单，就是==计算训练数据中事件发生的次数==。事实证明这样获得的参数能够最大化被观察到的数据的可能性。

## 马尔可夫网络

​	贝叶斯网络的计数方法对马尔可夫网络没有统计学上的支持（因此会得到次优的参数）。所以我们需要使用更加复杂的技术。这些技术背后的基本思想是梯度下降——我们定义一些描述其概率分布的参数，然后使用梯度下降来寻找能最大化被观察数据的可能性的参数值。最后，我们有了我们模型的参数，我们想在新数据上使用它们，也就是执行推理！





# 参考资料

读懂概率图模型：从基本概念和参数估计开始 - 机器之心的文章 - 知乎 https://zhuanlan.zhihu.com/p/31527050
