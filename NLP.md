[ã€ğŸš€æ€»ç»“ã€‘NLPæ—¥å¸¸å®è·µç»éªŒ](detail/NLP/ä¸ªäººNLPå®è·µç»éªŒ.md)

[ã€ğŸš€ç¬”è®°ã€‘CS224N](detail/NLP/CS224N-2019/CS224Nç¬”è®°.md)



# --PTMs--

Pre-training Modelsï¼ŒNLPç›¸å…³é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹

[ã€ğŸ©æ€»ç»“ã€‘PTM-NLP](detail/NLP/PTM-NLPæ€»ç»“.md)

[ã€ğŸš€èµ„æ–™ã€‘-PTM-NLPæ¦‚è§ˆ](detail/NLP/PTMæ¦‚è§ˆ.md)



## æç¤ºå­¦ä¹  prompt learning

[ã€ğŸš€èµ„æ–™ã€‘æç¤ºå­¦ä¹ æ¦‚è§ˆ](detail/NLP/æç¤ºå­¦ä¹ æ¦‚è§ˆ.md)



# --ä»»åŠ¡--

## äº‹ä»¶æ¨¡å¼å½’çº³

Event Schema Induction

[äº‹ä»¶æ¨¡å¼å½’çº³ç›¸å…³ç ”ç©¶ç®€è¿°- CSDN](https://blog.csdn.net/qq_27590277/article/details/124958441)

## NER å‘½åå®ä½“è¯†åˆ«

ä»Muti-head,Biaffineåˆ°Globalpointerç»†æ•°è¿‘å¹´æ¥çš„å‘½åå®ä½“è¯†åˆ« - æˆ‘æƒ³äº†å¾ˆå¤šäº‹çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/375805722

## MRC æœºå™¨é˜…è¯»ç†è§£

[ã€ğŸ¤–ï¸èµ„æ–™ã€‘MRCä¸ªäººæ€»ç»“](detail/NLP/MRCä¸ªäººæ€»ç»“.md)

## KPE å…³é”®è¯æå–

[ã€ğŸ‘“èµ„æ–™ã€‘KPEæ¦‚è§ˆ](detail/NLP/KPEæ¦‚è§ˆ.md)





# --æ–¹æ³•--

## Self-Attention è‡ªæ³¨æ„åŠ›

åœ¨Transformerçš„è®ºæ–‡ã€ŠAttention is all you needã€‹ä¸­è¢«ç†ŸçŸ¥

é€šä¿—çš„è§£é‡Šï¼šå¯¹åºåˆ—ç¼–ç è¡¨ç¤ºçš„ä¸€ä¸ªå…¨å±€ä¼˜åŒ–

> Self-Attentionçš„ä½œç”¨å°±æ˜¯å…¨å±€å…³è”æƒé‡ï¼Œç„¶ååšè¾“å…¥çš„åŠ æƒå’Œã€‚ ä¾‹å¦‚æˆ‘æœ‰ä¸‰ä¸ªè¯ A B C ï¼Œé¦–å…ˆå°†ABCç¼–ç æˆå‘é‡ï¼Œ ç„¶åè¿›Attentionå±‚ï¼Œ ç„¶åå¯¹ABCåˆ†åˆ«æ´—è„‘ï¼Œå‘Šè¯‰ä½ å…¶ä»–ä¸¤ä¸ªè¯å¯¹ä½ ä¹Ÿå¾ˆé‡è¦ï¼Œä½ çš„å¿ƒé‡Œè¦æœ‰ä»–ä»¬ï¼Œå°±è¿™æ ·æ¯ä¸ªè¯çš„é‡ç»„å°±æ˜¯ç»™è‡ªå·±ä¸€ç‚¹æƒé‡ï¼ˆæœ€å¤šï¼‰ï¼Œç»™å…¶ä»–ä¸¤ä¸ªäººä¸€ç‚¹æƒé‡ï¼Œç„¶åç»„åˆæˆæ–°çš„è‡ªå·±ã€‚https://zhuanlan.zhihu.com/p/449028081
>
> 
>
> The Annotated Transformerâ€”â€”åŸºäºPyTorchçš„å¤ç°æ³¨é‡Š
> 	https://nlp.seas.harvard.edu/2018/04/03/attention.html

## FGM æ¢¯åº¦å¯¹æŠ—

å®ç°ï¼šå¸ˆå…„çš„ç‰ˆæœ¬ï¼šhttp://192.168.126.124:9999/spico1026/ccks2022fewshotese/blob/main/src/task.py
åšå®¢ï¼šhttps://zhuanlan.zhihu.com/p/103593948
å®ç°ï¼šhttps://wmathor.com/index.php/archives/1537/

## CRF

https://github.com/Jianwei-Lv/chinese-event-extraction-pytorch
